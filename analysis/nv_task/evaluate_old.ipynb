{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GPT Extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../outputs/nv_task/extractions/full_lb_nv_task-zeroshot_gpt-4o-mini-2024-07-18.json') as f:\n",
    "    nv_task_gpt_json = json.load(f)\n",
    "\n",
    "# Convert to dictionary and clean\n",
    "nv_task_gpt = {}\n",
    "for item in nv_task_gpt_json:\n",
    "    pmcid = item['pmcid']\n",
    "    nv_task_gpt[pmcid] = {}\n",
    "    for key, value in item.items():\n",
    "        value = None if value in ['null', []] else value\n",
    "        if key == 'Designtype':\n",
    "            key = 'DesignType'\n",
    "\n",
    "        if key == 'Modality':\n",
    "            value = [v.replace(' ', '') for v in value]\n",
    "\n",
    "        nv_task_gpt[pmcid][key] = value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labelrepo.projects.nv_task import load_annotations\n",
    "\n",
    "annotations = load_annotations()\n",
    "annotations = annotations[annotations['annotator_name'] != 'alice_chen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TaskName', 'DesignType-RestingState', 'Modality-fMRI-BOLD',\n",
       "       'ContrastDefinition', 'Modality-StructuralMRI',\n",
       "       'Modality-DiffusionMRI', 'Modality-fMRI-CBF', 'TaskDescription',\n",
       "       'Condition', 'Exclude-MetaAnalysis', 'Modality-MRS',\n",
       "       'Modality-EEG'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.label_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_task_name(rows):\n",
    "    # Add TaskName, replacing 'None' and 'Unsure' with 'n/a'\n",
    "    rows = rows[rows.label_name == 'TaskName']\n",
    "    task_names = []\n",
    "    for _, row in rows.iterrows():\n",
    "        if row['None'] or row['Unsure']:\n",
    "            task_names.append('n/a')\n",
    "        else:\n",
    "            task_names.append(row['selected_text'])\n",
    "    return task_names\n",
    "\n",
    "# Convert to comparable dictionary\n",
    "annotations_summary = {}\n",
    "for pmcid, df in annotations.groupby('pmcid'):\n",
    "    design_type = []\n",
    "    if 'DesignType-RestingState' in df.label_name.values:\n",
    "        design_type.append('RestingState')\n",
    "    if 'TaskName' in df.label_name.values:\n",
    "        design_type.append('Task-based')\n",
    "\n",
    "    s = {\n",
    "        'pmcid': pmcid,\n",
    "        'DesignType': design_type,\n",
    "        'annotator_name': df.annotator_name.iloc[0],\n",
    "        'Exclude': next(\n",
    "            (label.split('-', 1)[1] for label in df.label_name if label.startswith('Exclude')), None\n",
    "        ),\n",
    "        'Modality': [\n",
    "            label.split('-', 1)[1] for label in df.label_name if label.startswith('Modality')\n",
    "        ] or None,\n",
    "    }\n",
    "\n",
    "    df_abstract = df[df.section == 'abstract']\n",
    "    abstract_tasks = _get_task_name(df_abstract)\n",
    "\n",
    "    df_body = df[df.section == 'body']\n",
    "    body_tasks = _get_task_name(df_body)\n",
    "\n",
    "    # Use body tasks if available, otherwise use abstract tasks\n",
    "    s['TaskName'] = body_tasks or abstract_tasks\n",
    "\n",
    "    for k in ['TaskDescription', 'Condition', 'ContrastDefinition']:\n",
    "        s[k] = df_body.loc[df_body.label_name == k, 'selected_text'].tolist() or None\n",
    "\n",
    "    annotations_summary[pmcid] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalation set properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tasks = []\n",
    "exclusions = []\n",
    "modality = []\n",
    "design_type = []\n",
    "has_fmri_bold = []\n",
    "for pmcid, d in annotations_summary.items():\n",
    "    unique_tasks += (d['TaskName'])\n",
    "    if d['Exclude']:\n",
    "        exclusions.append(d['Exclude'])\n",
    "\n",
    "    if d['Modality']:\n",
    "        modality += d['Modality']\n",
    "\n",
    "    design_type += d['DesignType']\n",
    "\n",
    "    if d['Modality'] and 'fMRI-BOLD' in d['Modality']:\n",
    "        has_fmri_bold.append(pmcid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(has_fmri_bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(unique_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tasks.count('n/a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896551724137931"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modality.count('fMRI-BOLD') / len(modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8631578947368421"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "design_type.count('Task-based') / len(design_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Extractions to Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from thefuzz import process, fuzz\n",
    "\n",
    "def _clean(x):\n",
    "    _li =  [x.lower().replace('paradigm', '').replace('task', '').replace('‚Äê', '').strip() for x in (x or [])]\n",
    "\n",
    "    # Remove abbannotationsreviations\n",
    "    return [re.sub(r'\\s*\\([^)]*\\)', '', x) for x in _li]\n",
    "\n",
    "def _compare(x, y):\n",
    "    x = x or ''\n",
    "    y = y or ''\n",
    "    return x.replace('-', '').lower() == y.replace('-', '').lower()\n",
    "\n",
    "def _score_comparison(x, y):\n",
    "    res = _compare(x, y)\n",
    "    return 100 if res else 0\n",
    "\n",
    "bert_scorer = BERTScorer(lang='en', rescale_with_baseline=True)\n",
    "\n",
    "def _score_f1_bert(x, y):\n",
    "    \"\"\" Score F1 using BERT \"\"\"\n",
    "    if not x or not y:\n",
    "        return 0\n",
    "\n",
    "    _, _, f1 = bert_scorer.score([x], [y])\n",
    "    _val = f1.item() * 100\n",
    "    if _val < 0:\n",
    "        return 0\n",
    "    return _val\n",
    "\n",
    "def score_combinations(correct_labels, extracted_labels, scorer=fuzz.token_set_ratio):\n",
    "    correct_labels = _clean(correct_labels)\n",
    "    extracted_labels = _clean(extracted_labels)\n",
    "    \n",
    "    matched_labels = []\n",
    "    \n",
    "    while correct_labels and extracted_labels:\n",
    "        # Collect all matches and their scores\n",
    "        all_matches = []\n",
    "        for correct_label in correct_labels:\n",
    "            matches = process.extract(correct_label, extracted_labels, limit=None, scorer=scorer)\n",
    "            for matched_label, score in matches:\n",
    "                all_matches.append((correct_label, matched_label, score))\n",
    "\n",
    "        # Sort all matches by score in descending order\n",
    "        all_matches.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        # Take the highest match\n",
    "        best_match = all_matches[0]\n",
    "        correct_label, matched_label, score = best_match\n",
    "\n",
    "        # Append to results and remove matched labels\n",
    "        matched_labels.append(score)\n",
    "        correct_labels.remove(correct_label)\n",
    "        extracted_labels.remove(matched_label)\n",
    "\n",
    "    matched_labels = (sum(matched_labels) / len(matched_labels)) / 100 if matched_labels else 0\n",
    "\n",
    "    return matched_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: ',']\n",
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: ',']\n",
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: ',']\n",
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    }
   ],
   "source": [
    "# Compare annotations to extractions\n",
    "all_scores = {}\n",
    "all_scores_bert = {}\n",
    "\n",
    "for pmcid, ann in annotations_summary.items():\n",
    "    all_scores[pmcid] = {\n",
    "        'Exclude': _compare(ann['Exclude'], nv_task_gpt[pmcid]['Exclude'])\n",
    "    }\n",
    "\n",
    "    all_scores_bert[pmcid] = {}\n",
    "\n",
    "    # Score non fuzzy for deterministic fields (E.g. with limited options)\n",
    "    # Modality and DesignType - do strict comparison\n",
    "    for k in ['Modality', 'DesignType']:\n",
    "        all_scores[pmcid][k] = score_combinations(ann[k], nv_task_gpt[pmcid][k], scorer=_score_comparison)\n",
    "\n",
    "    # N items match\n",
    "    for k in ['TaskName', 'Condition', 'ContrastDefinition', 'TaskDescription']:\n",
    "        all_scores[pmcid][k] = score_combinations(ann[k], nv_task_gpt[pmcid][k])\n",
    "\n",
    "    # for k in ['TaskName', 'TaskDescription']:\n",
    "    #     all_scores_bert[pmcid][k] = score_combinations(ann[k], nv_task_gpt[pmcid][k], scorer=_score_f1_bert)\n",
    "\n",
    "# Calculate scores\n",
    "n_articles = len(annotations_summary.keys())\n",
    "\n",
    "all_scores_df = pd.DataFrame(all_scores).T\n",
    "# all_scores_bert_df = pd.DataFrame(all_scores_bert).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_idx = [_p for _p, v in annotations_summary.items() if v['Exclude'] is not None]\n",
    "has_task_name = [_p for _p, v in annotations_summary.items() if 'Task-based' in v['DesignType'] and 'n/a' not in v['TaskName']]\n",
    "has_task_noname = [_p for _p, v in annotations_summary.items() if 'Task-based' in v['DesignType'] and 'n/a' in v['TaskName']]\n",
    "\n",
    "def compute_subset(df):\n",
    "    # Overall scores\n",
    "    mean_all = df.mean()\n",
    "    all_n = df.shape[0]\n",
    "\n",
    "    # Excluding articles with 'Exclude' label\n",
    "    no_exclude_res = df.loc[~df.index.isin(exclude_idx)]\n",
    "    no_exclude_res_n = no_exclude_res.shape[0]\n",
    "\n",
    "\n",
    "    # For papers with a clearly defined task name\n",
    "    has_task_name_res = df.loc[df.index.isin(has_task_name)]\n",
    "    has_task_name_n = has_task_name_res.shape[0]\n",
    "\n",
    "\n",
    "    # For papers with a task-based design but no annotated task name\n",
    "    has_task_name_noname = df.loc[df.index.isin(has_task_noname)]\n",
    "    has_task_name_noname_n = has_task_name_noname.shape[0]\n",
    "\n",
    "    # Combine results into a single dataframe\n",
    "    results = pd.concat([mean_all, no_exclude_res.mean(), has_task_name_res.mean(), has_task_name_noname.mean()], axis=1)\n",
    "    results.columns = ['All', 'No Exclude', 'Has Task Name', 'Has Task with No Name']\n",
    "\n",
    "    # Change dtype to float\n",
    "    results = results.astype(float)\n",
    "    results = results.round(2).T\n",
    "\n",
    "    combine_ns = pd.Series([all_n, no_exclude_res_n, has_task_name_n, has_task_name_noname_n], index=results.index, name='n').round(0)\n",
    "    results.insert(loc = 0, column = 'n', value = combine_ns)\n",
    "\n",
    "    return results\n",
    "\n",
    "results = compute_subset(all_scores_df)\n",
    "# results_bert = compute_subset(all_scores_bert_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Condition</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>104</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Exclude</th>\n",
       "      <td>95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Has Task Name</th>\n",
       "      <td>64</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Has Task with No Name</th>\n",
       "      <td>18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         n  Exclude  Modality  DesignType  TaskName  \\\n",
       "All                    104     0.98      0.91        0.87      0.60   \n",
       "No Exclude              95     1.00      1.00        0.95      0.66   \n",
       "Has Task Name           64     1.00      1.00        1.00      0.93   \n",
       "Has Task with No Name   18     1.00      1.00        1.00      0.19   \n",
       "\n",
       "                       Condition  ContrastDefinition  TaskDescription  \n",
       "All                         0.54                0.41             0.50  \n",
       "No Exclude                  0.59                0.45             0.55  \n",
       "Has Task Name               0.72                0.57             0.68  \n",
       "Has Task with No Name       0.58                0.35             0.48  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot results as table\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import table\n",
    "\n",
    "results = results.round(2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "tbl = table(ax, results, loc='center', cellLoc='center', colWidths=[0.2]*len(results.columns))\n",
    "tbl.auto_set_font_size(False)\n",
    "tbl.set_fontsize(12)\n",
    "tbl.scale(1.2, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Condition</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>104</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Exclude</th>\n",
       "      <td>95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Has Task Name</th>\n",
       "      <td>64</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Has Task with No Name</th>\n",
       "      <td>18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         n  Exclude  Modality  DesignType  TaskName  \\\n",
       "All                    104     0.98      0.91        0.87      0.60   \n",
       "No Exclude              95     1.00      1.00        0.95      0.66   \n",
       "Has Task Name           64     1.00      1.00        1.00      0.93   \n",
       "Has Task with No Name   18     1.00      1.00        1.00      0.19   \n",
       "\n",
       "                       Condition  ContrastDefinition  TaskDescription  \n",
       "All                         0.54                0.41             0.50  \n",
       "No Exclude                  0.59                0.45             0.55  \n",
       "Has Task Name               0.72                0.57             0.68  \n",
       "Has Task with No Name       0.58                0.35             0.48  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([pd.DataFrame(annotations_summary).T, pd.DataFrame(nv_task_gpt).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('max_colwidth', 300)\n",
    "    \n",
    "\n",
    "def _display(df, pmcids=None):\n",
    "    cols = list(set(df.keys()) - set(['pmcid']))\n",
    "    if pmcids:\n",
    "        df = df[df.pmcid.isin(pmcids)]\n",
    "    for _, _df in df.groupby('pmcid'):\n",
    "        display(_df[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3555187</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>MetaAnalysis</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3555187</th>\n",
       "      <td>[activity in the basal ganglia and cerebellum was significantly stronger for sensorimotor tasks, activity in cortical structures and the thalamus was significantly stronger for SRTT variants]</td>\n",
       "      <td>[motor learning, serial response time task]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[The study identified consistent activations across 70 motor learning experiments using activation likelihood estimation (ALE) meta-analysis. A global analysis of all tasks revealed a bilateral cortical-subcortical network consistently underlying motor learning across tasks.]</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[sensorimotor tasks, SRTT variants]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                      ContrastDefinition  \\\n",
       "3555187                                                                                                                                                                                             None   \n",
       "3555187  [activity in the basal ganglia and cerebellum was significantly stronger for sensorimotor tasks, activity in cortical structures and the thalamus was significantly stronger for SRTT variants]   \n",
       "\n",
       "                                            TaskName annotator_name  \\\n",
       "3555187                                           []    delavega_nv   \n",
       "3555187  [motor learning, serial response time task]            NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                              TaskDescription  \\\n",
       "3555187                                                                                                                                                                                                                                                                                  None   \n",
       "3555187  [The study identified consistent activations across 70 motor learning experiments using activation likelihood estimation (ALE) meta-analysis. A global analysis of all tasks revealed a bilateral cortical-subcortical network consistently underlying motor learning across tasks.]   \n",
       "\n",
       "              Exclude     Modality    DesignType  \\\n",
       "3555187  MetaAnalysis         None            []   \n",
       "3555187          None  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                   Condition  \n",
       "3555187                                 None  \n",
       "3555187  [sensorimotor tasks, SRTT variants]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10634720</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>MetaAnalysis</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10634720</th>\n",
       "      <td>[exploration &gt; exploitation, exploitation &gt; exploration]</td>\n",
       "      <td>[Explore-Exploit Decision Making]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Participants engage in tasks that require them to make decisions about whether to explore new options or exploit known ones, using various task designs such as n-armed bandit and foraging tasks.]</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Exploration, Exploitation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ContrastDefinition  \\\n",
       "10634720                                                      None   \n",
       "10634720  [exploration > exploitation, exploitation > exploration]   \n",
       "\n",
       "                                   TaskName         annotator_name  \\\n",
       "10634720                                 []  delavega-aliceoverlap   \n",
       "10634720  [Explore-Exploit Decision Making]                    NaN   \n",
       "\n",
       "                                                                                                                                                                                               TaskDescription  \\\n",
       "10634720                                                                                                                                                                                                  None   \n",
       "10634720  [Participants engage in tasks that require them to make decisions about whether to explore new options or exploit known ones, using various task designs such as n-armed bandit and foraging tasks.]   \n",
       "\n",
       "               Exclude     Modality    DesignType                    Condition  \n",
       "10634720  MetaAnalysis         None            []                         None  \n",
       "10634720          None  [fMRI-BOLD]  [Task-based]  [Exploration, Exploitation]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Diff on Exclude\n",
    "\n",
    "diff_exclude = all_scores_df[all_scores_df['Exclude'] == False].index.tolist()\n",
    "\n",
    "if diff_exclude:\n",
    "    _display(combined_df, diff_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on, we will exclude Excluded articles\n",
    "combined_df = combined_df[~combined_df.index.isin(exclude_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6331309</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331309</th>\n",
       "      <td>[Migraine vs Healthy Controls, RLS vs Healthy Controls, Migraine vs RLS, Comorbid Migraine and RLS vs Healthy Controls]</td>\n",
       "      <td>[MRI data acquisition]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[High-resolution T1-weighted images were acquired from 116 subjects: 27 RLS patients, 22 migraine patients, 22 patients with comorbid migraine and RLS, and 45 healthy controls.]</td>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Migraine, Restless Legs Syndrome, Comorbid Migraine and RLS, Healthy Controls]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              ContrastDefinition  \\\n",
       "6331309                                                                                                                     None   \n",
       "6331309  [Migraine vs Healthy Controls, RLS vs Healthy Controls, Migraine vs RLS, Comorbid Migraine and RLS vs Healthy Controls]   \n",
       "\n",
       "                       TaskName annotator_name  \\\n",
       "6331309                      []    delavega_nv   \n",
       "6331309  [MRI data acquisition]            NaN   \n",
       "\n",
       "                                                                                                                                                                           TaskDescription  \\\n",
       "6331309                                                                                                                                                                               None   \n",
       "6331309  [High-resolution T1-weighted images were acquired from 116 subjects: 27 RLS patients, 22 migraine patients, 22 patients with comorbid migraine and RLS, and 45 healthy controls.]   \n",
       "\n",
       "        Exclude         Modality    DesignType  \\\n",
       "6331309    None  [StructuralMRI]            []   \n",
       "6331309    None  [StructuralMRI]  [Task-based]   \n",
       "\n",
       "                                                                               Condition  \n",
       "6331309                                                                             None  \n",
       "6331309  [Migraine, Restless Legs Syndrome, Comorbid Migraine and RLS, Healthy Controls]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7426775</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7426775</th>\n",
       "      <td>[Age-related structural changes in the brain.]</td>\n",
       "      <td>[Brain Age Prediction]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Predicting chronological age from structural MRI scans using a deep learning model.]</td>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Age prediction based on structural MRI.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ContrastDefinition  \\\n",
       "7426775                                            None   \n",
       "7426775  [Age-related structural changes in the brain.]   \n",
       "\n",
       "                       TaskName         annotator_name  \\\n",
       "7426775                      []  delavega-aliceoverlap   \n",
       "7426775  [Brain Age Prediction]                    NaN   \n",
       "\n",
       "                                                                               TaskDescription  \\\n",
       "7426775                                                                                   None   \n",
       "7426775  [Predicting chronological age from structural MRI scans using a deep learning model.]   \n",
       "\n",
       "        Exclude         Modality    DesignType  \\\n",
       "7426775    None  [StructuralMRI]            []   \n",
       "7426775    None  [StructuralMRI]  [Task-based]   \n",
       "\n",
       "                                         Condition  \n",
       "7426775                                       None  \n",
       "7426775  [Age prediction based on structural MRI.]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7582181</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7582181</th>\n",
       "      <td>[Social reward expectancy vs. social threat expectancy]</td>\n",
       "      <td>[Imagining social interactions]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Participants were asked to vividly imagine themselves in a novel self-relevant event that was ambiguous with regards to possible social acceptance or rejection.]</td>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Social reward expectancy, Social threat expectancy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              ContrastDefinition  \\\n",
       "7582181                                                     None   \n",
       "7582181  [Social reward expectancy vs. social threat expectancy]   \n",
       "\n",
       "                                TaskName         annotator_name  \\\n",
       "7582181                               []  delavega-aliceoverlap   \n",
       "7582181  [Imagining social interactions]                    NaN   \n",
       "\n",
       "                                                                                                                                                            TaskDescription  \\\n",
       "7582181                                                                                                                                                                None   \n",
       "7582181  [Participants were asked to vividly imagine themselves in a novel self-relevant event that was ambiguous with regards to possible social acceptance or rejection.]   \n",
       "\n",
       "        Exclude         Modality    DesignType  \\\n",
       "7582181    None  [StructuralMRI]            []   \n",
       "7582181    None  [StructuralMRI]  [Task-based]   \n",
       "\n",
       "                                                    Condition  \n",
       "7582181                                                  None  \n",
       "7582181  [Social reward expectancy, Social threat expectancy]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8564184</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8564184</th>\n",
       "      <td>[GMV in ulcerative colitis vs. healthy controls, GMV in irritable bowel syndrome vs. healthy controls, Correlations between GMV and symptom severity, Correlations between GMV and chronic stress]</td>\n",
       "      <td>[Voxel-based morphometry]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Parallelized whole-brain voxel-based morphometry analyses in two patient cohorts with chronic visceral pain (ulcerative colitis in remission and irritable bowel syndrome) and healthy individuals.]</td>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Chronic visceral pain, Healthy controls]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                         ContrastDefinition  \\\n",
       "8564184                                                                                                                                                                                                None   \n",
       "8564184  [GMV in ulcerative colitis vs. healthy controls, GMV in irritable bowel syndrome vs. healthy controls, Correlations between GMV and symptom severity, Correlations between GMV and chronic stress]   \n",
       "\n",
       "                          TaskName         annotator_name  \\\n",
       "8564184                         []  delavega-aliceoverlap   \n",
       "8564184  [Voxel-based morphometry]                    NaN   \n",
       "\n",
       "                                                                                                                                                                                               TaskDescription  \\\n",
       "8564184                                                                                                                                                                                                   None   \n",
       "8564184  [Parallelized whole-brain voxel-based morphometry analyses in two patient cohorts with chronic visceral pain (ulcerative colitis in remission and irritable bowel syndrome) and healthy individuals.]   \n",
       "\n",
       "        Exclude         Modality    DesignType  \\\n",
       "8564184    None  [StructuralMRI]            []   \n",
       "8564184    None  [StructuralMRI]  [Task-based]   \n",
       "\n",
       "                                         Condition  \n",
       "8564184                                       None  \n",
       "8564184  [Chronic visceral pain, Healthy controls]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9202476</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD, DiffusionMRI]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202476</th>\n",
       "      <td>[PS &gt; GS, GS &gt; PS]</td>\n",
       "      <td>[match-to-sample (MTS) task]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[In this computerized task, subjects were presented with a complex figure in the middle of the screen. Then, a few patterns were shown in the periphery, from which one was matched with the presented pattern. In the first trials, two patterns were presented in the periphery, and it was increased ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD, DiffusionMRI]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[poor sleepers (PSs), good sleepers (GSs)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ContrastDefinition                      TaskName  \\\n",
       "9202476                None                            []   \n",
       "9202476  [PS > GS, GS > PS]  [match-to-sample (MTS) task]   \n",
       "\n",
       "                annotator_name  \\\n",
       "9202476  delavega-aliceoverlap   \n",
       "9202476                    NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "9202476                                                                                                                                                                                                                                                                                                         None   \n",
       "9202476  [In this computerized task, subjects were presented with a complex figure in the middle of the screen. Then, a few patterns were shown in the periphery, from which one was matched with the presented pattern. In the first trials, two patterns were presented in the periphery, and it was increased ...   \n",
       "\n",
       "        Exclude                   Modality      DesignType  \\\n",
       "9202476    None  [fMRI-BOLD, DiffusionMRI]  [RestingState]   \n",
       "9202476    None  [fMRI-BOLD, DiffusionMRI]    [Task-based]   \n",
       "\n",
       "                                          Condition  \n",
       "9202476                                        None  \n",
       "9202476  [poor sleepers (PSs), good sleepers (GSs)]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Diff on DesignType\n",
    "\n",
    "diff_dt = all_scores_df[all_scores_df['DesignType'] < 1].index.tolist()\n",
    "\n",
    "if diff_dt:\n",
    "    _display(combined_df, diff_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3445793,\n",
       " 3555187,\n",
       " 5243799,\n",
       " 6331309,\n",
       " 6344321,\n",
       " 7426775,\n",
       " 7582181,\n",
       " 8421705,\n",
       " 8564184,\n",
       " 9189080,\n",
       " 9202476,\n",
       " 10147761,\n",
       " 10318245,\n",
       " 10634720]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Exclude                   True\n",
       "TaskName                   0.8\n",
       "Condition             0.913333\n",
       "ContrastDefinition         1.0\n",
       "DesignType                 1.0\n",
       "Modality                   1.0\n",
       "TaskDescription              0\n",
       "Name: 2686646, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores_df.loc[2686646]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems most misses by GPT were due to task performed *outside the scanner*, yet listed as Task-based design, which in this instance should only refer to Task-based fMRI design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4374765</th>\n",
       "      <td>[spiders, control animals, IAPSnegative, IAPSneutral]</td>\n",
       "      <td>[MRS, fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>[SPIDERS &gt; ANIMALS, IAPSnegative &gt; IAPSneutral]</td>\n",
       "      <td>[fear inducing paradigm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374765</th>\n",
       "      <td>[SPIDERS, ANIMALS, IAPSnegative, IAPSneutral]</td>\n",
       "      <td>[fMRI-BOLD, MRS]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants were presented with still pictures of spiders, control animals (birds, caterpillars, snails, and lizards), negative pictures from the International Affective Picture System (IAPS), and neutral pictures from IAPS. They performed a covert task of detecting the presence of a human in ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[SPIDERS &gt; ANIMALS, IAPSnegative &gt; IAPSneutral]</td>\n",
       "      <td>[Fear provocation paradigm]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Condition  \\\n",
       "4374765  [spiders, control animals, IAPSnegative, IAPSneutral]   \n",
       "4374765          [SPIDERS, ANIMALS, IAPSnegative, IAPSneutral]   \n",
       "\n",
       "                 Modality    DesignType  \\\n",
       "4374765  [MRS, fMRI-BOLD]  [Task-based]   \n",
       "4374765  [fMRI-BOLD, MRS]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "4374765                                                                                                                                                                                                                                                                                                         None   \n",
       "4374765  [Participants were presented with still pictures of spiders, control animals (birds, caterpillars, snails, and lizards), negative pictures from the International Affective Picture System (IAPS), and neutral pictures from IAPS. They performed a covert task of detecting the presence of a human in ...   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "4374765    delavega_nv    None   \n",
       "4374765            NaN    None   \n",
       "\n",
       "                                      ContrastDefinition  \\\n",
       "4374765  [SPIDERS > ANIMALS, IAPSnegative > IAPSneutral]   \n",
       "4374765  [SPIDERS > ANIMALS, IAPSnegative > IAPSneutral]   \n",
       "\n",
       "                            TaskName  \n",
       "4374765     [fear inducing paradigm]  \n",
       "4374765  [Fear provocation paradigm]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5324609</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD, fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[participants learned a set of visual discriminations via trial‚Äêand‚Äêerror (learning phase), and were subsequently tested on their ability to generalize what they had learned (generalization phase). Both learning and generalization occurred within a single scanning session and took place in the c...</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>[linear increases in activity across the four learning stage,  HRF amplitude estimates relating to generalization test trials were correlated with generalization performance at the second‚Äêlevel]</td>\n",
       "      <td>[acquired equivalence task]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5324609</th>\n",
       "      <td>[rewarded, unrewarded]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[During scanning participants learned a set of visual discriminations via trial-and-error (learning phase), and were subsequently tested on their ability to generalize what they had learned (generalization phase). Both learning and generalization occurred within a single scanning session and too...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[generalization performance, hippocampal activity during the test, BOLD activity during the final stage of learning]</td>\n",
       "      <td>[learning phase, generalization phase]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Condition                Modality    DesignType  \\\n",
       "5324609                    None  [fMRI-BOLD, fMRI-BOLD]  [Task-based]   \n",
       "5324609  [rewarded, unrewarded]             [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "5324609  [participants learned a set of visual discriminations via trial‚Äêand‚Äêerror (learning phase), and were subsequently tested on their ability to generalize what they had learned (generalization phase). Both learning and generalization occurred within a single scanning session and took place in the c...   \n",
       "5324609  [During scanning participants learned a set of visual discriminations via trial-and-error (learning phase), and were subsequently tested on their ability to generalize what they had learned (generalization phase). Both learning and generalization occurred within a single scanning session and too...   \n",
       "\n",
       "                annotator_name Exclude  \\\n",
       "5324609  delavega-aliceoverlap    None   \n",
       "5324609                    NaN    None   \n",
       "\n",
       "                                                                                                                                                                                         ContrastDefinition  \\\n",
       "5324609  [linear increases in activity across the four learning stage,  HRF amplitude estimates relating to generalization test trials were correlated with generalization performance at the second‚Äêlevel]   \n",
       "5324609                                                                                [generalization performance, hippocampal activity during the test, BOLD activity during the final stage of learning]   \n",
       "\n",
       "                                       TaskName  \n",
       "5324609             [acquired equivalence task]  \n",
       "5324609  [learning phase, generalization phase]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5776089</th>\n",
       "      <td>[Task, Task+AgonistStim, Task+ControlStim]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[he fMRI session was composed of nine rest‚Äìtask cycles with 30 s for each period. Eyes were kept closed during scanning. The motor task consisted of repetitive alternating dorsiflexion and relaxation of the right foot (with range reaching 15¬∞). Foot movements were paced following an audio cue th...</td>\n",
       "      <td>delavega-other</td>\n",
       "      <td>None</td>\n",
       "      <td>[‚ÄúTask+AgonistStim minus Task‚Äù]</td>\n",
       "      <td>[Motor Task, Somatosensory Stimulation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5776089</th>\n",
       "      <td>[Task, Task+AgonistStim, Task+ControlStim]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[The motor task consisted of repetitive alternating dorsiflexion and relaxation of the right foot (with range reaching 15). Foot movements were paced following an audio cue that was sounded every 1.5 s.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Task+AgonistStim vs. Task, Task+ControlStim vs. Task, Task+AgonistStim vs. Task+ControlStim]</td>\n",
       "      <td>[right ankle dorsiflexion, ankle dorsiflexion coupled with simultaneous stimulation to the agonist muscle, ankle dorsiflexion coupled with simultaneous stimulation to a control area]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Condition     Modality  \\\n",
       "5776089  [Task, Task+AgonistStim, Task+ControlStim]  [fMRI-BOLD]   \n",
       "5776089  [Task, Task+AgonistStim, Task+ControlStim]  [fMRI-BOLD]   \n",
       "\n",
       "           DesignType  \\\n",
       "5776089  [Task-based]   \n",
       "5776089  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "5776089  [he fMRI session was composed of nine rest‚Äìtask cycles with 30 s for each period. Eyes were kept closed during scanning. The motor task consisted of repetitive alternating dorsiflexion and relaxation of the right foot (with range reaching 15¬∞). Foot movements were paced following an audio cue th...   \n",
       "5776089                                                                                                  [The motor task consisted of repetitive alternating dorsiflexion and relaxation of the right foot (with range reaching 15). Foot movements were paced following an audio cue that was sounded every 1.5 s.]   \n",
       "\n",
       "         annotator_name Exclude  \\\n",
       "5776089  delavega-other    None   \n",
       "5776089             NaN    None   \n",
       "\n",
       "                                                                                    ContrastDefinition  \\\n",
       "5776089                                                                [‚ÄúTask+AgonistStim minus Task‚Äù]   \n",
       "5776089  [Task+AgonistStim vs. Task, Task+ControlStim vs. Task, Task+AgonistStim vs. Task+ControlStim]   \n",
       "\n",
       "                                                                                                                                                                                       TaskName  \n",
       "5776089                                                                                                                                                 [Motor Task, Somatosensory Stimulation]  \n",
       "5776089  [right ankle dorsiflexion, ankle dorsiflexion coupled with simultaneous stimulation to the agonist muscle, ankle dorsiflexion coupled with simultaneous stimulation to a control area]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6219793</th>\n",
       "      <td>[self-distanced, self-immersed]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Following previous studies [ ‚Äì ], negative social feedback was used to induce emotions for two reasons: (a) in daily life emotions are often caused by social stimuli [ , ] and (b) social feedback elicits emotional responses that are long enough to study emotion dynamics [ ]. The social feedback...</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Social feedback paradigm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6219793</th>\n",
       "      <td>[Self-distanced perspective, Self-immersed perspective]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants were asked to adopt a self-immersed or self-distanced perspective while reading and thinking about negative social feedback.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Self-distanced &gt; Self-immersed, Self-immersed &gt; Self-distanced]</td>\n",
       "      <td>[Self-distanced vs. self-immersed perspective]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Condition     Modality  \\\n",
       "6219793                          [self-distanced, self-immersed]  [fMRI-BOLD]   \n",
       "6219793  [Self-distanced perspective, Self-immersed perspective]  [fMRI-BOLD]   \n",
       "\n",
       "           DesignType  \\\n",
       "6219793  [Task-based]   \n",
       "6219793  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "6219793  [Following previous studies [ ‚Äì ], negative social feedback was used to induce emotions for two reasons: (a) in daily life emotions are often caused by social stimuli [ , ] and (b) social feedback elicits emotional responses that are long enough to study emotion dynamics [ ]. The social feedback...   \n",
       "6219793                                                                                                                                                                  [Participants were asked to adopt a self-immersed or self-distanced perspective while reading and thinking about negative social feedback.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "6219793    delavega_nv    None   \n",
       "6219793            NaN    None   \n",
       "\n",
       "                                                       ContrastDefinition  \\\n",
       "6219793                                                              None   \n",
       "6219793  [Self-distanced > Self-immersed, Self-immersed > Self-distanced]   \n",
       "\n",
       "                                               TaskName  \n",
       "6219793                      [Social feedback paradigm]  \n",
       "6219793  [Self-distanced vs. self-immersed perspective]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6699247</th>\n",
       "      <td>None</td>\n",
       "      <td>[EEG, fMRI-BOLD, MRS]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[In this task, a simple line drawing of an object is presented (250‚ÄØms), and following a short delay (75‚ÄØms), a word appears. Subjects are instructed to respond by indicating whether the word was a semantic match (50% of trials) or non-match (unmatched, 50% of trials) to the preceding picture. P...</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[picture‚Äìword verification task]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6699247</th>\n",
       "      <td>[matched, in-category, out of category]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants performed a picture-word matching task, in which words were either matched by preceding pictures, or were unmatched by semantically related or unrelated pictures.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[IC vs. matched, OC vs. matched]</td>\n",
       "      <td>[picture-word matching task]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Condition               Modality  \\\n",
       "6699247                                     None  [EEG, fMRI-BOLD, MRS]   \n",
       "6699247  [matched, in-category, out of category]            [fMRI-BOLD]   \n",
       "\n",
       "           DesignType  \\\n",
       "6699247  [Task-based]   \n",
       "6699247  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "6699247  [In this task, a simple line drawing of an object is presented (250‚ÄØms), and following a short delay (75‚ÄØms), a word appears. Subjects are instructed to respond by indicating whether the word was a semantic match (50% of trials) or non-match (unmatched, 50% of trials) to the preceding picture. P...   \n",
       "6699247                                                                                                                            [Participants performed a picture-word matching task, in which words were either matched by preceding pictures, or were unmatched by semantically related or unrelated pictures.]   \n",
       "\n",
       "        annotator_name Exclude                ContrastDefinition  \\\n",
       "6699247    delavega_nv    None                              None   \n",
       "6699247            NaN    None  [IC vs. matched, OC vs. matched]   \n",
       "\n",
       "                                 TaskName  \n",
       "6699247  [picture‚Äìword verification task]  \n",
       "6699247      [picture-word matching task]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7018765</th>\n",
       "      <td>[‚Äúboth correct‚Äù, ‚Äúcostly error‚Äù, participant responded incorrectly, both the participant and their partner responded incorrectly,]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[his task builds on past studies that have utilized error processing paradigms in the study of ER (e.g.,  ;  ;  ) by specifically examining dyadic error processing. Parent-adolescent dyads completed the TEAM task while simultaneously undergoing fMRI scanning. This task was developed to examine b...</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>[‚Äúcostly error‚Äù ‚Äì ‚Äúboth correct‚Äù, ‚Äúcostly error‚Äù, ‚Äúboth correct‚Äù]</td>\n",
       "      <td>[Testing Emotional Attunement and Mutuality (TEAM)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7018765</th>\n",
       "      <td>[costly error condition, both correct condition]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[The TEAM task is an event-related design and consists of 17 trials during which participants first see a pattern of colored arrows presented sequentially on the screen for 3 s, twice in a row (totaling 6 s). They are then given 4 s to reconstruct the sequence by pressing colored buttons on a re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[costly error vs. both correct]</td>\n",
       "      <td>[TEAM task]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                  Condition  \\\n",
       "7018765  [‚Äúboth correct‚Äù, ‚Äúcostly error‚Äù, participant responded incorrectly, both the participant and their partner responded incorrectly,]   \n",
       "7018765                                                                                    [costly error condition, both correct condition]   \n",
       "\n",
       "            Modality    DesignType  \\\n",
       "7018765  [fMRI-BOLD]  [Task-based]   \n",
       "7018765  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "7018765  [his task builds on past studies that have utilized error processing paradigms in the study of ER (e.g.,  ;  ;  ) by specifically examining dyadic error processing. Parent-adolescent dyads completed the TEAM task while simultaneously undergoing fMRI scanning. This task was developed to examine b...   \n",
       "7018765  [The TEAM task is an event-related design and consists of 17 trials during which participants first see a pattern of colored arrows presented sequentially on the screen for 3 s, twice in a row (totaling 6 s). They are then given 4 s to reconstruct the sequence by pressing colored buttons on a re...   \n",
       "\n",
       "                annotator_name Exclude  \\\n",
       "7018765  delavega-aliceoverlap    None   \n",
       "7018765                    NaN    None   \n",
       "\n",
       "                                                        ContrastDefinition  \\\n",
       "7018765  [‚Äúcostly error‚Äù ‚Äì ‚Äúboth correct‚Äù, ‚Äúcostly error‚Äù, ‚Äúboth correct‚Äù]   \n",
       "7018765                                    [costly error vs. both correct]   \n",
       "\n",
       "                                                    TaskName  \n",
       "7018765  [Testing Emotional Attunement and Mutuality (TEAM)]  \n",
       "7018765                                          [TEAM task]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11063816</th>\n",
       "      <td>[open-loop, delay, open-loop, no-delay, closed-loop, delay, closed-loop, no-delay]</td>\n",
       "      <td>[fMRI-BOLD, fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[On each trial, a cue and six potential targets were presented simultaneously on the screen. The cue was presented in the middle of the screen with the six possible targets; one target and five foils form the same category (e.g., if the target was hammer, the five foils would be other randomly s...</td>\n",
       "      <td>delavega-other</td>\n",
       "      <td>None</td>\n",
       "      <td>[people, locations, objects, nontarget reinstatement effect, collapsing across the delay and no-delay condition, nontarget reinstatement effect differed between the delay and no-delay conditions , differences between closed- and open-loops in the cue and target regions in the delay and no-delay ...</td>\n",
       "      <td>[six-alternative forced-choice cued-recognition task]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11063816</th>\n",
       "      <td>[Delay, No-delay]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants learned events that comprised multiple overlapping pairs of event elements (e.g., person-location, object-location, location-person). Encoding occurred either immediately before or 24h before retrieval. Using fMRI during the retrieval of events, the study assessed whether episodic ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[closed-loop vs open-loop, immediate vs delayed retrieval]</td>\n",
       "      <td>[Episodic memory retrieval]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   Condition  \\\n",
       "11063816  [open-loop, delay, open-loop, no-delay, closed-loop, delay, closed-loop, no-delay]   \n",
       "11063816                                                                   [Delay, No-delay]   \n",
       "\n",
       "                        Modality    DesignType  \\\n",
       "11063816  [fMRI-BOLD, fMRI-BOLD]  [Task-based]   \n",
       "11063816             [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      TaskDescription  \\\n",
       "11063816  [On each trial, a cue and six potential targets were presented simultaneously on the screen. The cue was presented in the middle of the screen with the six possible targets; one target and five foils form the same category (e.g., if the target was hammer, the five foils would be other randomly s...   \n",
       "11063816  [Participants learned events that comprised multiple overlapping pairs of event elements (e.g., person-location, object-location, location-person). Encoding occurred either immediately before or 24h before retrieval. Using fMRI during the retrieval of events, the study assessed whether episodic ...   \n",
       "\n",
       "          annotator_name Exclude  \\\n",
       "11063816  delavega-other    None   \n",
       "11063816             NaN    None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                   ContrastDefinition  \\\n",
       "11063816  [people, locations, objects, nontarget reinstatement effect, collapsing across the delay and no-delay condition, nontarget reinstatement effect differed between the delay and no-delay conditions , differences between closed- and open-loops in the cue and target regions in the delay and no-delay ...   \n",
       "11063816                                                                                                                                                                                                                                                   [closed-loop vs open-loop, immediate vs delayed retrieval]   \n",
       "\n",
       "                                                       TaskName  \n",
       "11063816  [six-alternative forced-choice cued-recognition task]  \n",
       "11063816                            [Episodic memory retrieval]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Diff on TaskName\n",
    "\n",
    "has_taskname_df = combined_df[combined_df.index.isin(has_task_name)]\n",
    "\n",
    "diff_taskname = all_scores_df[all_scores_df['TaskName'] < 0.8].index.tolist()\n",
    "\n",
    "if diff_taskname:\n",
    "    _display(has_taskname_df, diff_taskname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmcid</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>Condition</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2241626</th>\n",
       "      <td>2241626</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[mental calculation task, language comprehension task]</td>\n",
       "      <td>[In the present research, our goal was to define a simple fMRI test, less than 5 minutes long, that could delineate, in a subject-specific manner, those cerebral circuits. A functional sequence was added to each functional imaging session performed in our lab (Figure  ), taking advantage of the ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686646</th>\n",
       "      <td>2686646</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[one-back task]</td>\n",
       "      <td>None</td>\n",
       "      <td>[words, objects, scrambled objects]</td>\n",
       "      <td>[words¬†&gt;¬†fixation, objects¬†&gt;¬†scrambled objects]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3078751</th>\n",
       "      <td>3078751</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>delavega-other</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[movement observation paradigm]</td>\n",
       "      <td>[participants had to register auditory cues that determined what movement the dancer was to perform next, and watch the ensuing movements, participants watched a dancer performing according to cue, but occasionally making mistakes. Previous to playing the task, the participants were instructed o...</td>\n",
       "      <td>[predicted, prediction-violating movements, correct movements, incorrect movements, first dissimilar transition cues, switch cues, repetition cues, null events]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4115625</th>\n",
       "      <td>4115625</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>delavega-other</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Voice localizer paradigm]</td>\n",
       "      <td>[The voice localizer includes a variety of different vocal sounds, not exclusively female or male voices, but also speech and non-speech of women, men and infants and non-vocal sounds (e.g., environmental sounds), Subjects were instructed to close their eyes and passively listen to a large varie...</td>\n",
       "      <td>[vocal, non-vocal ]</td>\n",
       "      <td>[vocal vs. non-vocal, male &gt; female, female &gt; male]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179768</th>\n",
       "      <td>4179768</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[single food choice task]</td>\n",
       "      <td>[During the functional MRI scan, participants performed a food choice task (Figure  ). In this ask, participants made 100 choices. In every trial, they viewed one of the study stimuli (3000 ms, choice period) and subsequently had to indicate with a button press (1500 ms, button press period) whe...</td>\n",
       "      <td>[HE choice periods, LE choice periods, button press screen, practice, missed]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9308012</th>\n",
       "      <td>9308012</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Trust Game]</td>\n",
       "      <td>[Participants played an economic trust game as the investor with three partners (friend, stranger, and computer) who played the role of investee.]</td>\n",
       "      <td>[Reciprocate, Defect]</td>\n",
       "      <td>[Reciprocate &gt; Defect, Friends &gt; Strangers, Friends &gt; Computers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9454014</th>\n",
       "      <td>9454014</td>\n",
       "      <td>[RestingState, Task-based]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Food Cue Reactivity Task]</td>\n",
       "      <td>[The Food Cue Reactivity Task was modified in-house from the Alcohol Cue Reactivity Task. It consisted of 44 food images [22 high energy-dense foods (HED), e.g., ice cream, cookies; 22 low energy-dense foods (LED), e.g., salad, fruit] matched on valence, arousal, image complexity, brightness, an...</td>\n",
       "      <td>[deflation, sham-deflation]</td>\n",
       "      <td>[food images vs. degraded images, deflation vs. sham-deflation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9837608</th>\n",
       "      <td>9837608</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[movement synchronization]</td>\n",
       "      <td>[Participants were scanned while engaged in movement synchronization, using a novel dyadic interaction paradigm.]</td>\n",
       "      <td>[random, free, sync]</td>\n",
       "      <td>[sync &gt; random, free &gt; random]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910278</th>\n",
       "      <td>9910278</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Emotion Reappraisal, Dietary Self-Control]</td>\n",
       "      <td>[Participants were presented with emotional stimuli and instructed to either view or reappraise the content to regulate their feelings. They rated their emotions after each trial., Participants made choices about whether to eat presented food items, which were tailored based on individual taste ...</td>\n",
       "      <td>[Reappraise, View, Healthy, Unhealthy]</td>\n",
       "      <td>[Reappraisal Success &gt; View, Self-Control Success &gt; No Challenge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9949505</th>\n",
       "      <td>9949505</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Social Incentive Delay Task]</td>\n",
       "      <td>[Participants completed the Social Incentive Delay Task while undergoing fMRI to measure neural responses to anticipating social rewards and threats. Each trial began with a cue that signaled whether the upcoming image was a potential reward, threat, or neutral.]</td>\n",
       "      <td>[reward, threat, neutral]</td>\n",
       "      <td>[threat vs neutral, reward vs neutral]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pmcid                  DesignType         annotator_name Exclude  \\\n",
       "2241626  2241626                [Task-based]  delavega-aliceoverlap    None   \n",
       "2686646  2686646                [Task-based]            delavega_nv    None   \n",
       "3078751  3078751                [Task-based]         delavega-other    None   \n",
       "4115625  4115625                [Task-based]         delavega-other    None   \n",
       "4179768  4179768                [Task-based]            delavega_nv    None   \n",
       "...          ...                         ...                    ...     ...   \n",
       "9308012  9308012                [Task-based]                    NaN    None   \n",
       "9454014  9454014  [RestingState, Task-based]                    NaN    None   \n",
       "9837608  9837608                [Task-based]                    NaN    None   \n",
       "9910278  9910278                [Task-based]                    NaN    None   \n",
       "9949505  9949505                [Task-based]                    NaN    None   \n",
       "\n",
       "            Modality                                                TaskName  \\\n",
       "2241626  [fMRI-BOLD]  [mental calculation task, language comprehension task]   \n",
       "2686646  [fMRI-BOLD]                                         [one-back task]   \n",
       "3078751  [fMRI-BOLD]                         [movement observation paradigm]   \n",
       "4115625  [fMRI-BOLD]                              [Voice localizer paradigm]   \n",
       "4179768  [fMRI-BOLD]                               [single food choice task]   \n",
       "...              ...                                                     ...   \n",
       "9308012  [fMRI-BOLD]                                            [Trust Game]   \n",
       "9454014  [fMRI-BOLD]                              [Food Cue Reactivity Task]   \n",
       "9837608  [fMRI-BOLD]                              [movement synchronization]   \n",
       "9910278  [fMRI-BOLD]             [Emotion Reappraisal, Dietary Self-Control]   \n",
       "9949505  [fMRI-BOLD]                           [Social Incentive Delay Task]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "2241626  [In the present research, our goal was to define a simple fMRI test, less than 5 minutes long, that could delineate, in a subject-specific manner, those cerebral circuits. A functional sequence was added to each functional imaging session performed in our lab (Figure  ), taking advantage of the ...   \n",
       "2686646                                                                                                                                                                                                                                                                                                         None   \n",
       "3078751  [participants had to register auditory cues that determined what movement the dancer was to perform next, and watch the ensuing movements, participants watched a dancer performing according to cue, but occasionally making mistakes. Previous to playing the task, the participants were instructed o...   \n",
       "4115625  [The voice localizer includes a variety of different vocal sounds, not exclusively female or male voices, but also speech and non-speech of women, men and infants and non-vocal sounds (e.g., environmental sounds), Subjects were instructed to close their eyes and passively listen to a large varie...   \n",
       "4179768  [During the functional MRI scan, participants performed a food choice task (Figure  ). In this ask, participants made 100 choices. In every trial, they viewed one of the study stimuli (3000 ms, choice period) and subsequently had to indicate with a button press (1500 ms, button press period) whe...   \n",
       "...                                                                                                                                                                                                                                                                                                              ...   \n",
       "9308012                                                                                                                                                           [Participants played an economic trust game as the investor with three partners (friend, stranger, and computer) who played the role of investee.]   \n",
       "9454014  [The Food Cue Reactivity Task was modified in-house from the Alcohol Cue Reactivity Task. It consisted of 44 food images [22 high energy-dense foods (HED), e.g., ice cream, cookies; 22 low energy-dense foods (LED), e.g., salad, fruit] matched on valence, arousal, image complexity, brightness, an...   \n",
       "9837608                                                                                                                                                                                            [Participants were scanned while engaged in movement synchronization, using a novel dyadic interaction paradigm.]   \n",
       "9910278  [Participants were presented with emotional stimuli and instructed to either view or reappraise the content to regulate their feelings. They rated their emotions after each trial., Participants made choices about whether to eat presented food items, which were tailored based on individual taste ...   \n",
       "9949505                                      [Participants completed the Social Incentive Delay Task while undergoing fMRI to measure neural responses to anticipating social rewards and threats. Each trial began with a cue that signaled whether the upcoming image was a potential reward, threat, or neutral.]   \n",
       "\n",
       "                                                                                                                                                                Condition  \\\n",
       "2241626                                                                                                                                                              None   \n",
       "2686646                                                                                                                               [words, objects, scrambled objects]   \n",
       "3078751  [predicted, prediction-violating movements, correct movements, incorrect movements, first dissimilar transition cues, switch cues, repetition cues, null events]   \n",
       "4115625                                                                                                                                               [vocal, non-vocal ]   \n",
       "4179768                                                                                     [HE choice periods, LE choice periods, button press screen, practice, missed]   \n",
       "...                                                                                                                                                                   ...   \n",
       "9308012                                                                                                                                             [Reciprocate, Defect]   \n",
       "9454014                                                                                                                                       [deflation, sham-deflation]   \n",
       "9837608                                                                                                                                              [random, free, sync]   \n",
       "9910278                                                                                                                            [Reappraise, View, Healthy, Unhealthy]   \n",
       "9949505                                                                                                                                         [reward, threat, neutral]   \n",
       "\n",
       "                                                        ContrastDefinition  \n",
       "2241626                                                               None  \n",
       "2686646                    [words¬†>¬†fixation, objects¬†>¬†scrambled objects]  \n",
       "3078751                                                               None  \n",
       "4115625                [vocal vs. non-vocal, male > female, female > male]  \n",
       "4179768                                                               None  \n",
       "...                                                                    ...  \n",
       "9308012   [Reciprocate > Defect, Friends > Strangers, Friends > Computers]  \n",
       "9454014    [food images vs. degraded images, deflation vs. sham-deflation]  \n",
       "9837608                                     [sync > random, free > random]  \n",
       "9910278  [Reappraisal Success > View, Self-Control Success > No Challenge]  \n",
       "9949505                             [threat vs neutral, reward vs neutral]  \n",
       "\n",
       "[128 rows x 9 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_taskname_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3825257</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825257</th>\n",
       "      <td>[task-unrelated thought, comprehension]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>[Participants read three excerpts from the official translation of Bill Bryson's 'A Short History of Everything' on a computer screen in an individual testing booth. Text was presented one sentence at a time and participants received experience sampling probes at random intervals.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[better comprehension vs. worse comprehension, higher task focus vs. lower task focus]</td>\n",
       "      <td>[reading]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Condition     Modality      DesignType  \\\n",
       "3825257                                     None  [fMRI-BOLD]  [RestingState]   \n",
       "3825257  [task-unrelated thought, comprehension]  [fMRI-BOLD]  [RestingState]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                    TaskDescription  \\\n",
       "3825257                                                                                                                                                                                                                                                                                        None   \n",
       "3825257  [Participants read three excerpts from the official translation of Bill Bryson's 'A Short History of Everything' on a computer screen in an individual testing booth. Text was presented one sentence at a time and participants received experience sampling probes at random intervals.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "3825257    delavega_nv    None   \n",
       "3825257            NaN    None   \n",
       "\n",
       "                                                                             ContrastDefinition  \\\n",
       "3825257                                                                                    None   \n",
       "3825257  [better comprehension vs. worse comprehension, higher task focus vs. lower task focus]   \n",
       "\n",
       "          TaskName  \n",
       "3825257         []  \n",
       "3825257  [reading]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4110030</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4110030</th>\n",
       "      <td>[Menstrual phase (day 24), Follicular phase (day 8-12), Luteal phase (day 20-22)]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>[Participants completed three sessions of resting state fMRI. They were instructed to relax and keep their eyes closed during scanning.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Women &gt; Men in right dorsal network, Women &gt; Men in anterior network]</td>\n",
       "      <td>[Resting State fMRI]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 Condition  \\\n",
       "4110030                                                                               None   \n",
       "4110030  [Menstrual phase (day 24), Follicular phase (day 8-12), Luteal phase (day 20-22)]   \n",
       "\n",
       "            Modality      DesignType  \\\n",
       "4110030  [fMRI-BOLD]  [RestingState]   \n",
       "4110030  [fMRI-BOLD]  [RestingState]   \n",
       "\n",
       "                                                                                                                                  TaskDescription  \\\n",
       "4110030                                                                                                                                      None   \n",
       "4110030  [Participants completed three sessions of resting state fMRI. They were instructed to relax and keep their eyes closed during scanning.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "4110030    delavega_nv    None   \n",
       "4110030            NaN    None   \n",
       "\n",
       "                                                             ContrastDefinition  \\\n",
       "4110030                                                                    None   \n",
       "4110030  [Women > Men in right dorsal network, Women > Men in anterior network]   \n",
       "\n",
       "                     TaskName  \n",
       "4110030                    []  \n",
       "4110030  [Resting State fMRI]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4386762</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386762</th>\n",
       "      <td>[Healthy elderly, Alzheimer's Disease (AD), Mild Cognitive Impairment (MCI)]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>[Subjects were instructed to keep their eyes closed, to not think of anything in particular and to refrain from falling asleep.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Healthy elderly vs. Alzheimer's Disease, Healthy elderly vs. Mild Cognitive Impairment, Alzheimer's Disease vs. Mild Cognitive Impairment]</td>\n",
       "      <td>[Resting state fMRI]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            Condition  \\\n",
       "4386762                                                                          None   \n",
       "4386762  [Healthy elderly, Alzheimer's Disease (AD), Mild Cognitive Impairment (MCI)]   \n",
       "\n",
       "            Modality      DesignType  \\\n",
       "4386762  [fMRI-BOLD]  [RestingState]   \n",
       "4386762  [fMRI-BOLD]  [RestingState]   \n",
       "\n",
       "                                                                                                                          TaskDescription  \\\n",
       "4386762                                                                                                                              None   \n",
       "4386762  [Subjects were instructed to keep their eyes closed, to not think of anything in particular and to refrain from falling asleep.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "4386762    delavega_nv    None   \n",
       "4386762            NaN    None   \n",
       "\n",
       "                                                                                                                                  ContrastDefinition  \\\n",
       "4386762                                                                                                                                         None   \n",
       "4386762  [Healthy elderly vs. Alzheimer's Disease, Healthy elderly vs. Mild Cognitive Impairment, Alzheimer's Disease vs. Mild Cognitive Impairment]   \n",
       "\n",
       "                     TaskName  \n",
       "4386762                    []  \n",
       "4386762  [Resting state fMRI]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4488375</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[The task used in this experiment was programmed using PsychoPy [ ]. The task featured a 0-back and a 1-back condition that continuously switched from one another throughout the experimental session (see  ). Our paradigm is broadly similar to the paradigm used by Smallwood and colleagues [ ] and...</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488375</th>\n",
       "      <td>[0-back, 1-back]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants alternated between two tasks. One task involved observing non-coloured shapes presented at fixation waiting for the presentation of a coloured slide at which point they would indicate using a button press which side of the fixation cross a target shape was (0-back). In the other ta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[1-back &gt; 0-back sustained responses, 1-back &gt; 0-back target transient responses]</td>\n",
       "      <td>[0-back, 1-back]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Condition     Modality    DesignType  \\\n",
       "4488375              None  [fMRI-BOLD]  [Task-based]   \n",
       "4488375  [0-back, 1-back]  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "4488375  [The task used in this experiment was programmed using PsychoPy [ ]. The task featured a 0-back and a 1-back condition that continuously switched from one another throughout the experimental session (see  ). Our paradigm is broadly similar to the paradigm used by Smallwood and colleagues [ ] and...   \n",
       "4488375  [Participants alternated between two tasks. One task involved observing non-coloured shapes presented at fixation waiting for the presentation of a coloured slide at which point they would indicate using a button press which side of the fixation cross a target shape was (0-back). In the other ta...   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "4488375    delavega_nv    None   \n",
       "4488375            NaN    None   \n",
       "\n",
       "                                                                        ContrastDefinition  \\\n",
       "4488375                                                                               None   \n",
       "4488375  [1-back > 0-back sustained responses, 1-back > 0-back target transient responses]   \n",
       "\n",
       "                 TaskName  \n",
       "4488375             [n/a]  \n",
       "4488375  [0-back, 1-back]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4526228</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD, DiffusionMRI]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4526228</th>\n",
       "      <td>[Low-intensity physical activity (LI-PA), Moderate-to-vigorous physical activity (MV-PA)]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>[Participants were asked to lie still with eyes closed during the resting state fMRI acquisition.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[LI-PA and MV-PA &gt; Sedentary behavior]</td>\n",
       "      <td>[Resting State]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         Condition  \\\n",
       "4526228                                                                                       None   \n",
       "4526228  [Low-intensity physical activity (LI-PA), Moderate-to-vigorous physical activity (MV-PA)]   \n",
       "\n",
       "                          Modality      DesignType  \\\n",
       "4526228  [fMRI-BOLD, DiffusionMRI]  [RestingState]   \n",
       "4526228                [fMRI-BOLD]  [RestingState]   \n",
       "\n",
       "                                                                                            TaskDescription  \\\n",
       "4526228                                                                                                None   \n",
       "4526228  [Participants were asked to lie still with eyes closed during the resting state fMRI acquisition.]   \n",
       "\n",
       "        annotator_name Exclude                      ContrastDefinition  \\\n",
       "4526228    delavega_nv    None                                    None   \n",
       "4526228            NaN    None  [LI-PA and MV-PA > Sedentary behavior]   \n",
       "\n",
       "                TaskName  \n",
       "4526228               []  \n",
       "4526228  [Resting State]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4547715</th>\n",
       "      <td>[liked, uninteresting, romantic, non-romantic]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[A) The task was conducted in an event-related design. Six trials from either the liked or the uninteresting male acquaintance were combined in a set. Each set contained pseudo-randomized four target trials (shown in red) with two control trials (shown in blue). (B) Each trial was created by a 2...</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547715</th>\n",
       "      <td>[Liked giver vs. Uninteresting giver, Romantic gifts vs. Non-romantic gifts]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants judged the attractiveness of gifts received from two male acquaintances, one preferred and one uninteresting, on a 9-point scale.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Liked giver &gt; Uninteresting giver, Romantic gifts &gt; Non-romantic gifts from liked giver, Romantic gifts &gt; Non-romantic gifts from uninteresting giver]</td>\n",
       "      <td>[Gift attractiveness judgment]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            Condition  \\\n",
       "4547715                                [liked, uninteresting, romantic, non-romantic]   \n",
       "4547715  [Liked giver vs. Uninteresting giver, Romantic gifts vs. Non-romantic gifts]   \n",
       "\n",
       "            Modality    DesignType  \\\n",
       "4547715  [fMRI-BOLD]  [Task-based]   \n",
       "4547715  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "4547715  [A) The task was conducted in an event-related design. Six trials from either the liked or the uninteresting male acquaintance were combined in a set. Each set contained pseudo-randomized four target trials (shown in red) with two control trials (shown in blue). (B) Each trial was created by a 2...   \n",
       "4547715                                                                                                                                                             [Participants judged the attractiveness of gifts received from two male acquaintances, one preferred and one uninteresting, on a 9-point scale.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "4547715    delavega_nv    None   \n",
       "4547715            NaN    None   \n",
       "\n",
       "                                                                                                                                              ContrastDefinition  \\\n",
       "4547715                                                                                                                                                     None   \n",
       "4547715  [Liked giver > Uninteresting giver, Romantic gifts > Non-romantic gifts from liked giver, Romantic gifts > Non-romantic gifts from uninteresting giver]   \n",
       "\n",
       "                               TaskName  \n",
       "4547715                           [n/a]  \n",
       "4547715  [Gift attractiveness judgment]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4914983</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4914983</th>\n",
       "      <td>[First viewing, Second viewing]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Three comedy clips were shown twice to 20 volunteers during functional magnetic resonance imaging (fMRI).]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[First viewing &gt; Second viewing, Humorousness ratings]</td>\n",
       "      <td>[Viewing and re-viewing of comedy movies]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Condition     Modality    DesignType  \\\n",
       "4914983                             None  [fMRI-BOLD]  [Task-based]   \n",
       "4914983  [First viewing, Second viewing]  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                     TaskDescription  \\\n",
       "4914983                                                                                                         None   \n",
       "4914983  [Three comedy clips were shown twice to 20 volunteers during functional magnetic resonance imaging (fMRI).]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "4914983    delavega_nv    None   \n",
       "4914983            NaN    None   \n",
       "\n",
       "                                             ContrastDefinition  \\\n",
       "4914983                                                    None   \n",
       "4914983  [First viewing > Second viewing, Humorousness ratings]   \n",
       "\n",
       "                                          TaskName  \n",
       "4914983                                      [n/a]  \n",
       "4914983  [Viewing and re-viewing of comedy movies]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5090046</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090046</th>\n",
       "      <td>[High frequency, Low frequency, High imageability, Medium imageability, Low imageability, Category fluency, Letter fluency]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState, Task-based]</td>\n",
       "      <td>[Participants performed a synonym judgement task to index the capacity to understand the meaning of an external stimulus., Participants had 1min to generate as many unique words as possible belonging to a semantic category (category fluency) or starting with a specific letter (letter fluency).]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[High frequency vs. Low frequency, High imageability vs. Low imageability, Category fluency vs. Letter fluency]</td>\n",
       "      <td>[Synonym Judgement, Verbal Fluency]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           Condition  \\\n",
       "5090046                                                                                                                         None   \n",
       "5090046  [High frequency, Low frequency, High imageability, Medium imageability, Low imageability, Category fluency, Letter fluency]   \n",
       "\n",
       "            Modality                  DesignType  \\\n",
       "5090046  [fMRI-BOLD]              [RestingState]   \n",
       "5090046  [fMRI-BOLD]  [RestingState, Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                 TaskDescription  \\\n",
       "5090046                                                                                                                                                                                                                                                                                                     None   \n",
       "5090046  [Participants performed a synonym judgement task to index the capacity to understand the meaning of an external stimulus., Participants had 1min to generate as many unique words as possible belonging to a semantic category (category fluency) or starting with a specific letter (letter fluency).]   \n",
       "\n",
       "                annotator_name Exclude  \\\n",
       "5090046  delavega-aliceoverlap    None   \n",
       "5090046                    NaN    None   \n",
       "\n",
       "                                                                                                      ContrastDefinition  \\\n",
       "5090046                                                                                                             None   \n",
       "5090046  [High frequency vs. Low frequency, High imageability vs. Low imageability, Category fluency vs. Letter fluency]   \n",
       "\n",
       "                                    TaskName  \n",
       "5090046                                   []  \n",
       "5090046  [Synonym Judgement, Verbal Fluency]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5552726</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega-other</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552726</th>\n",
       "      <td>[Asymptomatic carotid artery stenosis patients undergoing CAS before and after intervention.]</td>\n",
       "      <td>[fMRI-BOLD, StructuralMRI]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>[Cognition assessment including the Montreal Cognitive Assessment Beijing Version, the Minimum Mental State Examination, the Digit Symbol Test, the Rey Auditory Verbal Learning Test, and the Verbal Memory Test.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[3 months after CAS vs. baseline]</td>\n",
       "      <td>[Cognition assessment]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             Condition  \\\n",
       "5552726                                                                                           None   \n",
       "5552726  [Asymptomatic carotid artery stenosis patients undergoing CAS before and after intervention.]   \n",
       "\n",
       "                           Modality      DesignType  \\\n",
       "5552726                 [fMRI-BOLD]  [RestingState]   \n",
       "5552726  [fMRI-BOLD, StructuralMRI]  [RestingState]   \n",
       "\n",
       "                                                                                                                                                                                                             TaskDescription  \\\n",
       "5552726                                                                                                                                                                                                                 None   \n",
       "5552726  [Cognition assessment including the Montreal Cognitive Assessment Beijing Version, the Minimum Mental State Examination, the Digit Symbol Test, the Rey Auditory Verbal Learning Test, and the Verbal Memory Test.]   \n",
       "\n",
       "         annotator_name Exclude                 ContrastDefinition  \\\n",
       "5552726  delavega-other    None                               None   \n",
       "5552726             NaN    None  [3 months after CAS vs. baseline]   \n",
       "\n",
       "                       TaskName  \n",
       "5552726                      []  \n",
       "5552726  [Cognition assessment]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5607552</th>\n",
       "      <td>[distorted sentences, intact sentences, second presentation of the distorted sentences]</td>\n",
       "      <td>[fMRI-BOLD, fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[a degraded sentence is first experienced as very difficult to understand and, after a single presentation of its intact counterpart, the intelligibility of this same degraded sentence reaches near‚Äêperfect level., functional run consisted of 9 D‚ÄêI‚ÄêD stimulus sets, each of which comprised three b...</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>[second distorted versus first distorted, intact versus first distorted, intact versus second distorted]</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5607552</th>\n",
       "      <td>[Distorted sentences before intact presentation, Intact sentences, Distorted sentences after intact presentation]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Subjects listened to acoustically distorted sentences, followed by intact versions, and then the distorted sentences were presented again.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Second distorted vs. first distorted, Intact vs. first distorted, Intact vs. second distorted]</td>\n",
       "      <td>[Speech Comprehension]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 Condition  \\\n",
       "5607552                            [distorted sentences, intact sentences, second presentation of the distorted sentences]   \n",
       "5607552  [Distorted sentences before intact presentation, Intact sentences, Distorted sentences after intact presentation]   \n",
       "\n",
       "                       Modality    DesignType  \\\n",
       "5607552  [fMRI-BOLD, fMRI-BOLD]  [Task-based]   \n",
       "5607552             [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "5607552  [a degraded sentence is first experienced as very difficult to understand and, after a single presentation of its intact counterpart, the intelligibility of this same degraded sentence reaches near‚Äêperfect level., functional run consisted of 9 D‚ÄêI‚ÄêD stimulus sets, each of which comprised three b...   \n",
       "5607552                                                                                                                                                                 [Subjects listened to acoustically distorted sentences, followed by intact versions, and then the distorted sentences were presented again.]   \n",
       "\n",
       "                annotator_name Exclude  \\\n",
       "5607552  delavega-aliceoverlap    None   \n",
       "5607552                    NaN    None   \n",
       "\n",
       "                                                                                               ContrastDefinition  \\\n",
       "5607552  [second distorted versus first distorted, intact versus first distorted, intact versus second distorted]   \n",
       "5607552           [Second distorted vs. first distorted, Intact vs. first distorted, Intact vs. second distorted]   \n",
       "\n",
       "                       TaskName  \n",
       "5607552                   [n/a]  \n",
       "5607552  [Speech Comprehension]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5662713</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants watched 24 short videos while in the MRI scanner. The videos lasted on average 38‚Äâseconds (range 29‚Äì48‚Äâs) and were taken from short films or videos posted on   www.YouTube.com  . All videos depicted a short narrative and were presented without sound. The stories centered around one...</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5662713</th>\n",
       "      <td>[Encoding, Immediate Retrieval, Delayed Retrieval]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants watched 24 short videos while in the MRI scanner. The task involved encoding the videos, immediate retrieval, and delayed retrieval after one week.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Enc/ImRet, Enc/DelRet, ImRet/DelRet]</td>\n",
       "      <td>[Memory retrieval]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Condition     Modality  \\\n",
       "5662713                                                None  [fMRI-BOLD]   \n",
       "5662713  [Encoding, Immediate Retrieval, Delayed Retrieval]  [fMRI-BOLD]   \n",
       "\n",
       "           DesignType  \\\n",
       "5662713  [Task-based]   \n",
       "5662713  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "5662713  [Participants watched 24 short videos while in the MRI scanner. The videos lasted on average 38‚Äâseconds (range 29‚Äì48‚Äâs) and were taken from short films or videos posted on   www.YouTube.com  . All videos depicted a short narrative and were presented without sound. The stories centered around one...   \n",
       "5662713                                                                                                                                           [Participants watched 24 short videos while in the MRI scanner. The task involved encoding the videos, immediate retrieval, and delayed retrieval after one week.]   \n",
       "\n",
       "        annotator_name Exclude                     ContrastDefinition  \\\n",
       "5662713    delavega_nv    None                                   None   \n",
       "5662713            NaN    None  [Enc/ImRet, Enc/DelRet, ImRet/DelRet]   \n",
       "\n",
       "                   TaskName  \n",
       "5662713               [n/a]  \n",
       "5662713  [Memory retrieval]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5716095</th>\n",
       "      <td>[family, self]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5716095</th>\n",
       "      <td>[Self-condition, Family-condition]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants saw individuals expressing emotional distress in negative contexts and were instructed to perceive the target face as either their family member or themselves.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[self &gt; family]</td>\n",
       "      <td>[Empathy Task]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Condition     Modality    DesignType  \\\n",
       "5716095                      [family, self]  [fMRI-BOLD]  [Task-based]   \n",
       "5716095  [Self-condition, Family-condition]  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                        TaskDescription  \\\n",
       "5716095                                                                                                                                                                            None   \n",
       "5716095  [Participants saw individuals expressing emotional distress in negative contexts and were instructed to perceive the target face as either their family member or themselves.]   \n",
       "\n",
       "        annotator_name Exclude ContrastDefinition        TaskName  \n",
       "5716095    delavega_nv    None               None           [n/a]  \n",
       "5716095            NaN    None    [self > family]  [Empathy Task]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5895040</th>\n",
       "      <td>[confidence, follow]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[In the perceptual task, subjects were asked to report the brighter of two stimuli on each trial, In the memory task, subjects began each miniblock by learning a set of nine consecutively presented stimuli]</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[n/a, n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5895040</th>\n",
       "      <td>[confidence, follow]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[In the perceptual task, subjects were asked to report the brighter of two stimuli on each trial., In the memory task, subjects began each miniblock by learning a set of nine consecutively presented stimuli. A stimulus from this set was then presented on each subsequent trial (in randomized orde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[memory confidence &gt; memory follow, perception confidence &gt; perception follow]</td>\n",
       "      <td>[perceptual task, memory task]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Condition     Modality    DesignType  \\\n",
       "5895040  [confidence, follow]  [fMRI-BOLD]  [Task-based]   \n",
       "5895040  [confidence, follow]  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "5895040                                                                                               [In the perceptual task, subjects were asked to report the brighter of two stimuli on each trial, In the memory task, subjects began each miniblock by learning a set of nine consecutively presented stimuli]   \n",
       "5895040  [In the perceptual task, subjects were asked to report the brighter of two stimuli on each trial., In the memory task, subjects began each miniblock by learning a set of nine consecutively presented stimuli. A stimulus from this set was then presented on each subsequent trial (in randomized orde...   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "5895040    delavega_nv    None   \n",
       "5895040            NaN    None   \n",
       "\n",
       "                                                                     ContrastDefinition  \\\n",
       "5895040                                                                            None   \n",
       "5895040  [memory confidence > memory follow, perception confidence > perception follow]   \n",
       "\n",
       "                               TaskName  \n",
       "5895040                      [n/a, n/a]  \n",
       "5895040  [perceptual task, memory task]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6102316</th>\n",
       "      <td>[unresolved, non-habitual, habitual, ‚Äúverbal‚Äù]</td>\n",
       "      <td>[fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6102316</th>\n",
       "      <td>[unresolved interactions, non-habitual interactions, habitual interactions, verbal interactions]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants watched a 20-min movie narrative encompassing verbal and non-verbal social interactions.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[verbal &gt; non-verbal, non-verbal &gt; verbal, unresolved &gt; habitual, habitual &gt; unresolved, non-habitual &gt; habitual, non-habitual &gt; unresolved]</td>\n",
       "      <td>[Naturalistic viewing of social interactions]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                Condition  \\\n",
       "6102316                                                    [unresolved, non-habitual, habitual, ‚Äúverbal‚Äù]   \n",
       "6102316  [unresolved interactions, non-habitual interactions, habitual interactions, verbal interactions]   \n",
       "\n",
       "                                                                                                    Modality  \\\n",
       "6102316  [fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD]   \n",
       "6102316                                                                                          [fMRI-BOLD]   \n",
       "\n",
       "           DesignType  \\\n",
       "6102316  [Task-based]   \n",
       "6102316  [Task-based]   \n",
       "\n",
       "                                                                                                 TaskDescription  \\\n",
       "6102316                                                                                                     None   \n",
       "6102316  [Participants watched a 20-min movie narrative encompassing verbal and non-verbal social interactions.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "6102316    delavega_nv    None   \n",
       "6102316            NaN    None   \n",
       "\n",
       "                                                                                                                                   ContrastDefinition  \\\n",
       "6102316                                                                                                                                          None   \n",
       "6102316  [verbal > non-verbal, non-verbal > verbal, unresolved > habitual, habitual > unresolved, non-habitual > habitual, non-habitual > unresolved]   \n",
       "\n",
       "                                              TaskName  \n",
       "6102316                                          [n/a]  \n",
       "6102316  [Naturalistic viewing of social interactions]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6331309</th>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331309</th>\n",
       "      <td>[Migraine, Restless Legs Syndrome, Comorbid Migraine and RLS, Healthy Controls]</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[High-resolution T1-weighted images were acquired from 116 subjects: 27 RLS patients, 22 migraine patients, 22 patients with comorbid migraine and RLS, and 45 healthy controls.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Migraine vs Healthy Controls, RLS vs Healthy Controls, Migraine vs RLS, Comorbid Migraine and RLS vs Healthy Controls]</td>\n",
       "      <td>[MRI data acquisition]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               Condition  \\\n",
       "6331309                                                                             None   \n",
       "6331309  [Migraine, Restless Legs Syndrome, Comorbid Migraine and RLS, Healthy Controls]   \n",
       "\n",
       "                Modality    DesignType  \\\n",
       "6331309  [StructuralMRI]            []   \n",
       "6331309  [StructuralMRI]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                           TaskDescription  \\\n",
       "6331309                                                                                                                                                                               None   \n",
       "6331309  [High-resolution T1-weighted images were acquired from 116 subjects: 27 RLS patients, 22 migraine patients, 22 patients with comorbid migraine and RLS, and 45 healthy controls.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "6331309    delavega_nv    None   \n",
       "6331309            NaN    None   \n",
       "\n",
       "                                                                                                              ContrastDefinition  \\\n",
       "6331309                                                                                                                     None   \n",
       "6331309  [Migraine vs Healthy Controls, RLS vs Healthy Controls, Migraine vs RLS, Comorbid Migraine and RLS vs Healthy Controls]   \n",
       "\n",
       "                       TaskName  \n",
       "6331309                      []  \n",
       "6331309  [MRI data acquisition]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6382839</th>\n",
       "      <td>[disorder-related , neutral narrative]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega-other</td>\n",
       "      <td>None</td>\n",
       "      <td>[disorder-related‚Äâ&gt;‚Äâneutral scripts]</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6382839</th>\n",
       "      <td>[disorder-related, neutral]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants were exposed to disorder-related and neutral narrative scripts while brain activation was measured with fMRI. They were encouraged to imagine the narrative scripts as vividly as possible.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[disorder-related vs. neutral scripts]</td>\n",
       "      <td>[script-driven imagery]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Condition     Modality    DesignType  \\\n",
       "6382839  [disorder-related , neutral narrative]  [fMRI-BOLD]  [Task-based]   \n",
       "6382839             [disorder-related, neutral]  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                    TaskDescription  \\\n",
       "6382839                                                                                                                                                                                                        None   \n",
       "6382839  [Participants were exposed to disorder-related and neutral narrative scripts while brain activation was measured with fMRI. They were encouraged to imagine the narrative scripts as vividly as possible.]   \n",
       "\n",
       "         annotator_name Exclude                      ContrastDefinition  \\\n",
       "6382839  delavega-other    None    [disorder-related‚Äâ>‚Äâneutral scripts]   \n",
       "6382839             NaN    None  [disorder-related vs. neutral scripts]   \n",
       "\n",
       "                        TaskName  \n",
       "6382839                    [n/a]  \n",
       "6382839  [script-driven imagery]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6391069</th>\n",
       "      <td>[mother‚Äôs voice, unfamiliar female voices, environmental sounds, catch trials]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Auditory stimuli were presented in 10 separate runs, each lasting 4 min. One run consisted of 56 trials of mother‚Äôs voice, unfamiliar female voices, environmental sounds and catch trials, which were pseudo-randomly ordered within each run. Stimulus presentation order was the same for each subje...</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>[unfamiliar female voices¬†&gt;¬†environmental sounds, mother‚Äôs voice¬†&gt;¬†unfamiliar female voices]</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6391069</th>\n",
       "      <td>[Mothers voice, Unfamiliar voices, Environmental sounds]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants listened to auditory stimuli including mothers voice, unfamiliar female voices, and environmental sounds while undergoing fMRI scanning.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Mothers voice &gt; Unfamiliar voices, Unfamiliar voices &gt; Environmental sounds]</td>\n",
       "      <td>[Voice Processing]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              Condition  \\\n",
       "6391069  [mother‚Äôs voice, unfamiliar female voices, environmental sounds, catch trials]   \n",
       "6391069                        [Mothers voice, Unfamiliar voices, Environmental sounds]   \n",
       "\n",
       "            Modality    DesignType  \\\n",
       "6391069  [fMRI-BOLD]  [Task-based]   \n",
       "6391069  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "6391069  [Auditory stimuli were presented in 10 separate runs, each lasting 4 min. One run consisted of 56 trials of mother‚Äôs voice, unfamiliar female voices, environmental sounds and catch trials, which were pseudo-randomly ordered within each run. Stimulus presentation order was the same for each subje...   \n",
       "6391069                                                                                                                                                      [Participants listened to auditory stimuli including mothers voice, unfamiliar female voices, and environmental sounds while undergoing fMRI scanning.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "6391069    delavega_nv    None   \n",
       "6391069            NaN    None   \n",
       "\n",
       "                                                                                   ContrastDefinition  \\\n",
       "6391069  [unfamiliar female voices¬†>¬†environmental sounds, mother‚Äôs voice¬†>¬†unfamiliar female voices]   \n",
       "6391069                 [Mothers voice > Unfamiliar voices, Unfamiliar voices > Environmental sounds]   \n",
       "\n",
       "                   TaskName  \n",
       "6391069               [n/a]  \n",
       "6391069  [Voice Processing]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6397754</th>\n",
       "      <td>[food, non-food]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[For this task, all participants were told that they would watch a clip from a nature documentary featuring Canadian bighorn sheep, and at times various words would appear and move around on the screen, but they were told to ‚Äúavoid reading these distractor words whenever they appear.‚Äù On average...</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>[words present vs. absent, words-versus-film]</td>\n",
       "      <td>[n/a, watched]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6397754</th>\n",
       "      <td>[Effortful self-control, Food cue exposure]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants were required to actively inhibit reading a series of words that appeared on the screen over the course of seven minutes., Participants engaged in a food-cue reactivity task involving food commercials.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[words present vs. absent, food events vs. control events]</td>\n",
       "      <td>[Effortful self-control task, Food cue reactivity task]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Condition     Modality  \\\n",
       "6397754                             [food, non-food]  [fMRI-BOLD]   \n",
       "6397754  [Effortful self-control, Food cue exposure]  [fMRI-BOLD]   \n",
       "\n",
       "           DesignType  \\\n",
       "6397754  [Task-based]   \n",
       "6397754  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "6397754  [For this task, all participants were told that they would watch a clip from a nature documentary featuring Canadian bighorn sheep, and at times various words would appear and move around on the screen, but they were told to ‚Äúavoid reading these distractor words whenever they appear.‚Äù On average...   \n",
       "6397754                                                                                     [Participants were required to actively inhibit reading a series of words that appeared on the screen over the course of seven minutes., Participants engaged in a food-cue reactivity task involving food commercials.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "6397754    delavega_nv    None   \n",
       "6397754            NaN    None   \n",
       "\n",
       "                                                 ContrastDefinition  \\\n",
       "6397754               [words present vs. absent, words-versus-film]   \n",
       "6397754  [words present vs. absent, food events vs. control events]   \n",
       "\n",
       "                                                        TaskName  \n",
       "6397754                                           [n/a, watched]  \n",
       "6397754  [Effortful self-control task, Food cue reactivity task]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6821801</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[During the event-related fMRI sessions, participants performed either purchasing or perceptual decisions in alternating blocks (Fig.¬† ). Each good was presented twice: once during the purchasing decision and once during the perceptual decision. Before each block started, both blocks were cued v...</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6821801</th>\n",
       "      <td>[Hedonic goods, Utilitarian goods]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants indicated their willingness to pay a specific price for that book during purchasing decisions., Participants indicated the total number of people and lines on the book cover during perceptual decisions.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Explicit hedonic values vs. explicit utilitarian values, Task-irrelevant hedonic values vs. task-irrelevant utilitarian values]</td>\n",
       "      <td>[Purchasing decisions, Perceptual decisions]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Condition     Modality    DesignType  \\\n",
       "6821801                                None  [fMRI-BOLD]  [Task-based]   \n",
       "6821801  [Hedonic goods, Utilitarian goods]  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "6821801  [During the event-related fMRI sessions, participants performed either purchasing or perceptual decisions in alternating blocks (Fig.¬† ). Each good was presented twice: once during the purchasing decision and once during the perceptual decision. Before each block started, both blocks were cued v...   \n",
       "6821801                                                                                    [Participants indicated their willingness to pay a specific price for that book during purchasing decisions., Participants indicated the total number of people and lines on the book cover during perceptual decisions.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "6821801    delavega_nv    None   \n",
       "6821801            NaN    None   \n",
       "\n",
       "                                                                                                                       ContrastDefinition  \\\n",
       "6821801                                                                                                                              None   \n",
       "6821801  [Explicit hedonic values vs. explicit utilitarian values, Task-irrelevant hedonic values vs. task-irrelevant utilitarian values]   \n",
       "\n",
       "                                             TaskName  \n",
       "6821801                                         [n/a]  \n",
       "6821801  [Purchasing decisions, Perceptual decisions]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7426775</th>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7426775</th>\n",
       "      <td>[Age prediction based on structural MRI.]</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Predicting chronological age from structural MRI scans using a deep learning model.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Age-related structural changes in the brain.]</td>\n",
       "      <td>[Brain Age Prediction]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Condition         Modality  \\\n",
       "7426775                                       None  [StructuralMRI]   \n",
       "7426775  [Age prediction based on structural MRI.]  [StructuralMRI]   \n",
       "\n",
       "           DesignType  \\\n",
       "7426775            []   \n",
       "7426775  [Task-based]   \n",
       "\n",
       "                                                                               TaskDescription  \\\n",
       "7426775                                                                                   None   \n",
       "7426775  [Predicting chronological age from structural MRI scans using a deep learning model.]   \n",
       "\n",
       "                annotator_name Exclude  \\\n",
       "7426775  delavega-aliceoverlap    None   \n",
       "7426775                    NaN    None   \n",
       "\n",
       "                                     ContrastDefinition  \\\n",
       "7426775                                            None   \n",
       "7426775  [Age-related structural changes in the brain.]   \n",
       "\n",
       "                       TaskName  \n",
       "7426775                      []  \n",
       "7426775  [Brain Age Prediction]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7582181</th>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7582181</th>\n",
       "      <td>[Social reward expectancy, Social threat expectancy]</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants were asked to vividly imagine themselves in a novel self-relevant event that was ambiguous with regards to possible social acceptance or rejection.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Social reward expectancy vs. social threat expectancy]</td>\n",
       "      <td>[Imagining social interactions]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Condition  \\\n",
       "7582181                                                  None   \n",
       "7582181  [Social reward expectancy, Social threat expectancy]   \n",
       "\n",
       "                Modality    DesignType  \\\n",
       "7582181  [StructuralMRI]            []   \n",
       "7582181  [StructuralMRI]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                            TaskDescription  \\\n",
       "7582181                                                                                                                                                                None   \n",
       "7582181  [Participants were asked to vividly imagine themselves in a novel self-relevant event that was ambiguous with regards to possible social acceptance or rejection.]   \n",
       "\n",
       "                annotator_name Exclude  \\\n",
       "7582181  delavega-aliceoverlap    None   \n",
       "7582181                    NaN    None   \n",
       "\n",
       "                                              ContrastDefinition  \\\n",
       "7582181                                                     None   \n",
       "7582181  [Social reward expectancy vs. social threat expectancy]   \n",
       "\n",
       "                                TaskName  \n",
       "7582181                               []  \n",
       "7582181  [Imagining social interactions]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7859438</th>\n",
       "      <td>[parallel, isolated, rest, auditory, visual, environmental sounds, vocal sounds, scene images, face images]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[It measures predominantly memory encoding, but also perception and attention in both the auditory and visual domains within 10 min of fMRI acquisition time using simple instructions. To our knowledge, memory-encoding paradigms so far presented stimuli of one sensory condition or did face‚Äìname a...</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>[auditory and visual blocks (c1), (c2) environmental sounds vs. vocal sounds masked by auditory activity greater than visual activity, (c3) face images vs. scene images masked by visual activity greater than auditory activit, c4) all (visual and auditory) hits vs. all misses (sensory-unspecific ...</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859438</th>\n",
       "      <td>[Auditory stimuli, Visual stimuli, Rest blocks]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[A functional MRI paradigm designed to map memory encoding across auditory and visual sensory conditions using a mixed design.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Auditory &gt; Visual, Environmental &gt; Vocal, Face &gt; Scene, Positive ESA (hits &gt; misses), Negative ESA (misses &gt; hits), Auditory &gt; rest, Visual &gt; rest, Auditory events &gt; rest, Visual events &gt; rest]</td>\n",
       "      <td>[Memory Encoding Task]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                           Condition  \\\n",
       "7859438  [parallel, isolated, rest, auditory, visual, environmental sounds, vocal sounds, scene images, face images]   \n",
       "7859438                                                              [Auditory stimuli, Visual stimuli, Rest blocks]   \n",
       "\n",
       "            Modality    DesignType  \\\n",
       "7859438  [fMRI-BOLD]  [Task-based]   \n",
       "7859438  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "7859438  [It measures predominantly memory encoding, but also perception and attention in both the auditory and visual domains within 10 min of fMRI acquisition time using simple instructions. To our knowledge, memory-encoding paradigms so far presented stimuli of one sensory condition or did face‚Äìname a...   \n",
       "7859438                                                                                                                                                                              [A functional MRI paradigm designed to map memory encoding across auditory and visual sensory conditions using a mixed design.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "7859438    delavega_nv    None   \n",
       "7859438            NaN    None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                  ContrastDefinition  \\\n",
       "7859438  [auditory and visual blocks (c1), (c2) environmental sounds vs. vocal sounds masked by auditory activity greater than visual activity, (c3) face images vs. scene images masked by visual activity greater than auditory activit, c4) all (visual and auditory) hits vs. all misses (sensory-unspecific ...   \n",
       "7859438                                                                                                           [Auditory > Visual, Environmental > Vocal, Face > Scene, Positive ESA (hits > misses), Negative ESA (misses > hits), Auditory > rest, Visual > rest, Auditory events > rest, Visual events > rest]   \n",
       "\n",
       "                       TaskName  \n",
       "7859438                   [n/a]  \n",
       "7859438  [Memory Encoding Task]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8104963</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[The main experimental session was organized in three blocks. In the first block, participants saw a fixation cross (1 s) followed by one video (3 s) and were asked to estimate the amount of pain experienced by the person in the video, by moving a randomly-presented rectangular cursor on a Visua...</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8104963</th>\n",
       "      <td>[Pain facial expressions]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants appraised the pain of facial expression video-clips, and subsequently were confronted with two feedbacks: the self-report of the person in pain and the average opinion of 20 medical practitioners.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Target &gt; MPs, MPs &gt; Target]</td>\n",
       "      <td>[Pain assessment]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Condition     Modality    DesignType  \\\n",
       "8104963                       None  [fMRI-BOLD]  [Task-based]   \n",
       "8104963  [Pain facial expressions]  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "8104963  [The main experimental session was organized in three blocks. In the first block, participants saw a fixation cross (1 s) followed by one video (3 s) and were asked to estimate the amount of pain experienced by the person in the video, by moving a randomly-presented rectangular cursor on a Visua...   \n",
       "8104963                                                                                          [Participants appraised the pain of facial expression video-clips, and subsequently were confronted with two feedbacks: the self-report of the person in pain and the average opinion of 20 medical practitioners.]   \n",
       "\n",
       "        annotator_name Exclude            ContrastDefinition  \\\n",
       "8104963    delavega_nv    None                          None   \n",
       "8104963            NaN    None  [Target > MPs, MPs > Target]   \n",
       "\n",
       "                  TaskName  \n",
       "8104963              [n/a]  \n",
       "8104963  [Pain assessment]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8443248</th>\n",
       "      <td>[genuine pain, genuine no pain, pretended pain, pretended no pain]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants were explicitly instructed to recreate the feelings of the demonstrators shown in the videos as vividly and intensely as possible. Based on the validation and pilot study, the painful   expressions   for the genuine and pretended conditions were matched. We also counterbalanced the...</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>[main effect of genuine: pain ‚Äì no pain, main effect of pretended: pain ‚Äì no pain, interaction: genuine (pain ‚Äì no pain) ‚Äì pretended (pain ‚Äì no pain)]</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443248</th>\n",
       "      <td>[Genuine pain, Pretended pain, Pain, No pain]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants watched video clips of people either feeling or pretending to feel pain.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[genuine: pain &gt; no pain, pretended: pain &gt; no pain, genuine (pain &gt; no pain) &gt; pretended (pain &gt; no pain)]</td>\n",
       "      <td>[Watch video clips of genuine vs. pretended pain]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  Condition  \\\n",
       "8443248  [genuine pain, genuine no pain, pretended pain, pretended no pain]   \n",
       "8443248                       [Genuine pain, Pretended pain, Pain, No pain]   \n",
       "\n",
       "            Modality    DesignType  \\\n",
       "8443248  [fMRI-BOLD]  [Task-based]   \n",
       "8443248  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "8443248  [Participants were explicitly instructed to recreate the feelings of the demonstrators shown in the videos as vividly and intensely as possible. Based on the validation and pilot study, the painful   expressions   for the genuine and pretended conditions were matched. We also counterbalanced the...   \n",
       "8443248                                                                                                                                                                                                                      [Participants watched video clips of people either feeling or pretending to feel pain.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "8443248    delavega_nv    None   \n",
       "8443248            NaN    None   \n",
       "\n",
       "                                                                                                                                             ContrastDefinition  \\\n",
       "8443248  [main effect of genuine: pain ‚Äì no pain, main effect of pretended: pain ‚Äì no pain, interaction: genuine (pain ‚Äì no pain) ‚Äì pretended (pain ‚Äì no pain)]   \n",
       "8443248                                             [genuine: pain > no pain, pretended: pain > no pain, genuine (pain > no pain) > pretended (pain > no pain)]   \n",
       "\n",
       "                                                  TaskName  \n",
       "8443248                                              [n/a]  \n",
       "8443248  [Watch video clips of genuine vs. pretended pain]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8564184</th>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8564184</th>\n",
       "      <td>[Chronic visceral pain, Healthy controls]</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Parallelized whole-brain voxel-based morphometry analyses in two patient cohorts with chronic visceral pain (ulcerative colitis in remission and irritable bowel syndrome) and healthy individuals.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[GMV in ulcerative colitis vs. healthy controls, GMV in irritable bowel syndrome vs. healthy controls, Correlations between GMV and symptom severity, Correlations between GMV and chronic stress]</td>\n",
       "      <td>[Voxel-based morphometry]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Condition         Modality  \\\n",
       "8564184                                       None  [StructuralMRI]   \n",
       "8564184  [Chronic visceral pain, Healthy controls]  [StructuralMRI]   \n",
       "\n",
       "           DesignType  \\\n",
       "8564184            []   \n",
       "8564184  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                               TaskDescription  \\\n",
       "8564184                                                                                                                                                                                                   None   \n",
       "8564184  [Parallelized whole-brain voxel-based morphometry analyses in two patient cohorts with chronic visceral pain (ulcerative colitis in remission and irritable bowel syndrome) and healthy individuals.]   \n",
       "\n",
       "                annotator_name Exclude  \\\n",
       "8564184  delavega-aliceoverlap    None   \n",
       "8564184                    NaN    None   \n",
       "\n",
       "                                                                                                                                                                                         ContrastDefinition  \\\n",
       "8564184                                                                                                                                                                                                None   \n",
       "8564184  [GMV in ulcerative colitis vs. healthy controls, GMV in irritable bowel syndrome vs. healthy controls, Correlations between GMV and symptom severity, Correlations between GMV and chronic stress]   \n",
       "\n",
       "                          TaskName  \n",
       "8564184                         []  \n",
       "8564184  [Voxel-based morphometry]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8927597</th>\n",
       "      <td>[2Back, Classification, SWr, SWi, RP]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8927597</th>\n",
       "      <td>[2Back-SWr, 2Back-SWi, 2Back-RP, Classification-SWr, Classification-SWi, Classification-RP]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants performed a 2-back updating task (2Back) and a classification task (Classification) with numbers or letters. The task involved classifying stimuli as odd/even or vowel/consonant and determining whether the current target was identical to the stimulus presented two trials before.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[(2Back-SWi&gt;2Back-SWr)&gt;(Classification-SWi&gt;Classification-SWr)]</td>\n",
       "      <td>[2-back updating task, classification task]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           Condition  \\\n",
       "8927597                                                        [2Back, Classification, SWr, SWi, RP]   \n",
       "8927597  [2Back-SWr, 2Back-SWi, 2Back-RP, Classification-SWr, Classification-SWi, Classification-RP]   \n",
       "\n",
       "            Modality    DesignType  \\\n",
       "8927597  [fMRI-BOLD]  [Task-based]   \n",
       "8927597  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                TaskDescription  \\\n",
       "8927597                                                                                                                                                                                                                                                                                                    None   \n",
       "8927597  [Participants performed a 2-back updating task (2Back) and a classification task (Classification) with numbers or letters. The task involved classifying stimuli as odd/even or vowel/consonant and determining whether the current target was identical to the stimulus presented two trials before.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "8927597    delavega_nv    None   \n",
       "8927597            NaN    None   \n",
       "\n",
       "                                                      ContrastDefinition  \\\n",
       "8927597                                                             None   \n",
       "8927597  [(2Back-SWi>2Back-SWr)>(Classification-SWi>Classification-SWr)]   \n",
       "\n",
       "                                            TaskName  \n",
       "8927597                                        [n/a]  \n",
       "8927597  [2-back updating task, classification task]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8975992</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>[vse_new‚Äâ&gt;‚Äâvpe_old, geo,‚Äâgroom‚Äâ&gt;‚Äâall non-ge]</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8975992</th>\n",
       "      <td>[Visual spatial information, Semantic spatial information]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[Participants watched the movie Forrest Gump and listened to its audio-description.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[vse_new &gt; vpe_old, geo, groom &gt; non-spatial]</td>\n",
       "      <td>[Watching a movie, Listening to audio-description]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          Condition  \\\n",
       "8975992                                                        None   \n",
       "8975992  [Visual spatial information, Semantic spatial information]   \n",
       "\n",
       "            Modality    DesignType  \\\n",
       "8975992  [fMRI-BOLD]  [Task-based]   \n",
       "8975992  [fMRI-BOLD]  [Task-based]   \n",
       "\n",
       "                                                                              TaskDescription  \\\n",
       "8975992                                                                                  None   \n",
       "8975992  [Participants watched the movie Forrest Gump and listened to its audio-description.]   \n",
       "\n",
       "        annotator_name Exclude                             ContrastDefinition  \\\n",
       "8975992    delavega_nv    None   [vse_new‚Äâ>‚Äâvpe_old, geo,‚Äâgroom‚Äâ>‚Äâall non-ge]   \n",
       "8975992            NaN    None  [vse_new > vpe_old, geo, groom > non-spatial]   \n",
       "\n",
       "                                                   TaskName  \n",
       "8975992                                               [n/a]  \n",
       "8975992  [Watching a movie, Listening to audio-description]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9202476</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD, DiffusionMRI]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202476</th>\n",
       "      <td>[poor sleepers (PSs), good sleepers (GSs)]</td>\n",
       "      <td>[fMRI-BOLD, DiffusionMRI]</td>\n",
       "      <td>[Task-based]</td>\n",
       "      <td>[In this computerized task, subjects were presented with a complex figure in the middle of the screen. Then, a few patterns were shown in the periphery, from which one was matched with the presented pattern. In the first trials, two patterns were presented in the periphery, and it was increased ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[PS &gt; GS, GS &gt; PS]</td>\n",
       "      <td>[match-to-sample (MTS) task]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Condition  \\\n",
       "9202476                                        None   \n",
       "9202476  [poor sleepers (PSs), good sleepers (GSs)]   \n",
       "\n",
       "                          Modality      DesignType  \\\n",
       "9202476  [fMRI-BOLD, DiffusionMRI]  [RestingState]   \n",
       "9202476  [fMRI-BOLD, DiffusionMRI]    [Task-based]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "9202476                                                                                                                                                                                                                                                                                                         None   \n",
       "9202476  [In this computerized task, subjects were presented with a complex figure in the middle of the screen. Then, a few patterns were shown in the periphery, from which one was matched with the presented pattern. In the first trials, two patterns were presented in the periphery, and it was increased ...   \n",
       "\n",
       "                annotator_name Exclude  ContrastDefinition  \\\n",
       "9202476  delavega-aliceoverlap    None                None   \n",
       "9202476                    NaN    None  [PS > GS, GS > PS]   \n",
       "\n",
       "                             TaskName  \n",
       "9202476                            []  \n",
       "9202476  [match-to-sample (MTS) task]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9729227</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9729227</th>\n",
       "      <td>[Attention networks]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>[We used 110 7T resting-state functional MRI datasets from the Human Connectome Project S1200. Images were preprocessed and registered to the MNI152 space as specified in the Human Connectome Project protocol.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ventral Attention Network (VAN) vs. Dorsal Attention Network (DAN)]</td>\n",
       "      <td>[Resting-state functional imaging]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Condition     Modality      DesignType  \\\n",
       "9729227                  None  [fMRI-BOLD]  [RestingState]   \n",
       "9729227  [Attention networks]  [fMRI-BOLD]  [RestingState]   \n",
       "\n",
       "                                                                                                                                                                                                            TaskDescription  \\\n",
       "9729227                                                                                                                                                                                                                None   \n",
       "9729227  [We used 110 7T resting-state functional MRI datasets from the Human Connectome Project S1200. Images were preprocessed and registered to the MNI152 space as specified in the Human Connectome Project protocol.]   \n",
       "\n",
       "        annotator_name Exclude  \\\n",
       "9729227    delavega_nv    None   \n",
       "9729227            NaN    None   \n",
       "\n",
       "                                                           ContrastDefinition  \\\n",
       "9729227                                                                  None   \n",
       "9729227  [Ventral Attention Network (VAN) vs. Dorsal Attention Network (DAN)]   \n",
       "\n",
       "                                   TaskName  \n",
       "9729227                                  []  \n",
       "9729227  [Resting-state functional imaging]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10028637</th>\n",
       "      <td>None</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[RestingState, Task-based]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10028637</th>\n",
       "      <td>[wakefulness, moderate anaesthesia]</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Task-based, RestingState]</td>\n",
       "      <td>[Participants performed a computerized auditory target detection task (50 trials) to assess individual responsiveness differences during moderate anaesthesia. Sound was presented, and participants pressed a button upon hearing an auditory beep., Participants listened to a plot-driven auditory na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[Responses during wakefulness vs. responses during moderate anaesthesia, Functional connectivity within and between the DAN, ECN, and DMN during wakefulness vs. moderate anaesthesia]</td>\n",
       "      <td>[auditory target detection task, narrative listening, resting state]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Condition     Modality  \\\n",
       "10028637                                 None  [fMRI-BOLD]   \n",
       "10028637  [wakefulness, moderate anaesthesia]  [fMRI-BOLD]   \n",
       "\n",
       "                          DesignType  \\\n",
       "10028637  [RestingState, Task-based]   \n",
       "10028637  [Task-based, RestingState]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      TaskDescription  \\\n",
       "10028637                                                                                                                                                                                                                                                                                                         None   \n",
       "10028637  [Participants performed a computerized auditory target detection task (50 trials) to assess individual responsiveness differences during moderate anaesthesia. Sound was presented, and participants pressed a button upon hearing an auditory beep., Participants listened to a plot-driven auditory na...   \n",
       "\n",
       "         annotator_name Exclude  \\\n",
       "10028637    delavega_nv    None   \n",
       "10028637            NaN    None   \n",
       "\n",
       "                                                                                                                                                                              ContrastDefinition  \\\n",
       "10028637                                                                                                                                                                                    None   \n",
       "10028637  [Responses during wakefulness vs. responses during moderate anaesthesia, Functional connectivity within and between the DAN, ECN, and DMN during wakefulness vs. moderate anaesthesia]   \n",
       "\n",
       "                                                                      TaskName  \n",
       "10028637                                                                 [n/a]  \n",
       "10028637  [auditory target detection task, narrative listening, resting state]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Modality</th>\n",
       "      <th>DesignType</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>ContrastDefinition</th>\n",
       "      <th>TaskName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10031743</th>\n",
       "      <td>None</td>\n",
       "      <td>[StructuralMRI, DiffusionMRI, fMRI-CBF]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>None</td>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10031743</th>\n",
       "      <td>[Patients with schizophrenia spectrum disorders]</td>\n",
       "      <td>[fMRI-CBF, StructuralMRI, DiffusionMRI]</td>\n",
       "      <td>[RestingState]</td>\n",
       "      <td>[This study tested whether four FTD dimensions differ in their association with brain perfusion and brain structure.]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[rsCBF and GMV associations with TALD subscales controlling for age, medication, total intracranial volume, and for variance of the 3 other TALD subscales.]</td>\n",
       "      <td>[Neural Correlates of Formal Thought Disorder Dimensions in Psychosis]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Condition  \\\n",
       "10031743                                              None   \n",
       "10031743  [Patients with schizophrenia spectrum disorders]   \n",
       "\n",
       "                                         Modality      DesignType  \\\n",
       "10031743  [StructuralMRI, DiffusionMRI, fMRI-CBF]  [RestingState]   \n",
       "10031743  [fMRI-CBF, StructuralMRI, DiffusionMRI]  [RestingState]   \n",
       "\n",
       "                                                                                                                TaskDescription  \\\n",
       "10031743                                                                                                                   None   \n",
       "10031743  [This study tested whether four FTD dimensions differ in their association with brain perfusion and brain structure.]   \n",
       "\n",
       "         annotator_name Exclude  \\\n",
       "10031743    delavega_nv    None   \n",
       "10031743            NaN    None   \n",
       "\n",
       "                                                                                                                                                    ContrastDefinition  \\\n",
       "10031743                                                                                                                                                          None   \n",
       "10031743  [rsCBF and GMV associations with TALD subscales controlling for age, medication, total intracranial volume, and for variance of the 3 other TALD subscales.]   \n",
       "\n",
       "                                                                        TaskName  \n",
       "10031743                                                                      []  \n",
       "10031743  [Neural Correlates of Formal Thought Disorder Dimensions in Psychosis]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Doesn't have TaskName (according to annotations)\n",
    "\n",
    "has_notaskname_df = combined_df[combined_df.index.isin(has_task_name) == False]\n",
    "\n",
    "diff_notaskname = all_scores_df[all_scores_df['TaskName'] < 0.8].index.tolist()\n",
    "\n",
    "if diff_notaskname:\n",
    "    _display(has_notaskname_df, diff_notaskname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add post-hoc rules to further clean up these results\n",
    "\n",
    "e.g. if Resting-State, ignore Task Name features\n",
    "\n",
    "Double check: 5090046"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Focus on TaskDescription\n",
    "\n",
    "has_notaskname_df = combined_df[combined_df.index.isin(has_task_name) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loop_print_row(df, fields=['TaskName', 'TaskDescription']):\n",
    "    for _, row in df.iterrows():\n",
    "        for f in fields:\n",
    "            if row[f]:\n",
    "                for x in row[f]:\n",
    "                    if x:\n",
    "                        print(f'{f}:', x)\n",
    "            else:\n",
    "                print(f'{f}: n/a')\n",
    "\n",
    "def clean_print_by_pmcid(df):\n",
    "    for pmcid, _df in df.groupby('pmcid'):\n",
    "        print('PMCID:', pmcid)\n",
    "\n",
    "        # For row in annot, compare TaskDescription to extract\n",
    "        print('--- Annotator ---')\n",
    "        _loop_print_row(_df.loc[_df.annotator_name.isna() == False])\n",
    "\n",
    "        print('--- Extract ---')\n",
    "        _loop_print_row(_df.loc[_df.annotator_name.isna()])\n",
    "\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMCID: 2241626\n",
      "--- Annotator ---\n",
      "TaskName: mental calculation task\n",
      "TaskName: language comprehension task\n",
      "TaskDescription: In the present research, our goal was to define a simple fMRI test, less than 5 minutes long, that could delineate, in a subject-specific manner, those cerebral circuits. A functional sequence was added to each functional imaging session performed in our lab (Figure  ), taking advantage of the continuous flow of volunteers recruited for various protocols. Because we wanted to capture the maximal amount of functional information in the minimum amount of time, we designed the sequence according the following challenging constraints: \n",
      "\n",
      "‚ñ™ the sequence had to be short, so as to disrupt as little as possible the main protocol. We choose 5 minutes for performing 100 trials. \n",
      "\n",
      "‚ñ™ we aimed to obtain for each subject a description of different levels of functional architecture, from sensori-motor areas (perception and action) to more associative areas involved in reading, language processing and calculation. \n",
      "\n",
      "‚ñ™ we aimed to capture in 5 minutes most of the individual networks related to each task. \n",
      "\n",
      "‚ñ™ individual networks described in 5 min had to be reproducible over sessions and time. \n",
      "\n",
      "The feasibility of using short stimulation designs (ranging from 10 to 25 min long) to reveal individual functional maps has been previously assessed for language mapping [ ], for visual areas [ ] and recently for a set of functional networks covering sensorimotor processes, working memory, executive functions and emotional processes [ ]. Beyond that point, the main goal of the current study was to use the data obtained with this fMRI protocol with individual subjects as the cornerstone of a large-scale hybrid database. Because the individual functional information that can be captured in such a short sequence should be considered with caution, we focus here on the design efficiency and within-subject. We then describe preliminary data obtained from 81 subjects scanned in a 3T scanner and address new methodological issues including statistical methods for analysis and visualization of inter-individual functional variability. Subsequent publications will exploit the potential of this database to focus on characterizing inter-individual variability. \n",
      "--- Extract ---\n",
      "TaskName: auditory and visual perception\n",
      "TaskName: motor actions\n",
      "TaskName: reading\n",
      "TaskName: language comprehension\n",
      "TaskName: mental calculation\n",
      "TaskDescription: The protocol captures the cerebral bases of auditory and visual perception, motor actions, reading, language comprehension and mental calculation at an individual level.\n",
      "\n",
      "PMCID: 2686646\n",
      "--- Annotator ---\n",
      "TaskName: one-back task\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: 1-back task\n",
      "TaskDescription: Participants performed a one-back task with four categories of visual stimuli: written words, pictures of common objects, scrambled pictures of the same objects, and consonant letter strings.\n",
      "\n",
      "PMCID: 3078751\n",
      "--- Annotator ---\n",
      "TaskName: movement observation paradigm\n",
      "TaskDescription: participants had to register auditory cues that determined what movement the dancer was to perform next, and watch the ensuing movements\n",
      "TaskDescription: participants watched a dancer performing according to cue, but occasionally making mistakes. Previous to playing the task, the participants were instructed on the cue-movement associations that rule the task (low chord: repetition; high chord: switch). They received a short training where they could choose either four or eight example movies that contained up to 19 cued movements, before they started the task. The movie of the dancer was displayed in the middle of an otherwise gray computer screen, using the Software Presentation 12.0 (Neurobehavioral Systems, San Francisco, CA, USA). Visual input did not extend further than 5¬∞ of visual angle. The movies were stopped in irregular intervals and participants had to indicate by button press, whether the dancer had performed correctly immediately before video offset. That is, participants had to indicate whether the very last movement had been correct, irrespective of possible earlier errors. Questions were indicated by a question mark (‚Äú?‚Äù) displayed in font size 24 for 1500‚Äâms or until the first response. Participants had to press the arrow-to-the-left key (index finger) if they judged the last movement to have been according to cue or press the arrow-to-the-right key (middle finger) if they thought the movement had not been according to cue. Responses had to be given within a timeframe of 1500‚Äâms and were followed by a valid feedback for 400‚Äâms indicating correct, incorrect or delayed responses (‚Äú+‚Äù/‚Äú‚àí‚Äù/‚Äú0‚Äù; Figure  ). \n",
      "--- Extract ---\n",
      "TaskName: Movement Observation Paradigm\n",
      "TaskDescription: Participants watched movies of a dancer producing whole-body movements according to auditory cues, with some movements violating the expected sequence.\n",
      "\n",
      "PMCID: 4115625\n",
      "--- Annotator ---\n",
      "TaskName: Voice localizer paradigm\n",
      "TaskDescription: The voice localizer includes a variety of different vocal sounds, not exclusively female or male voices, but also speech and non-speech of women, men and infants and non-vocal sounds (e.g., environmental sounds)\n",
      "TaskDescription: Subjects were instructed to close their eyes and passively listen to a large variety of sounds. Stimuli were presented in a simple block design and divided into vocal (20 blocks) and non-vocal (20 blocks) conditions. Vocal blocks contained only sounds of human vocal origin (excluding sounds without vocal fold vibration such as whistling or whispering) and consisted of speech (e.g., words, syllables, connected speech in different languages) or non-speech (e.g., coughs, laughs, sighs and cries). The vocal stimuli consisted of recordings from 7 babies, 12 adults, 23 children, and 5 elderly people. Half of the vocal sounds (speech and non-speech) consisted of vocalizations from adults and elderly people (women and men) with comparable proportions for both genders (~24% female, ~22% male). The other half of the vocal sounds consisted of infant vocalizations (speech and non-speech) which also included baby crying/laughing. Recorded non-vocal sounds included various environmental sounds (e.g., animal vocalizations, musical instruments, nature and industrial sounds). A total number of 40 blocks were presented. Each block lasted for 8 s with an inter-block interval of 2 s. Stimuli (16bit, mono, 22050 Hz sampling rate) were normalized for RMS and are available at   (Belin et al.,  ). \n",
      "--- Extract ---\n",
      "TaskName: Voice localizer\n",
      "TaskDescription: Subjects were instructed to close their eyes and passively listen to a large variety of sounds. Stimuli were presented in a simple block design and divided into vocal (20 blocks) and non-vocal (20 blocks) conditions.\n",
      "\n",
      "PMCID: 4179768\n",
      "--- Annotator ---\n",
      "TaskName: single food choice task\n",
      "TaskDescription: During the functional MRI scan, participants performed a food choice task (Figure  ). In this ask, participants made 100 choices. In every trial, they viewed one of the study stimuli (3000 ms, choice period) and subsequently had to indicate with a button press (1500 ms, button press period) whether they wanted to eat a portion of the snack or not. During the button press period the words ‚Äúyes‚Äù and ‚Äúno‚Äù were shown left/right (randomized) on the screen. After indicating their choice, a yellow box appeared around the yes or no. Participants were instructed to make their choice already during the period that the image was shown. To ensure that their choices were actually made in direct response to the food pictures, the button press period was so short that it only allowed them to locate whether they had to push the left or right button. The choice trials were interspersed with a random interval (2000 and 5000 ms). At the beginning, halfway (after 50 trials) and at the end an additional baseline period of 30,000 ms was included in the task. \n",
      "--- Extract ---\n",
      "TaskName: food choice task\n",
      "TaskDescription: Participants made 100 choices between high-energy (HE) and low-energy (LE) snacks while their brains were scanned using fMRI.\n",
      "\n",
      "PMCID: 4374765\n",
      "--- Annotator ---\n",
      "TaskName: fear inducing paradigm\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Fear provocation paradigm\n",
      "TaskDescription: Participants were presented with still pictures of spiders, control animals (birds, caterpillars, snails, and lizards), negative pictures from the International Affective Picture System (IAPS), and neutral pictures from IAPS. They performed a covert task of detecting the presence of a human in the pictures.\n",
      "\n",
      "PMCID: 4398371\n",
      "--- Annotator ---\n",
      "TaskName: Soccer Paradigm\n",
      "TaskName: monetary incentive paradigm\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Soccer Paradigm\n",
      "TaskName: Monetary Paradigm\n",
      "TaskDescription: Participants decided to either pass or shoot the ball in a soccer scenario presented in a fMRI setting.\n",
      "TaskDescription: Participants guessed under which box a circle was hidden in a monetary incentive task.\n",
      "\n",
      "PMCID: 4440210\n",
      "--- Annotator ---\n",
      "TaskName: emotional matching task\n",
      "TaskDescription: Subjects were shown triplets of geometric shapes (as neutral stimuli) and of threatening scenes as well as fearful faces (as emotional conditions) presented in alternating blocks of neutral and emotional stimuli.\n",
      "--- Extract ---\n",
      "TaskName: emotional matching task\n",
      "TaskName: resting state scan\n",
      "TaskDescription: Subjects were shown triplets of geometric shapes (as neutral stimuli) and of threatening scenes as well as fearful faces (as emotional conditions) presented in alternating blocks of neutral and emotional stimuli.\n",
      "TaskDescription: Resting-state functional connectivity was computed from those same voxels.\n",
      "\n",
      "PMCID: 4517759\n",
      "--- Annotator ---\n",
      "TaskName: Typing task\n",
      "TaskName: Reading task\n",
      "TaskName: Typing-movement task \n",
      "TaskName: Writing task\n",
      "TaskName: Reading task\n",
      "TaskName: Writing-movement task\n",
      "TaskDescription: In the typing task, a word was presented on a screen and the subject was instructed to type the visually presented word on a keyboard (Fujitsu CP218230-02) that had been modified for use in the scanner by eliminating all ferro-magnetic components. The word list for the task consisted of 110 kana nouns obtained from the Nippon Telegraph and Telephone corporation (NTT) psycholinguistic database called \"Lexical Properties of Japanese\" [ ]. Words were matched on linguistic parameters including word frequency, imagability, number of syllables, and key positions on the QWERTY keyboard so as not to require unequal hand usage. All words consisted of four or five kana characters (seven to nine alphabetic characters) and did not include the Roman characters q, w, z, x, c, v, p, or l, because there are relatively few opportunities to press these keys in the Japanese language and these characters are located in the corners of the QWERTY keyboard ( ). No word was shown more than once in the experiment (i.e., the six scanning sessions containing the writing, reading, and the typing conditions). \n",
      "   Positions of the keys required to type the words included in the word list.  \n",
      "The typing task required the subjects to type the letters corresponding to the red colored keys. The keys q, w, z, x, c, v, p, and l were excluded because there are few opportunities to press these keys for Japanese typists and these keys are located in the corners of the QWERTY keyboard. \n",
      "  \n",
      "Before the typing task, a picture of a hand, which indicated that the subject should type, was presented for 2000 ms followed by six different words presented sequentially over a period of 18000 ms with no interval and subjects were instructed to type each word on the keyboard ( ). We adopted the most popular way of typing Japanese called ‚Äúalphabetical input‚Äù. In this method, a western-style keyboard is used, and the kana character ‚Äúka‚Äù is produced by typing ‚Äúk‚Äù followed by ‚Äúa‚Äù [ ]. Although there are other ways of typing, all subjects were instructed to type using the alphabetical input method. \n",
      "\n",
      "The keys pressed were recorded through the optical fibers placed in the keyboard buttons and the accuracy of typewriting was calculated offline. The percentage of left-hand and right-hand usage on the keyboard was calculated for each word to ensure that there was no bias in hand use. Behavioral data obtained from the typing experiment were processed in MATLAB 7.7.0 (MathWorks Inc., Natick, MA, USA). Accurate trials were defined as those for which subjects produced the correct key-pressing sequence for a given word. We were primarily interested in spelling errors as opposed to typing errors such as a slip in a keypress to the adjacent key, and we therefore did not consider key presses that were either one to the right or left of the correct key to be incorrect (e.g. for the letter ‚Äúy‚Äù, ‚Äút‚Äù or ‚Äúu‚Äù were considered correct). Typing speed (number of keys per second) was calculated offline to check the difference between consecutive key presses for each trial involving a motor control task. \n",
      "\n",
      "TaskDescription: For the reading task, a picture of a book, which indicated that the subject should read, was presented for 2000 ms followed by six different words presented sequentially over a period of 18000 ms with no interval, and subjects were instructed to read each word silently and not to move their fingers ( ). The word list for the reading task was the same as that used in the writing task, and no word was shown more than once in the experiment. \n",
      "TaskDescription: In the typing-movement task, which was designed as a control, a double circle (‚óé) was presented for 2000-ms periods with a blank screen presented for 1000 ms between each period, and subjects were instructed to type randomly with both hands at a pre-learned speed during the presentation of the double circle ( ). Behavioral data were obtained in the same manner as for the typing task. \n",
      "TaskDescription: In the writing task, subjects were instructed to write words with their right index finger in the air. It is natural for Japanese people to write with the index finger in the air, and they often move their index finger to trace the imagined letter. Before the task, a picture of a hand, which indicated that the subject should write, was presented for 2000 ms followed by six different words presented sequentially over a period of 18000 ms with no interval and subjects were instructed to trace each word in the air with their index finger in Japanese phonograms (kana) ( ). The word list for this task was the same as that used in the typing condition. During the fMRI scan, finger movements were checked by examiners via a video monitor. \n",
      "TaskDescription: For the reading task, a picture of a book, which indicated that the subject should read, was presented for 2000 ms followed by six different words presented sequentially over a period of 18000 ms with no interval, and subjects were instructed to read each word silently ( ). The word list for the reading task was the same as that used in the typing condition. \n",
      "TaskDescription: In the writing-movement task, a double circle (‚óé) was presented and subjects were instructed to move their index finger randomly at a pre-learned speed, in the way they do when they write in the air. The double circle was presented for 2500-ms periods with a blank screen presented for 500 ms between each period ( ). The stimulus duration in the writing-movement task was longer than that in the typing-movement task to control for the difference in the time taken to complete these two tasks. During the fMRI scan, finger movements were checked by examiners via a video monitor. \n",
      "\n",
      "--- Extract ---\n",
      "TaskName: Typing Task\n",
      "TaskName: Writing Task\n",
      "TaskName: Reading Task\n",
      "TaskName: Typing-Movement Task\n",
      "TaskName: Writing-Movement Task\n",
      "TaskDescription: Subjects typed visually presented Japanese words on an MRI-compatible QWERTY keyboard.\n",
      "TaskDescription: Subjects wrote words with their right index finger in the air.\n",
      "TaskDescription: Subjects read words silently and did not move their fingers.\n",
      "TaskDescription: Subjects typed randomly with both hands when a double circle symbol was present on the monitor.\n",
      "TaskDescription: Subjects moved their index finger randomly at a pre-learned speed when a double circle symbol was present on the monitor.\n",
      "\n",
      "PMCID: 4530666\n",
      "--- Annotator ---\n",
      "TaskName: implicit emotion processing task \n",
      "TaskName: explicit emotion identification task\n",
      "TaskDescription: In the implicit task, participants were presented with each stimulus twice in a pseudorandom fashion, resulting in a total of 16 presentations of each emotion over two runs.\n",
      "TaskDescription: In the explicit task, participants were shown each stimulus once, resulting in eight presentations per emotion over one run.\n",
      "--- Extract ---\n",
      "TaskName: implicit emotion processing task\n",
      "TaskName: explicit emotion identification task\n",
      "TaskDescription: Participants were presented with an implicit emotion processing task during fMRI and eyetracking, followed by an explicit emotion identification task.\n",
      "TaskDescription: In the implicit task, participants were presented with face stimuli for gender identification while undergoing fMRI and eyetracking. In the explicit task, participants identified emotions displayed in face stimuli outside the scanner.\n",
      "\n",
      "PMCID: 5324609\n",
      "--- Annotator ---\n",
      "TaskName: acquired equivalence task\n",
      "TaskDescription: participants learned a set of visual discriminations via trial‚Äêand‚Äêerror (learning phase), and were subsequently tested on their ability to generalize what they had learned (generalization phase). Both learning and generalization occurred within a single scanning session and took place in the context of a first‚Äêperson virtual reality environment (see Fig.  ). On each trial, a scene was presented depicting two buildings positioned equidistantly from a start location. One building concealed a pile of gold (the ‚Äúreward‚Äù). The location of this gold was determined by the configuration of wall textures rendered onto the towers of each building. Participants were required to select the rewarded building (within 3 s). Following a response, video feedback was presented (7 s) before an inter‚Äêtrial interval (1 s). \n",
      "--- Extract ---\n",
      "TaskName: learning phase\n",
      "TaskName: generalization phase\n",
      "TaskDescription: During scanning participants learned a set of visual discriminations via trial-and-error (learning phase), and were subsequently tested on their ability to generalize what they had learned (generalization phase). Both learning and generalization occurred within a single scanning session and took place in the context of a first-person virtual reality environment.\n",
      "\n",
      "PMCID: 5404760\n",
      "--- Annotator ---\n",
      "TaskName: revised Social Norm Processing Task (SNPT-R)\n",
      "TaskDescription: the SNPT-R contains stories describing neutral social situations, stories on unintentional social norm violations, and stories depicting intentional social norm violations. However, in contrast to earlier versions of the SNPT [ ‚Äì ], the SNPT-R uses only personal stories in order to maximize personal involvement of the participants while reading the stories \n",
      "TaskDescription: During the story-reading phase (1), participants read stories consisting of a stem sentence and an ending sentence, describing either a neutral social situation, a situation in which a social norm was unintentionally transgressed or situation in which a social norm was violated intentionally. Participants were instructed to imagine themselves in the situation described. In the rating phase (2), participants rated all stories on embarrassment and inappropriateness. \n",
      "  \n",
      "In the story-reading phase, participants read short stories written in second person. Each story consisted of two sentences, a stem sentence (duration: 3 s) and an ending sentence (duration: 6 s). The stories described either a situation in which no social norm was transgressed (‚Äúneutral condition‚Äù), a situation in which a social norm was unintentionally transgressed (‚Äúunintentional condition‚Äù) or a situation in which a social norm was intentionally transgressed (‚Äúintentional condition‚Äù). It is important to note that the unintentional (‚ÄúYou are baking with friends. You use salt instead of sugar without realizing.‚Äù) and intentional (‚ÄúYou are baking with friends. You use salt instead of sugar as a joke.‚Äù) condition differ only in the intention of the actor, while we aimed to keep the actual outcome of the action (for example, a distasteful cake) in general the same (although the outcome of some intentional stories could be considered to be more severe in comparison to the outcome of the matching unintentional story, inherent to the verb used to describe intentionality; we refer the reader to   for a sensitivity analysis). \n",
      "\n",
      "The stories in the SNPT-R were developed in collaboration with Karina S. Blair, author of previous work on the SNPT [ ]. All stories described everyday social situations, in which the protagonist was accompanied by at least one other person, and the stories outlined relative innocuous violations of conventional social norms, in which no severe harm was done to others. The stories were heterogeneous with respect to the context (for example, in the presence of one friend or in public space like an airport) and the nature of the social norm transgression (for example, breaking decency rules versus hurting somebody), in order to increase the external validity of the paradigm. Stories were developed to be suitable for a broad audience and age-range (for children from age 8). However, given the fact that the stories of the SNPT-R were all personal (written in ‚Äòyou‚Äô form) in order to maximize personal involvement of participants, some small changes were necessary in stories describing age- or genderspecific elements. Therefore, four age- and gender specific versions of the task were developed: for boys < 18 years of age (version 1), girls < 18 years of age (version 2), men ‚â• 18 years of age (version 3) and women ‚â• 18 years of age (version 4). For example, the school environment (< 18 years) was replaced for a work environment (‚â• 18 years of age), and ‚Äòbikini bottoms‚Äô (females) for ‚Äòswimming trunks‚Äô (males). However, these changes were only minimal (see   and osf.io/m8r76 for a full list of stories included in the SNPT-R [ ]). \n",
      "\n",
      "In line with the SNPT described by Blair et al. [ ], twenty-six stem sentences were developed, with three different types of ending. Therefore, the SNPT-R consisted of 78 short stories in total. These stories were presented in a pseudo-random order using E-Prime software (version 2.0.10, Psychology Software Tools; script available at osf.io/m8r76 [ ]), separated by a fixation cross (jittered duration between 2‚Äì7 s, determined using Optseq software ( ), mean duration fixation: 3.5 s) and divided into two consecutive blocks of 39 stories (duration each block: 8 min 44 s). Participants were instructed to imagine themselves in the social situations described and to press a button with their right index finger after reading the stem sentence of each story. A button press within 3 s resulted in visual feedback to the participant (a green checkmark presented beneath the sentence). This element was added to the paradigm in order to be able to check whether participants engaged with the task. Prior to the start of the experiment, all participants were familiarized with the story-reading phase by performing a short version of the task (using 5 stories). \n",
      "\n",
      "In the (unannounced) rating phase of the task ( ), participants read all stories again and were asked to rate them on a 5-point Likert scale on embarrassment (ranging from 1, not embarrassing at all, to 5, extremely embarrassing) and inappropriateness (ranging from 1, not inappropriate at all, to 5, extremely inappropriate), similar as in the SNPT described by Blair and colleagues [ ]. These tasks were also presented using E-Prime software (version 2.0.10, Psychology Software Tools; scripts available at osf.io/m8r76 [ ]). \n",
      "--- Extract ---\n",
      "TaskName: Social Norm Processing Task (SNPT-R)\n",
      "TaskDescription: Participants read stories consisting of a stem sentence and an ending sentence, describing either a neutral social situation, a situation in which a social norm was unintentionally transgressed or situation in which a social norm was violated intentionally. Participants were instructed to imagine themselves in the situation described.\n",
      "\n",
      "PMCID: 5662181\n",
      "--- Annotator ---\n",
      "TaskName: Color-Word Stroop task\n",
      "TaskDescription: The children performed a modified version of the Color-Word Stroop task designed to be used inside the fMRI scanner [ ]. During each trial, participants were presented with a word on a black screen written in one of four colors (red, yellow, green, or blue) and were instructed to subvocalize the color of the stimulus, regardless of the meaning of the written word, and simultaneously press an arbitrary button on a button box. Subvocalization, in which participants say the color of the ink in their heads rather than out loud, is a commonly used approach in developmental neuroimaging studies of cognitive control [ , ] because it helps minimize movement-related artifacts. On the other hand, the technique makes it inherently difficult to ensure that the participant is correctly completing the task. Therefore, extensive pre-scanning training was conducted and in-scanner response times between the conditions were evaluated to have on objective measure of whether the participant was indeed performing the task. Prior to scanning, participants practiced the task in a lab testing room. First, the child was presented with 3 blocks (1 congruent, 2 incongruent with 4 trials in each) and was instructed to say the color of the written word aloud. Second, the child was presented with 1 incongruent block (4 trials total) and was instructed to say the color of the word in his or her head and simultaneously press a button. \n",
      "--- Extract ---\n",
      "TaskName: Color-Word Stroop task\n",
      "TaskDescription: Children performed a modified version of the Color-Word Stroop task designed to be used inside the fMRI scanner. During each trial, participants were presented with a word on a black screen written in one of four colors (red, yellow, green, or blue) and were instructed to subvocalize the color of the stimulus, regardless of the meaning of the written word, and simultaneously press an arbitrary button on a button box.\n",
      "\n",
      "PMCID: 5776089\n",
      "--- Annotator ---\n",
      "TaskName: Motor Task\n",
      "TaskName: Somatosensory Stimulation\n",
      "TaskDescription: he fMRI session was composed of nine rest‚Äìtask cycles with 30 s for each period. Eyes were kept closed during scanning. The motor task consisted of repetitive alternating dorsiflexion and relaxation of the right foot (with range reaching 15¬∞). Foot movements were paced following an audio cue that was sounded every 1.5 s. These small range of motion and medium speed were applied to avoid large head motions. In each task period, the last audio cue is a verbal command ‚Äústop‚Äù. The verbal command, ‚Äúready, right foot movement, go‚Äù was delivered within a 2 s duration that is 3 s ahead of each task period. These audio cues and commands were recorded in advance and transmitted via the intercom system of the MR scanner. Each subject was trained prior to MR scanning to perform the motor task as gently as possible. \n",
      "--- Extract ---\n",
      "TaskName: right ankle dorsiflexion\n",
      "TaskName: ankle dorsiflexion coupled with simultaneous stimulation to the agonist muscle\n",
      "TaskName: ankle dorsiflexion coupled with simultaneous stimulation to a control area\n",
      "TaskDescription: The motor task consisted of repetitive alternating dorsiflexion and relaxation of the right foot (with range reaching 15). Foot movements were paced following an audio cue that was sounded every 1.5 s.\n",
      "\n",
      "PMCID: 5973829\n",
      "--- Annotator ---\n",
      "TaskName: Food task\n",
      "TaskName: Altruism task\n",
      "TaskDescription: ¬†The non-social fMRI task was a modified version of an established food task ( ). On every trial, subjects chose between one of 90 food items presented on-screen (4 s) and a default food chosen prior to scanning ( ). Subjects responded by pressing one of four buttons corresponding to ‚Äòstrong yes‚Äô, ‚Äòyes‚Äô, ‚Äòno‚Äô, ‚Äòstrong no‚Äô (displayed at the bottom of the screen), using a button box placed in their right hand. The assignment of choice preferences to buttons was fixed throughout the task and the right-left orientation of the scale was counterbalanced across subjects. Inter-trial intervals varied from 1 to 4 s (average of 2 s), during which a white fixation cross was presented against a black background. After scanning, one trial was randomly drawn to determine what the subject would eat before leaving the lab. If subjects failed to respond within the 4 s of the selected trial either the on-screen or the default option was randomly chosen. \n",
      "\n",
      "Subjects made food choices under three conditions:   Respond Naturally   (‚Äòrespond as you naturally would‚Äô, [NC]),   Focus on Health   (‚Äòfocus on the healthiness of the food when making the choice‚Äô, [HC]), or   Focus on Taste   (‚Äòfocus on the tastiness of the food when making the choice‚Äô, [TC]) (see Appendix 1 ‚Äì Instructions for regulatory conditions in both choice tasks for instructions). Importantly, subjects were explicitly instructed to always make the decision based on their preference, regardless of the condition. Every condition comprised nine blocks (with 10 trials per block), resulting in a total of 90 trials per condition. Prior to every block, detailed instructions appeared for 4 s. In addition, during food display, a short description (‚ÄòRespond Naturally‚Äô, ‚ÄòFocus on Health‚Äô, ‚ÄòFocus on Taste‚Äô) appeared at the top of the screen to remind participants of the current instruction. Each of the nine functional scanning runs contained one block of every condition (i.e., three task blocks per run), with the order of conditions randomized across runs and subjects. The only exception was the first task block, which was pre-assigned to ‚Äònatural‚Äô for every subject. Practice trials as well as a short quiz prior to scanning ensured that subjects understood the instructions for each condition and were comfortable with the timing of the task. \n",
      "\n",
      "Food items varied in their perceived tastiness and healthiness and included healthy snacks (e.g., apples, broccoli) and junk foods (e.g., candy bars, chips). Items were selected based on subjects ratings in a self-paced computerized task prior to scanning that assessed perceived tastiness (5-point Likert scale, ‚Äòvery untasty‚Äô to¬†‚Äòvery tasty‚Äô) and healthiness (5-point Likert scale, ‚Äòvery unhealthy‚Äô to ‚Äòvery healthy‚Äô) of 200 food items ( ;  ). Ninety food items were selected from this larger set to cover the range of health and taste ratings in a roughly uniform manner. In addition, for each subject we chose one default food that was perceived as neutral for taste and health. Each food item was presented once in each of three choice conditions, with presentation order randomized across blocks, functional runs, and subjects. To ensure the motivational saliency of the food items, subjects were asked to refrain from eating 4 hr prior to testing. Stimulus presentation was implemented using high-resolution color pictures (72 dpi) and Psychophysics Toolbox Version 3 ( ) together with Matlab (2014a). \n",
      "TaskDescription: The altruism task was an fMRI compatible version of the dictator game modified from ( ). On every trial, subjects were presented with a monetary proposal that affected their own ($Self) and another persons‚Äô ($Other) monetary payoff ( ). Subjects had 4 s to chose between the on-screen proposal and a constant default allocation ($20 to both) by pressing one of the four response buttons (‚Äòstrong yes‚Äô, ‚Äòyes‚Äô, ‚Äòno‚Äô, ‚Äòstrong no‚Äô; direction counter-balanced across subjects). Payouts for self and other ranged from $0 to $40 and always involved a tradeoff between self and other (i.e. prizes for one individual were equal or less than the default, while prizes for the other individual exceeded the default). Thus, subjects always had to choose between acting altruistically (benefitting the other at a cost to oneself) or selfishly (benefitting oneself at a cost to the other) on every trial. At the end of the experiment, one trial was randomly selected and implemented according to the subjects‚Äô choice. If subjects failed to respond within 4 s for this trial, both individuals received $0. \n",
      "\n",
      "Similar to the food task, subjects performed the task under three different conditions:   Respond Naturally   (‚Äòrespond as you naturally would‚Äô, [NC]),   Focus on Ethics   (‚Äòfocus on doing the right thing and consider the ethical or moral implications of your choice‚Äô, [EC]), or   Focus on Partner   (‚Äòfocus on your partner‚Äôs feelings and how the other person is affected by your choice‚Äô, [PC]). Subjects were reminded to always make their choice based on their preference, regardless of the condition. Conditions were implemented in separate blocks of 10 trials each, with the beginning of a new block signaled by a short reminder instruction (4 s). Matching the food task, subjects performed 9 blocks of each condition (i.e., 90 trials per condition and a total of 270 trials), with the block order counter-balanced across subjects and functional runs, with the exception that the first two blocks were always natural choice trials. Choices in these NC blocks were used to estimate a logistic regression [Choice¬†=¬†w  * $Self¬†+¬†w  * $Other] and used for a subject-specific selection of 30% of proposals most likely to elicit generous behavior and 30% of proposals likely to elicit selfish behavior. The remaining 40% of trials were randomly chosen from the full proposal space. Practice trials and a quiz prior to scanning verified that subjects were capable and comfortable to make the choice within 4 s. \n",
      "--- Extract ---\n",
      "TaskName: Food Task\n",
      "TaskName: Altruism Task\n",
      "TaskDescription: Subjects chose between on-screen food items that varied in tastiness and healthiness and a neutral default food. Choices were made in Natural [NC], Focus on Health [HC], and Focus on Taste Conditions [TC].\n",
      "TaskDescription: Subjects chose between on-screen proposals that affected the payoff of themselves ($Self) and an anonymous partner ($Other) and a default option ($20 for both). Choices were made in Natural [NC], Focus on Ethics [EC], and Focus on Partner Conditions [PC].\n",
      "\n",
      "PMCID: 6024199\n",
      "--- Annotator ---\n",
      "TaskName: involuntary memory\n",
      "TaskDescription: During the re-encoding runs, each sound-picture pair and unpaired sound was presented. Participants rated on a 7-point scale how emotional the stimuli were. During the recall runs, the 55 paired and 40 unpaired sounds (randomly intermixed) were presented, panned 15¬∞ to either the left or the right using specialized audio software (Audacity,  ). Participants indicated the side on which the sound was louder. We presented each sound for 4‚ÄØs, followed by a 1.5-second response period, during which the screen was blank and participants completed the sound localization judgment. A fixation period followed (jittered with a mean of 4‚ÄØs, range: 1.5‚ÄØs‚Äì10.4‚ÄØs). At the beginning of the run, we instructed participants to wait until after the sound had ended to respond. Immediately following the scanning session, participants completed a post-scan questionnaire to assess their memory for the scenes. Participants heard all 95 sounds. After the presentation of each sound, participants reported: (1) whether they had remembered a picture when the sound was played during the sound localization task (yes/no), (2) how hard they tried to perform the sound localization task correctly (1‚ÄØ=‚ÄØdid not try at all, 7‚ÄØ=‚ÄØtried very hard), (3) how hard they tried to recall a picture during the scan (1‚ÄØ=‚ÄØdid not try at all, 7‚ÄØ=‚ÄØtried very hard), and (4) how vivid the memory of the picture was during the scan (1‚ÄØ=‚ÄØnot at all vivid, 7‚ÄØ=‚ÄØvery vivid). Lastly, we presented the 55 paired sounds a final time and participants provided a description of the picture originally paired with the sound. Two independent judges rated these descriptions for accuracy. In the case of disagreement, a third judge broke the tie. As described below, only paired sounds that elicited a memory without effort and unpaired sounds that did not elicit a memory and had no retrieval effort were analyzed. \n",
      "\n",
      "--- Extract ---\n",
      "TaskName: Involuntary memory task\n",
      "TaskDescription: Participants performed an involuntary memory task while undergoing a functional magnetic resonance imaging scan. Environmental sounds served as cues for well-associated pictures of negative and neutral scenes.\n",
      "\n",
      "PMCID: 6037859\n",
      "--- Annotator ---\n",
      "TaskName: synchrony judgment\n",
      "TaskName: temporal order judgment \n",
      "TaskDescription: The fMRI procedure was similar to the behavioral experiment, except that a reduced stimulus set was presented: ‚àí333, 0, PSS, and +333. The PSS values were obtained individually from the pre-fMRI experiment separately for SJ and TOJ. To be as accurate as possible the individual PSS conditions were selected as the closest COA level to that of the PSS value derived from the pre-fMRI data fits. Although COA levels in the pre-fMRI experiment were restricted to ¬±333, ¬±267, ¬±200, ¬±133, ¬±67, and 0 ms, COA values for the PSS condition in the fMRI experiment could be any COA level between 0 and ¬±333 in 16 ms increments i.e., one frame at a time. This use of an individually determined stimulus level (PSS) is similar to the approach used by Binder ( ) to determine stimuli levels, but did not use the simultaneity threshold approach based on separate sound-first and flash-first trials. For TOJ-unable participants, we used average results from a behavioral study using identical stimuli (Petrini et al.,  ). An optimized mixed block/event-related design was used to enable investigation of differences between the tasks at both transient and sustained levels of processing. \n",
      "\n",
      "Each of two functional runs (~22 min each) consisted of 32 stimulation blocks (half SJ and half TOJ, randomized) and after every two stimulation blocks there was a 16 s fixation block (Figure  ). Within a stimulation block (25 s) there were 9 events: 5 stimuli (each 3 s) separated by 4 fixation events (1, 2, 3, or 4 s in pseudorandom order). Each COA condition was presented a total of 40 times (20 per run) per task. To minimize the correlation between the transient (stimuli) and sustained (stimulation block) regressors the number of times an individual COA condition was presented within a single stimulation block was manipulated as follows: in a run, a COA level was presented 0 times during 4 stimulation blocks, once in 6 blocks, twice in 4 blocks and 3 times in 2 blocks, i.e., a total of 20 presentations for each COA level and task. One thousand sequences with different randomizations of the order of events and blocks were created and the best chosen by balancing efficiency and correlation. In the chosen sequence, the mean correlation between sustained and transient regressors was 0.47, which enabled reliable estimation of both types of BOLD response (Otten et al.,  ). \n",
      "\n",
      "Auditory stimuli were presented via Sensimetrics S14 insert headphones at approximately 85 dB. The visual cue was back-projected (Panasonic PT-D7700E DLP; 1,024 √ó 768 pixel resolution, 60 Hz refresh rate) onto a screen behind the participant's head, visible via a mirror mounted on the MR head coil with an approximate viewing distance of 65 cm. \n",
      "--- Extract ---\n",
      "TaskName: Synchrony Judgment (SJ)\n",
      "TaskName: Temporal Order Judgment (TOJ)\n",
      "TaskDescription: Participants made SJs and TOJs to synchronous and asynchronous audiovisual stimuli.\n",
      "\n",
      "PMCID: 6137311\n",
      "--- Annotator ---\n",
      "TaskName: Yellow Light Game (YLG)\n",
      "TaskDescription: The YLG (Op de Macks   et al  .,  ) is an adaption of the widely used Stoplight Task (Gardner and Steinberg,  ; Chein   et al  .,  ;) that examines risk-taking at the behavioral and neural level. In the YLG, participants were asked to drive a virtual car from the driver‚Äôs point of view along a straight track, during which they encountered several intersections with yellow lights ( ). They were instructed that the goal of the game was to get through all of the intersections in the shortest amount of time. At each intersection, participants had to indicate by button press whether they wanted to accelerate and go through the yellow light (go decision) or brake before arriving at the intersection (stop decision). Deciding to accelerate through the intersection‚Äîa go decision‚Äîconstitutes a risky decision, and could result either in a successful go associated with no delay (i.e. if there was no other car passing through the intersection), or a delay of 5¬†s in the event of a crash (i.e. if there was another car passing through the intersection). A successful go was shown on the screen with a blue tilde and a positive chiming sound ( ), whereas a crash was shown as a cracked car window, honking car and crash sound ( ).\n",
      "--- Extract ---\n",
      "TaskName: Yellow Light Game (YLG)\n",
      "TaskDescription: Participants completed the Yellow Light Game (YLG), a computerized driving task, during which they could make safe or risky decisions, in the presence of a peer and their parent.\n",
      "\n",
      "PMCID: 6175904\n",
      "--- Annotator ---\n",
      "TaskName: autobiographical reminiscence task\n",
      "TaskDescription: participants were instructed that they would be viewing images from the experience sampling experiment they recently completed and told that each image would be displayed for 8 s. Participants were asked to ‚Äú‚Ä¶ try to remember the event depicted in the picture, and try to relive your experience mentally‚Äù. After the remembrance period for each event, participants were asked if they remembered the event (‚Äúyes‚Äù or ‚Äúno‚Äù) and how vividly they recalled the event (‚Äúlots of detail‚Äù or ‚Äúvery little detail‚Äù). Participants were given 2.5 s to respond to each of those questions using a button box held in their right hand. The images were presented in random order, and the task was split into eight runs with 15 images in each run. With each image presented for 8 s and each question for presented 2.5 s with a 0.5 s interstimulus interval, each trial took a total of 14 s. The intertrial interval was jittered uniformly between 4 and 10 s, allowing for a true event-related design.\n",
      "--- Extract ---\n",
      "TaskName: Autobiographical reminiscence\n",
      "TaskDescription: Participants relived their experiences in an fMRI scanner cued by images from their own lives.\n",
      "\n",
      "PMCID: 6200838\n",
      "--- Annotator ---\n",
      "TaskName: affective Stroop task\n",
      "TaskName: The Affective Stroop Task \n",
      "TaskDescription: variation of a response inhibition task and comprises a number Stroop task with trials which vary in cognitive load\n",
      "TaskDescription: Each trial started with an emotional stimulus, i.e., a negative (Neg) or neutral (Neu) stimulus (150 ms), followed by a task trial (congruent/incongruent/neutral Stroop trial or a blank screen) and finally a relaxation period, i.e., blank screen (350 ms). All pictures were selected from a child-appropriate image system [Developmental Affective Photo System (DAPS);  ]. During task trials, participants were presented with an array of 1 to 4 digits or a blank screen and were asked to press a button corresponding to the number of items displayed. The number of items was either congruent (C; e.g., number 3 in an array of 3) or incongruent (IC; e.g., number 1 in an array of 2) with the digits presented. Star shaped stimuli (S; as a neutral baseline counting condition) and blank trials (B; no response expected from participants) were used as control conditions (for further details see  ). Trial order and interstimulus intervals (which were 350‚Äì1850 ms) were randomized using Optseq  and kept constant across participants. A total of 300 task and 100 blank trials were administered (100 for C/IC/S trials, 50 with preceding negative images, 50 with neutral images, in 2 runs), with a total scan time of about 16 min (7.59 min each run). \n",
      "--- Extract ---\n",
      "TaskName: Affective Stroop Task\n",
      "TaskDescription: Each trial started with an emotional stimulus (negative/neutral) followed by a task trial (congruent/incongruent/neutral Stroop trial or a blank screen) and finally a relaxation period (blank screen). Participants were presented with an array of 1 to 4 digits or a blank screen and were asked to press a button corresponding to the number of items displayed.\n",
      "\n",
      "PMCID: 6219793\n",
      "--- Annotator ---\n",
      "TaskName: Social feedback paradigm\n",
      "TaskDescription: Following previous studies [ ‚Äì ], negative social feedback was used to induce emotions for two reasons: (a) in daily life emotions are often caused by social stimuli [ , ] and (b) social feedback elicits emotional responses that are long enough to study emotion dynamics [ ]. The social feedback consisted of ratings on desirable (e.g., interesting, honest) and undesirable (e.g., stubborn, superficial) personality traits as well as on an item assessing whether the evaluator would like to have the participant as a friend (an English translation of the original feedback forms is shown in  ). Negative feedback consisted of low (high) ratings on desirable (undesirable) items as well as on the evaluator‚Äôs desire to have the participant as a friend. Neutral feedback consisted of ratings close to the neutral scale midpoint of all items. Feedback was shown in one of two pre-specified orders, preventing the presentation of more than two consecutive trials of the same valence (negative or neutral), counterbalanced across participants. \n",
      "--- Extract ---\n",
      "TaskName: Self-distanced vs. self-immersed perspective\n",
      "TaskDescription: Participants were asked to adopt a self-immersed or self-distanced perspective while reading and thinking about negative social feedback.\n",
      "\n",
      "PMCID: 6303343\n",
      "--- Annotator ---\n",
      "TaskName: Balloon Analogue Risk Task (BART)\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Balloon Analog Risk Task\n",
      "TaskDescription: Participants completed a version of the Balloon Analogue Risk Task (BART), a well-validated experimental paradigm that has been adapted for fMRI in developmental populations. The BART measures participants willingness to engage in risky behavior in order to earn rewards, and is associated with real-life risk taking in adolescents and adults. During the scan session, participants were presented with a sequence of 24 balloons that they could pump up to earn points. Each pump decision was associated with earning one point but also increased the risk that a balloon would explode. If participants pumped a balloon too many times, the balloon would explode and participants would lose all the points they had earned for that balloon. However, if participants chose to cash out before the balloon exploded, the points they earned would be added to the running total of points, which was presented on the screen as a points meter. Participants were instructed that their goal was to earn as many points as possible during the task. Each event (e.g., larger balloon following a pump, new balloon following cashed or explosion outcomes) was separated with a random jitter (500-4000ms). Balloons exploded after 4 to 10 pumps, and the order of balloons was presented in a fixed order (after being pseudo-randomly ordered prior to data collection), although none of this information was made available to participants. The BART was self-paced and would not advance unless the participant made the choice to either pump or cash out. Participants were told that they could win a $10 gift card at the end of the neuroimaging session if they earned enough points during the task. The point threshold for winning this prize was intentionally left ambiguous so that participants were motivated to continue earning points throughout the task. In reality, all participants were given a $10 gift card after completing the scan session.\n",
      "\n",
      "PMCID: 6411911\n",
      "--- Annotator ---\n",
      "TaskName: CUPS task\n",
      "TaskDescription: they had to decide whether to accept or refuse a series of mixed gambles. In each trial participants were presented with a set of cups (3 to 11). They were informed that one of the cups contained a gain (amount ranging between $3 and $8) and the rest of the cups contained a loss of $1, and they were asked to accept or reject the gamble ( ). When the gamble was accepted, the participant was informed of the gain or loss after a short waiting period. When the gamble was rejected, the participant did not win or lose any money. When no selection was made during the response window, the participant lost $1.   \n",
      "--- Extract ---\n",
      "TaskName: CUPS task\n",
      "TaskDescription: Participants decided to accept or refuse a series of mixed gambles, where one cup contained a gain and the rest contained a loss.\n",
      "\n",
      "PMCID: 6463125\n",
      "--- Annotator ---\n",
      "TaskName: food cue reactivity\n",
      "TaskDescription: During the experimental runs images of neutral objects, high-calorie foods, and low-calorie foods were presented in a block design format. Each run consisted of four randomly presented 21‚ÄØs long blocks for each image category in which 7 individual images were presented for 2‚ÄØs followed by a 1‚ÄØs gap. Each block was separated by 9‚ÄØs and each run began with 15‚ÄØs of blank screen with only a fixation cross present. \n",
      "\n",
      "A total of 112 pictures for each image category were collected from the International Affective Picture System (IAPS) database ( ) and Internet search engines. In the two sessions, the same 112 images were randomly presented in four runs of 28 images for each image category. High-calorie food images contained an equal number of sweet and savory food images. All images were equated for luminance and contrast and presented centrally, subtending 8‚ÄØ√ó‚ÄØ6¬∞, on a uniform gray background. Stimuli were projected onto a translucent screen located at the back of the scanner bore using a Panasonic PT-D3500E DLP projector (Matsushita Electric Industrial Co., Ltd., Kadoma, Osaka, Japan) at a refresh rate of 60‚ÄØHz, and they were viewed through a mirror attached to the head coil at a viewing distance of 57‚ÄØcm. Head motion was minimized using foam padding. Stimulus presentation was controlled by MATLAB R2010a (The MathWorks Inc., Natick, MA, USA) using PTB-3 ( ;  ;   http://psychtoolbox.org/  ). Participants were instructed to pay attention to the images presented on the screen. \n",
      "\n",
      "--- Extract ---\n",
      "TaskName: fMRI food cue reactivity\n",
      "TaskDescription: Participants underwent fMRI scanning while viewing images of high-calorie foods, low-calorie foods, and neutral objects in a block design format.\n",
      "\n",
      "PMCID: 6699247\n",
      "--- Annotator ---\n",
      "TaskName: picture‚Äìword verification task\n",
      "TaskDescription: In this task, a simple line drawing of an object is presented (250‚ÄØms), and following a short delay (75‚ÄØms), a word appears. Subjects are instructed to respond by indicating whether the word was a semantic match (50% of trials) or non-match (unmatched, 50% of trials) to the preceding picture. Pictures consisted of 102 line drawings selected from ( ) and classified into 10 natural categories (clothing, animal, bird, appliance, tool, vehicle, vegetable, fruit, toy, and musical instrument). The full set of pictures comprised a block of 102 trials which was repeated across a total of four blocks, with the order of pictures varying across blocks. Half of the unmatched trials comprised picture-word pairs from a related semantic category (related co-hyponym, e.g., APPLE ‚Üílemon) and the other half comprised picture-word pairs from unrelated semantic categories (unrelated, e.g., APPLE ‚Üícow). Subjects were not asked to distinguish between related and unrelated unmatched target words. Occasional null events were inserted between trials, resulting in trial-to-trial intervals that ranged from 3 to 11‚ÄØs. Participants responded by button press with right or left index fingers to indicate if the word matched or did not match the picture. Left / right button presses were counterbalanced across participants. All participants performed several practice trials prior to ERP or fMRI recordings. No feedback was given to signal performance accuracy and participants were told to respond as quickly as possible without sacrificing accuracy. All subjects performed the task at >90% accuracy (See Supplementary Table 1. HC‚ÄØ=‚ÄØ95.0% and SZ‚ÄØ=‚ÄØ91.8%, F(1,45)‚ÄØ=‚ÄØ5.7295,   p  ‚ÄØ=‚ÄØ0.02). We found no effect of practice across the two sessions, performance accuracy was similar for the first and second session across groups (paired   t  -test, t-stat‚ÄØ=‚ÄØ‚àí0.70,   p  ‚ÄØ=‚ÄØ0.49) and remained insignificant after controlling for the time difference between sessions (p‚ÄØ=‚ÄØ0.49). Moreover, there was no effect of session order or time between sessions on the loading weight of the joint component reported below (ANCOVA model with covariates of group, session order and times between sessions). Reaction latency data were collected, and the median reaction times per subject, per condition (matched, unmatched) was determined (see Supplementary Methods and Resul\n",
      "--- Extract ---\n",
      "TaskName: picture-word matching task\n",
      "TaskDescription: Participants performed a picture-word matching task, in which words were either matched by preceding pictures, or were unmatched by semantically related or unrelated pictures.\n",
      "\n",
      "PMCID: 6715348\n",
      "--- Annotator ---\n",
      "TaskName: fear conditioning and extinction paradigm\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Fear Conditioning\n",
      "TaskDescription: Participants underwent a differential fear conditioning paradigm during 7T magnetic resonance imaging. An event-related design allowed separation of fMRI signals related to the visual conditioned stimulus (CS) from signals related to the subsequent unconditioned stimulus (US; an aversive electric shock).\n",
      "\n",
      "PMCID: 6831914\n",
      "--- Annotator ---\n",
      "TaskName: Monetary Incentive Delay (MID) Task\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Monetary Incentive Delay (MID) task\n",
      "TaskDescription: Participants completed a version of the Monetary Incentive Delay (MID) Task designed to elicit neural and behavioral responses to monetary incentives and their outcomes during FMRI scan acquisition. Each trial began with presentation of a visual cue indicating the valence (gain or loss) and magnitude of incentives. After viewing the cue, participants were shown a fixation cross for a variable interval, followed by a target that briefly appeared. Participants were instructed to try to press a button before the disappearance of each target to either gain or avoid losing the previously cued amount of money. After a second variable delay, participants received feedback informing them of the amount they had gained or lost on each trial.\n",
      "\n",
      "PMCID: 6847532\n",
      "--- Annotator ---\n",
      "TaskName: Go/No-Go\n",
      "TaskName: social Go/No-Go\n",
      "TaskDescription: While completing an fMRI scan, participants completed both a control go/no-go, which was used solely to establish baseline cognitive performance in the absence of socioaffective cues, and a social go/no-go task ( ) adapted from prior research ( ), which was used to assess emotion regulation in the context of socioaffective cues. The control go/no-go task consisted of four blocks, each containing 25 trials. The control task was completed prior to the social go/no-go, which included four aversive and four appetitive blocks, which were presented in a randomized order. Participants were presented with blocks of socially appetitive or aversive scenes for 300¬†ms, after which a letter was superimposed on the image for 500¬†ms. During this 500¬†ms window, participants were instructed to respond as quickly as possible by pushing a button for every letter shown (‚Äògo‚Äô) except the letter ‚ÄòX‚Äô (‚Äòno-go‚Äô). The control go/no-go task was identical in design structure, but did not include superimposed images (rather, a white square was presented on a black screen for 300¬†ms, after which a black letter was superimposed on the white background for 500¬†ms). In both task variants, 28% of the trials were no-go trials, which created a prepotent response to press, requiring inhibition on no-go trials. A jittered Intertrial interval (ITI) was presented between trials, averaging 1200¬†ms. In total, the social go/no-go consisted of 100 trials per condition across eight randomized blocks. Socially appetitive blocks included scenes of people celebrating, cooperating, and being affiliative, while socially aversive blocks included scenes of people excluding one another, bullying peers, and showing negative affect (see   for selection and reliability of the stimuli). The task was programmed and presented using E-Prime 2.0 (2012). \n",
      "--- Extract ---\n",
      "TaskName: Go/No-Go task\n",
      "TaskName: social Go/No-Go task\n",
      "TaskDescription: Participants completed a modified go/no-go task during which they were instructed to inhibit a prepotent behavioral response while distracted by socioaffective cues, which were either appetitive or aversive social stimuli.\n",
      "TaskDescription: The control go/no-go task was used solely to establish baseline cognitive performance in the absence of socioaffective cues.\n",
      "\n",
      "PMCID: 6969196\n",
      "--- Annotator ---\n",
      "TaskName: emotional go/no-go paradigm\n",
      "TaskDescription: A modified emotional go/no-go task was created with stimuli selected based on data from the pilot phase ( ). Within a go/no-go paradigm participants are instructed to press a button as quickly as possible when shown a ‚Äògo‚Äô (i.e., target) stimulus and to inhibit their response by not pressing the button when shown a ‚Äòno-go‚Äô (i.e., non-target) stimulus.   \n",
      "Go/no-go task design. The figure displays three trials in a run with calm faces as target stimuli and angry faces as non-targets. \n",
      "  Fig. 1   \n",
      "\n",
      "The rapid event-related task comprised four blocks presented across four different runs. Each block contained two facial emotions (calm and happy, or calm and angry), one instructed to be the target and one as the non-target stimulus, leading to four conditions: (i) happy go/calm no-go, (ii) happy no-go/calm go, (iii) angry go/calm no-go, and (iv) angry no-go/calm go. \n",
      "\n",
      "At the start of each block participants were shown a screen indicating the target emotion for that block, and reminding them not to respond to other emotions. Each block consisted of 16 trials, with targets (‚Äògo‚Äô) occurring on 75% of these trials, resulting in a total of 96 angry/happy go trials (48 happy, 48 angry), 32 angry/happy no-go trials (16 happy, 16 angry), 96‚ÄØcalm go trials and 32‚ÄØcalm no-go trials across the four runs (256 trials in total). Trials within a block, and block order within a run, were randomized across participants. Each trial started with a face, which was displayed for 500‚ÄØms, followed by a fixation cross which was displayed for a variable interstimulus interval between 1500‚ÄØms‚Äì2500‚ÄØms (in steps of 500‚ÄØms). After the last trial of a block, the fixation cross was displayed for 10‚ÄØs. During the last trial of a run the fixation cross was displayed for 20‚ÄØs in order to acquire the final BOLD response in full. \n",
      "--- Extract ---\n",
      "TaskName: emotional go/no-go fMRI task\n",
      "TaskDescription: Participants were administered an emotional go/no-go fMRI task comprising angry, happy and calm faces. Participants were instructed to respond to one emotion and ignore the other.\n",
      "\n",
      "PMCID: 6969350\n",
      "--- Annotator ---\n",
      "TaskName: passive viewing task\n",
      "TaskDescription: Forty-five short video-clips were taken from a larger set created and validated by  . Each clip depicted one actor, dressed in black against a green background, moving in an angry, happy or neutral manner. Six actors were males and nine females, with each actor recorded three times for each of the three emotions. The videos were recorded using a digital video camera and were edited to two-second long clips (50 frames at 25 frames per second). The faces in the videos were masked with Gaussian filters so that only information from the body was perceived (for full details and validation of stimuli (see   and  )). In addition, to use as control stimuli, we selected videos depicting non-human moving objects (e.g. windscreen wipers, windmills, metronomes, etc.) from the internet. We edited these clips using Adobe Premiere so that they matched the body stimuli in terms of size, resolution, and luminance. A green border matching the colour of the human video background was added. In the fMRI experiment, stimuli were presented in blocks of five clips (10‚ÄØs). \n",
      "--- Extract ---\n",
      "TaskName: Passive viewing\n",
      "TaskDescription: Participants passively viewed short videos of angry, happy or neutral body movements.\n",
      "\n",
      "PMCID: 6970153\n",
      "--- Annotator ---\n",
      "TaskName: asymmetric matching pennies game\n",
      "TaskDescription: The participants played the ‚Äòasymmetric matching pennies game‚Äô in the MRI scanner with three types of opponents: the human opponent (HUM) who played the game outside the scanner and the two artificial agents (FIX and LRN). FIX‚Äôs choices were determined stochastically according to the mixed Nash strategy, while LRN used a machine-learning algorithm to attempt to predict and exploit the participant‚Äôs choices. In the instructions, the human opponent was described as a student of the same sex at the same university, FIX as a computer program that would always follow a fixed, economically rational strategy and LRN as a computer program that would constantly learn to predict the participant‚Äôs choices through interaction. \n",
      "\n",
      "At the beginning of each trial, a cue indicating the type of opponent (HUM, FIX or LRN) was presented for 0.5¬†s ( ). Then two choice options were presented on the left and right sides of the display. The participant was asked to choose either option within 2¬†s. Immediately after either button was pressed, the frame of the chosen option was colored red. After a jittered fixation duration (2, 4 or 6¬†s), the choice of the opponent was indicated by a green frame, along with the outcome amount for the participant displayed at the center. After a jittered inter-trial interval (ITI) (2, 4 or 6¬†s), the next trial began. \n",
      "  \n",
      "Task sequence (A) and payoff matrix (B). A. In each trial, a conditional cue was presented for 0.5¬†s. Two options were presented on the left and right sides of the display for 2¬†s. When the participant pressed a button, the frame of the chosen option was colored red. After a jittered fixation duration, the choice of the opponent was shown as a green frame along with the monetary outcome the participant obtained. ITI‚Äâ=‚Äâinter-trial interval. B. The participant won the game when his/her choice matched with the opponent‚Äôs choice. The participant received 60 JPY by winning with the left choice (‚Äòstar‚Äô) and 20 JPY by winning with the right choice (‚Äòdiamond‚Äô) but received nothing for losing the game. The combination of left/right, star/diamond and outcome amounts was counterbalanced across participants. \n",
      "  \n",
      "The participant won the game and received a monetary reward when their choice matched the opponent‚Äôs choice. As shown in  , the participant received 60 JPY if both participant and opponent selected the left choice (‚Äòstar‚Äô in this example) and 20 JPY if both selected the right choice (‚Äòdiamond‚Äô), while the opponent received nothing in either case. In contrast, when the two selections did not match, the participant received nothing, while the opponent received 40 JPY for ‚Äòstar‚Äô or 20 JPY for ‚Äòdiamond‚Äô. Thus, the game payoff was asymmetric between the two players. We adopted this asymmetric payoff to make the game-theoretic probabilistic mixed strategy different from a random 50‚Äì50 choice; if the payoffs were symmetrical, it would be impossible to distinguish the game-theoretically rational strategy from a purely random choice ( ;  ). The combination of left/right sides, star/diamond shapes and outcome amounts was counterbalanced across participants. The participant‚Äôs understanding of the payoff matrix was confirmed by a series of quizzes prior to the experiment. \n",
      "--- Extract ---\n",
      "TaskName: asymmetric matching pennies game\n",
      "TaskDescription: Participants played the asymmetric matching pennies game in the MRI scanner with three types of opponents: a human opponent (HUM) who played the game outside the scanner and the two artificial agents (FIX and LRN). FIX's choices were determined stochastically according to the mixed Nash strategy, while LRN used a machine-learning algorithm to attempt to predict and exploit the participants choices.\n",
      "\n",
      "PMCID: 6981017\n",
      "--- Annotator ---\n",
      "TaskName: simple motor task (SMT)\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: simple motor task\n",
      "TaskName: noxious thermal stimulation task\n",
      "TaskDescription: Participants completed a simple motor task (SMT) designed to elicit intrinsic network connectivity without the risk of participants falling asleep. The SMT required participants to respond to the direction of a projected arrow (left or right). Arrows were sequentially displayed for 500ms seconds at a time, with a total of 150 trials. Left and right arrows were shown in equal proportion. Interstimulus intervals were jittered and ranged from 1s to 4s each.\n",
      "TaskDescription: Participants completed a noxious thermal stimulation task, during which a noxious thermal stimulus was applied to the left medial forearm.\n",
      "\n",
      "PMCID: 7018765\n",
      "--- Annotator ---\n",
      "TaskName: Testing Emotional Attunement and Mutuality (TEAM)\n",
      "TaskDescription: his task builds on past studies that have utilized error processing paradigms in the study of ER (e.g.,  ;  ;  ) by specifically examining dyadic error processing. Parent-adolescent dyads completed the TEAM task while simultaneously undergoing fMRI scanning. This task was developed to examine brain activation in both parents and adolescents when the other member of the parent-adolescent dyad makes a costly error. It thus allows us to probe emotion reactivity and regulation in response to being ‚Äúlet down‚Äù by the other person within the context of a relationship and processing an error made by a family member. The costly errors result in a monetary loss for the dyad, which evokes negative affect in participants, particularly because they themselves responded accurately ( ). In order to continue performing the task on the next trial, participants must regulate their emotional response to their partner‚Äôs error (implicit ER).\n",
      "TaskDescription: The TEAM task is an event-related design and consists of 17 trials during which participants first see a pattern of colored arrows presented sequentially on the screen for 3 s, twice in a row (totaling 6 s). They are then given 4 s to reconstruct the sequence by pressing colored buttons on a response box (shown in  ). At the end of the 4-s response window, participants see a message with feedback regarding both dyad members‚Äô performance on that trial. Prior to the scan, participants are told that if one or both members of the parent-adolescent dyad respond incorrectly to a trial, they will lose $5 from a starting amount of $50. \n",
      "--- Extract ---\n",
      "TaskName: TEAM task\n",
      "TaskDescription: The TEAM task is an event-related design and consists of 17 trials during which participants first see a pattern of colored arrows presented sequentially on the screen for 3 s, twice in a row (totaling 6 s). They are then given 4 s to reconstruct the sequence by pressing colored buttons on a response box. At the end of the 4-s response window, participants see a message with feedback regarding both dyad members performance on that trial.\n",
      "\n",
      "PMCID: 7235961\n",
      "--- Annotator ---\n",
      "TaskName: Pain empathy task\n",
      "TaskDescription: During the pain empathy task, participants received a cue (2¬†s) if the electrical shock was directed at themselves (arrow pointing left, self-directed trial;  ) or at another participant (arrow pointing right, other-directed trial;  ). Additionally, the color of the arrow informed the participant about the upcoming stimulation intensity (red, painful; green, non-painful). The other participant was a member of the experimental team and actually never received any shocks. After a brief delay jittered between 2 and 5¬†s (mean‚Äâ=‚Äâ3.5¬†s), participants saw a photo of the shock recipient on the screen (1¬†s; self-directed trial: scrambled photo of themselves; other-directed trial: photo of the confederate with painful/neutral facial expression), and a brief electrical shock (500¬†ms) was delivered (during self-directed trials only). This was followed by a fixation period ranging from 3 to 7¬†s (mean‚Äâ=‚Äâ5¬†s) and affect ratings (6¬†s) which were collected during one-third of the trials (self-directed pain ratings: ‚ÄòHow painful was this stimulus for you?‚Äô, other-directed affect ratings: ‚ÄòHow painful was this stimulus for the other person?‚Äô, and ‚ÄòHow unpleasant did it feel when the other person was stimulated?‚Äô). Trials were separated with a short fixation period (2¬†s). In total, participants completed 15 trials per condition (i.e. self-directed painful, self-directed non-painful, other-directed painful, other-directed non-painful). The task was programmed and presented with Cogent (version 1.32,   www.vislab.ucl.ac.uk/cogent.php  ) and lasted for approx. 16¬†min. \n",
      "\n",
      "Stimulation intensities during self-directed trials were set to individually calibrated stimulation intensities related to pain ratings of 1 (i.e. non-painful trial) and 7 (i.e. painful trial) throughout the task. The average stimulation intensities were 0.15‚Äâ¬±‚Äâ0.14¬†mA (mean‚Äâ¬±‚ÄâSEM; pain intensity rating of 1) and 0.74‚Äâ¬±‚Äâ0.6¬†mA (pain intensity rating of 7) during non-painful and painful trials, respectively. \n",
      "\n",
      "--- Extract ---\n",
      "TaskName: Pain empathy task\n",
      "TaskDescription: Participants received a cue indicating whether the electrical shock was directed at themselves or at a confederate, followed by a photo of the shock recipient and a brief electrical shock during self-directed trials.\n",
      "\n",
      "PMCID: 7377905\n",
      "--- Annotator ---\n",
      "TaskName: social perceptual decision task\n",
      "TaskDescription: On each trial, subjects made a group decision about a visual stimulus with one of four partners. Subjects were told that the partners were created by replaying the responses of four people performing the perceptual task on a separate day but, in reality, the partners were simulated. First, subjects judged whether a field of dots was moving left or right. Next, after being informed about the identity of their partner on the current trial, subjects were asked to report their confidence in the perceptual judgement ‚Äì an estimate which would enter into the group decision. Subjects were then shown the current partner‚Äôs response for that trial. Finally, implementing a common group decision rule ( ), the individual decision made with higher confidence was automatically selected as the group decision, after which feedback about its accuracy was delivered. Subjects were incentivised to help each group achieve as many correct group decisions as possible but, by design, could only affect group decisions through their confidence reports. \n",
      "--- Extract ---\n",
      "TaskName: social perceptual decision task\n",
      "TaskDescription: Subjects performed a social perceptual decision task in separate prescan and scan sessions. Each trial began with the presentation of a fixation cross at the centre of a circular aperture. After a uniformly sampled delay, subjects viewed a field of moving dots inside the aperture. Once the stimulus terminated, subjects had to press one of two buttons to indicate whether the average direction of dot motion was left or right. After making a choice, subjects were presented with a screen informing them about their partner on the current trial. Subjects then indicated their confidence in the perceptual decision, by moving a marker along a scale from 1 to 6. Finally, subjects received feedback about the accuracy of the group decision.\n",
      "\n",
      "PMCID: 7562935\n",
      "--- Annotator ---\n",
      "TaskName: Attitude conformity fMRI task \n",
      "TaskDescription: Participants completed the Attitude Conformity task during fMRI. On each trial, participants first viewed a behavior they previously rated (but were not reminded of their original ratings) (2 s). Following a jittered inter-stimulus interval (  M   = 2 s), participants were then shown their parents‚Äô and peers‚Äô ratings on each behavior and instructed to choose which person they agreed with most (maximum of 5 s). Participants pressed the left index finger when they agreed with their parent or right index finger when they agreed with their peer. Participants‚Äô choices were self-paced, such that the task advanced to the next behavior upon participant response. Behaviors were presented in random order and were separated by jittered inter-trial fixation periods (  M   = 2 s). Conformity was operationalized as choosing the person whose rating conflicted with the adolescent‚Äôs original rating, whereas resistance was operationalized as choosing the person whose rating was the same as the adolescent‚Äôs original rating (described below). \n",
      "\n",
      "In order to examine decisions to conform in the face of conflicting attitudes, we tailored the task to each participant based on their ratings assessed during the behavioral session. Although we collected the parents‚Äô actual ratings during the behavioral session, and ostensibly collected peers‚Äô ratings, such ratings were not used as we carefully manipulated the ratings to fall within the attitude conflict and social influence conditions described below. Of the 200 behaviors that participants originally rated at the behavioral session, 120 behaviors were selected for the fMRI task based on two criteria. First, the participant‚Äôs rating for a behavior needed to fall between minimum and maximum plausible ratings determined for each behavior based on pilot data, thereby maximizing ecological validity and checking for deviant responding (e.g., rating ‚Äúcheating on a test‚Äù as 10=very good was outside the range of plausibility). Second, given that extreme ratings may be less likely to change ( ), the participant‚Äôs rating for a behavior could not fall at the extremes of the scale (i.e., 1 or 10), ensuring that their parents‚Äô and peers‚Äô ratings could be manipulated to be below, above, or centered at participants‚Äô original ratings. Thus, the strength of participants‚Äô original ratings was relatively moderate across the subset of behaviors included in the fMRI task, with a balanced distribution across constructive and unconstructive behaviors. \n",
      "\n",
      "--- Extract ---\n",
      "TaskName: Attitude Conformity task\n",
      "TaskDescription: Participants completed the Attitude Conformity task during fMRI. On each trial, participants first viewed a behavior they previously rated (but were not reminded of their original ratings) (2 s). Following a jittered inter-stimulus interval (M = 2 s), participants were then shown their parents and peers ratings on each behavior and instructed to choose which person they agreed with most (maximum of 5 s). Participants pressed the left index finger when they agreed with their parent or right index finger when they agreed with their peer. Participants choices were self-paced, such that the task advanced to the next behavior upon participant response. Behaviors were presented in random order and were separated by jittered inter-trial fixation periods (M = 2 s).\n",
      "\n",
      "PMCID: 7649291\n",
      "--- Annotator ---\n",
      "TaskName: fine motor learning task\n",
      "TaskName: gross motor comparison task\n",
      "TaskDescription: The fine motor learning task involved accurate, rapid tracing of randomize trail mazes, with performance scores incorporating distance and error-rate. All scans were 6 min long with seven on-blocks of 24 s each during which a unique trail was displayed. These on blocks were interleaved with jittered rest blocks averaging 24 s. Each scan began and ended with a rest block. The trails were designed to be too complex to be completed in a single 24 s block. The gross motor training task followed the same block structure and visual display as the fine motor task, but simply had the participants move the mouse at random, with no accuracy metrics. \n",
      "--- Extract ---\n",
      "TaskName: fine motor learning task\n",
      "TaskName: gross motor training task\n",
      "TaskDescription: Participants used an MRI compatible mouse to guide a cursor through a marked trail displayed on the monitor, minimizing errors and maximizing distance traveled. The fine motor learning task involved accurate, rapid tracing of randomized trail mazes, with performance scores incorporating distance and error-rate. The gross motor training task involved moving the mouse at random, with no accuracy metrics.\n",
      "\n",
      "PMCID: 7689031\n",
      "--- Annotator ---\n",
      "TaskName: Regulation of Craving (ROC) Task\n",
      "TaskDescription: Participants were trained to decrease their desire to consume personally-desired (for task purposes these are referred to as ‚Äúcraved‚Äù foods) foods using cognitive reappraisal ( ;  ). Participants viewed unhealthy craved foods (‚ÄúCraved‚Äù condition), unhealthy not-craved foods (‚ÄúNot Craved‚Äù condition), or healthy vegetables (‚ÄúNeutral‚Äù condition). For unhealthy craved foods, participants either actively viewed the foods (‚ÄúLook‚Äù condition) or reappraised their craving for them (‚ÄúRegulate‚Äù condition). On ‚ÄúLook‚Äù trials, participants imagined how they would interact with the food if it were in front of them. On ‚ÄúRegulate‚Äù trials, participants reappraised the foods by focusing on the short- or long-term negative health consequences associated with consumption (e.g., stomach aches, weight gain, cavities, etc.). Participants generated several negative health consequences with the help of the experimenter, to ensure they had multiple reappraisals they could use during the task. To minimize demand characteristics (e.g., reduced craving ratings on regulate trials), participants were reassured they were not expected to be able to regulate well on every trial and were told that it was important to rate their cravings honestly. Neutral stimuli were only viewed under ‚ÄúLook‚Äù instructions, and are not used in the present analyses. To keep task time to a minimum, only craved foods were viewed under ‚ÄúRegulate‚Äù instructions. \n",
      "\n",
      "To maximize craving, participants selected their most craved and least craved food from the following menu of unhealthy food categories for the ‚ÄúCrave‚Äù and ‚ÄúNot Craved‚Äù conditions respectively: chocolate, cookies, donuts, French fries, ice cream, pasta, pizza. Stimuli were independently rated for desirability, such that the mean desirability of stimuli within each unhealthy food category did not differ significantly across categories ( ). As such, each participant viewed a set of individually adapted unhealthy food stimulus categories; across all participants, the only domain on which the ‚ÄúCraved‚Äù and ‚ÄúNot Craved‚Äù conditions differed was with regard to individual food preferences. Each condition (Look Neutral, Look Craved, Look Not Craved, and Regulate Craved) had 20 trials, presented across two task runs. On each trial (see  ), participants are presented with an instruction (2 s; Look or Regulate), viewed a food image while following the instruction (5 s), and rated their craving for the food on a 5-point Likert scale (4 s; 1 = not at all, 5 = very much). Each 11s trial of this event-related design was followed by a jittered fixation cross (  M   = 1 s) and trial order is optimized using a genetic algorithm ( ). Stimuli were presented using Psychtoolbox 3 ( ), and participants responded using a five-button box. \n",
      "--- Extract ---\n",
      "TaskName: food craving reactivity and regulation task\n",
      "TaskDescription: Participants viewed unhealthy craved foods (Craved condition), unhealthy not-craved foods (Not Craved condition), or healthy vegetables (Neutral condition). For unhealthy craved foods, participants either actively viewed the foods (Look condition) or reappraised their craving for them (Regulate condition). On Look trials, participants imagined how they would interact with the food if it were in front of them. On Regulate trials, participants reappraised the foods by focusing on the short- or long-term negative health consequences associated with consumption.\n",
      "\n",
      "PMCID: 7836234\n",
      "--- Annotator ---\n",
      "TaskName: Face-word Stroop task\n",
      "TaskDescription: The task comprised face-word stimuli consisting of male and female faces [Glasgow Face Database,  ], overlaid by the words ‚Äúman‚Äù or ‚Äúwoman‚Äù (in German language). These words could be either congruent or incongruent with the sex of the face ( A). The paradigm consisted of the 120 faces (60 males and 60 females) previously presented in the familiarization task. Faces were displayed for 1500¬†ms in the center of a white screen. We used an event-related design in order to identify separate trial-related Blood-oxygen-level-dependent (BOLD) responses. Trial onsets randomly varied with a pseudo-exponentially distributed SOA of 5000¬†ms (75%), 7500¬†ms (17%) and 10,000¬†ms (8%). The words were displayed in red ink superimposed on the faces. Both trial types were presented in a randomized order and occurred with a probability of 50%. Subjects were randomly assigned to one of two stimulus sets. Stimuli that were congruent in one group of subjects were incongruent in the other group. Subjects had to indicate whether a face was male or female by pressing the left or right mouse button (button assignments were counterbalanced across participants in each group). Throughout the entire paradigm, a black fixation dot was displayed in the middle of the face (and right below the word) to ensure accurate fixation.   \n",
      "--- Extract ---\n",
      "TaskName: Face-word Stroop task\n",
      "TaskName: Incidental memory task\n",
      "TaskDescription: Participants performed a face-word Stroop task during functional magnetic resonance imaging (fMRI) followed by a recognition task for the faces.\n",
      "TaskDescription: An incidental memory task of to the previously seen faces was conducted outside of the MRI scanner.\n",
      "\n",
      "PMCID: 7913329\n",
      "--- Annotator ---\n",
      "TaskName: monetary incentive delay task (MIDT)\n",
      "TaskName: Monetary incentive delay task (MIDT)\n",
      "TaskDescription: a bet (a dollar, a cent, or no money, randomly intermixed) appeared on the screen at the beginning of each trial. After a randomized fore-period between 1 and 5¬†s (uniform distribution), a target box was shown for a short period (response window, see below). Subjects were told to press a button as quickly as possible to collect the money (win) before the target box disappeared. An accurate trial was defined by a button press before disappearance of the target box. Otherwise, subjects would lose the bet, with the amount deducted from the total win. A premature button press prior to the appearance of the target box terminated the trial, and similarly resulted in loss. Feedback was shown on the screen after each trial to indicate the amount of money won or lost. Approximately 42% of all trials were dollar trials, 42% were cent trials, and ‚Äúno money‚Äù constituted the remaining trials. The inter-trial-interval was 1.5¬†s. The response window started at 300¬†ms, and was stair-cased for each trial type (dollar/cent/no money) separately; for instance, if the subject succeeded at two successive dollar trials, the window decreased by 30¬†ms, making it more difficult to win again; conversely, if a subject failed for two successive trials, the response window increased by 30¬†ms, making it easier to win. We anticipated that the subjects would win in approximately 67% each for dollar and cent trials. Each subject completed two 10-min runs of the task. Across subjects, there were 184‚Äâ¬±‚Äâ4 (mean‚Äâ¬±‚ÄâSD) trials in a study\n",
      "--- Extract ---\n",
      "TaskName: Monetary Incentive Delay Task (MIDT)\n",
      "TaskDescription: Participants pressed a button to collect reward ($1, 1, or nil), with the reaction time window titrated across trials so participants achieved a success rate of approximately 67%.\n",
      "\n",
      "PMCID: 8107785\n",
      "--- Annotator ---\n",
      "TaskName: Empathy for Pain Paradigm\n",
      "TaskName: Touch Paradigm\n",
      "TaskDescription: In this task, short-lasting (500¬†ms) and individually calibrated painful or nonpainful electrical stimulation was delivered to participants, or to another person (a confederate of the experimenters). Trials were structured as follows: First, an arrow (2000¬†ms) indicated the   target   (self vs. other) of the upcoming stimulus. The intensity of the upcoming stimulus was indicated by the color of this arrow (red: painful vs. green: nonpainful). After a jittered blank screen (3500‚Äâ¬±‚Äâ1500¬†ms), the electrical stimulus (500¬†ms) was delivered during simultaneous presentation of another visual delivery stimulus (1000¬†ms). The latter consisted of a picture of the confederate‚Äôs face, shown with either a painful or a neutral expression, or, in case of self-directed stimulation, scrambled versions of these pictures were shown to control for visual stimulation. Depending on the stimulus category, these pictures were accompanied by either a red (painful) or green (nonpainful) flash in the lower right corner of the picture. The delivery cue was followed by a fixation cross (5000‚Äâ¬±‚Äâ2500¬†ms), and an optional rating (self-directed: one rating question; other-directed: two rating questions; 6000-ms answering time per each question). After self-directed stimulation, participants rated their own pain (self-directed pain ratings), using the question ‚ÄúHow painful was this stimulus for you?‚Äù on a seven-point rating scale ranging from ‚Äúnot at all‚Äù to ‚Äúextremely painful.‚Äù After other-directed stimulation, participants rated the other person‚Äôs pain (other-directed pain ratings; ‚ÄúHow painful was this stimulus for the other person?‚Äù answered using the same seven-point rating scale as for the self-directed pain ratings), as well as their own unpleasantness during other-directed stimulation (unpleasantness ratings; ‚ÄúHow unpleasant did it feel when the other person was stimulated?‚Äù; seven-point scale, from ‚Äúnot at all‚Äù to ‚Äúextremely unpleasant‚Äù). Ratings were collected in about one third of the trials in a pseudorandomized fashion. Between trials, a fixation cross (2000¬†ms) was presented. In sum, 15 trials per condition (i.e., self-directed pain/no pain; other-directed pain/no pain) were presented. Participants were instructed to empathize with the other person. \n",
      "TaskDescription: Following the empathy for pain paradigm, we applied a touch paradigm ( ;  ) including 15 pleasant, 15 unpleasant and 15 neutral stimuli in pseudo-randomized order (see also  ). This paradigm consisted of two separate runs: In the first run (self-directed affective touch), the participant was stimulated to measure behavioral responses and brain activation related to the first-hand experience of affective touch. In the second run (empathy for affective touch) a confederate acting as a second participant was supposedly undergoing affective touch, and participants were instructed to empathize with her feelings. In every single self-directed trial, visual presentation of an object was accompanied by simultaneous stroking of the left palm at 1¬†Hz for 2¬†s in proximal-to-distal direction with a material whose touch resembled the touch of the object depicted on the screen. For example, touching the participant‚Äôs hand with down feathers was accompanied by the picture of a chick to elicit a pleasant affective touch experience. The stimuli had been selected in extensive pretesting based on maximum agreement among participants in terms of congruency between visual and somatosensory stimulus and emotional responses (see   for paradigm validation test). In one third of the trials (5 per condition), participants were asked to rate the stimulation in that trial on a 9-point scale ranging from very unpleasant (left extreme of the scale) to very pleasant (right extreme) for either themselves or, supposedly, for the other participant (i.e., the confederate). Each single trial consisted of a jittered fixation cross (5000‚Äâ+‚Äâ‚àí2000¬†ms), followed by visuo-tactile stimulation (2000¬†ms) and a jittered blank screen (1500‚Äâ+‚Äâ‚àí1000¬†ms). In trials with ratings, the rating was presented after the jittered blank screen for 5000¬†ms and was followed by another jittered blank screen (1500‚Äâ+‚Äâ‚àí1000¬†ms). Other-directed trials were identical apart from the absence of tactile stimulation of the participant, and the instruction that participants should empathize with their feelings. \n",
      "TaskDescription: self-unpleasant: control group‚Äâ>‚Äâplacebo group]\n",
      "TaskDescription: other-unpleasant: control group‚Äâ>‚Äâplacebo group\n",
      "TaskDescription: self-pleasant: control group‚Äâ>‚Äâplacebo group\n",
      "TaskDescription: other-pleasant: control group‚Äâ>‚Äâplacebo group\n",
      "--- Extract ---\n",
      "TaskName: Empathy for Pain Paradigm\n",
      "TaskName: Touch Paradigm\n",
      "TaskDescription: In this task, short-lasting (500ms) and individually calibrated painful or nonpainful electrical stimulation was delivered to participants, or to another person (a confederate of the experimenters). Trials were structured as follows: First, an arrow (2000ms) indicated the target (self vs. other) of the upcoming stimulus. The intensity of the upcoming stimulus was indicated by the color of this arrow (red: painful vs. green: nonpainful). After a jittered blank screen (35001500ms), the electrical stimulus (500ms) was delivered during simultaneous presentation of another visual delivery stimulus (1000ms). The latter consisted of a picture of the confederates face, shown with either a painful or a neutral expression, or, in case of self-directed stimulation, scrambled versions of these pictures were shown to control for visual stimulation. Depending on the stimulus category, these pictures were accompanied by either a red (painful) or green (nonpainful) flash in the lower right corner of the picture. The delivery cue was followed by a fixation cross (50002500ms), and an optional rating (self-directed: one rating question; other-directed: two rating questions; 6000-ms answering time per each question). After self-directed stimulation, participants rated their own pain (self-directed pain ratings), using the question How painful was this stimulus for you? on a seven-point rating scale ranging from not at all to extremely painful. After other-directed stimulation, participants rated the other persons pain (other-directed pain ratings; How painful was this stimulus for the other person? answered using the same seven-point rating scale as for the self-directed pain ratings), as well as their own unpleasantness during other-directed stimulation (unpleasantness ratings; How unpleasant did it feel when the other person was stimulated?; seven-point scale, from not at all to extremely unpleasant). Ratings were collected in about one third of the trials in a pseudorandomized fashion. Between trials, a fixation cross (2000ms) was presented. In sum, 15 trials per condition (i.e., self-directed pain/no pain; other-directed pain/no pain) were presented. Participants were instructed to empathize with the other person.\n",
      "TaskDescription: Following the empathy for pain paradigm, we applied a touch paradigm including 15 pleasant, 15 unpleasant and 15 neutral stimuli in pseudo-randomized order. This paradigm consisted of two separate runs: In the first run (self-directed affective touch), the participant was stimulated to measure behavioral responses and brain activation related to the first-hand experience of affective touch. In the second run (empathy for affective touch) a confederate acting as a second participant was supposedly undergoing affective touch, and participants were instructed to empathize with her feelings. In every single self-directed trial, visual presentation of an object was accompanied by simultaneous stroking of the left palm at 1Hz for 2s in proximal-to-distal direction with a material whose touch resembled the touch of the object depicted on the screen. For example, touching the participants hand with down feathers was accompanied by the picture of a chick to elicit a pleasant affective touch experience. The stimuli had been selected in extensive pretesting based on maximum agreement among participants in terms of congruency between visual and somatosensory stimulus and emotional responses.\n",
      "\n",
      "PMCID: 8318202\n",
      "--- Annotator ---\n",
      "TaskName: attentive listening\n",
      "TaskName: word repetition\n",
      "TaskDescription: During attentive listening, participants were asked to stay alert, still, and keep their eyes focused on a fixation cross while listening to a sequence of auditory sounds, including words, silence, and noise (single-channel noise vocoded words)\n",
      "TaskDescription: During word repetition, participants were asked to do the same as in attentive listening, with the addition of repeating the word they just heard aloud. Participants were instructed to repeat the words following the volume acquisition after each word\n",
      "--- Extract ---\n",
      "TaskName: Attentive Listening\n",
      "TaskName: Word Repetition\n",
      "TaskDescription: Participants were asked to stay alert, still, and keep their eyes focused on a fixation cross while listening to a sequence of auditory sounds, including words, silence, and noise (single-channel noise vocoded words).\n",
      "TaskDescription: Participants were asked to do the same as in attentive listening, with the addition of repeating the word they just heard aloud.\n",
      "\n",
      "PMCID: 8342928\n",
      "--- Annotator ---\n",
      "TaskName: Taylor Aggression Paradigm\n",
      "TaskDescription: To investigate the role of MT in altering neural signatures of retaliatory aggression, we administered a version of the TAP adapted for the fMRI scanner (e.g.,  ;  ). Recent research supports flexible use of the TAP, as it has shown to be psychometrically robust to variations in sampling, laboratory settings, and analytical approaches ( ;  ). Participants were informed that they would play an online computerized game with a participant situated in another lab. Participants were told that they would compete in multiple trials of a reaction time competition, in which the loser of each trial received an aversive noise blast through headphones, at one of four noise levels chosen by the other player. In reality, participants played against a preset computer program designed to produce four volume levels of white noise, with volume settings ranging from 1 (60 dB) to 4 (105 dB), in 22.5 dB intervals. The TAP consisted of 16 trials ( ). Each trial began with a fixation phase, followed by a decision phase, in which participants selected the volume of noise blast that their partner would receive if their partner lost the reaction time trial. Participants then viewed a fixation cross with a jittered duration (0.5/1.0/1.5 s) before the competition phase, during which participants were required to quickly press a button when a red square target was shown on-screen (5 s). Participants then viewed their opponent‚Äôs (pre-programmed) volume setting. This time point of notification, when the participant perceived the opponent‚Äôs intended noise blast setting, was modeled as the provocation phase (see  ). Finally, in the outcome phase, participants learned whether they won or lost the trial. The ‚Äúlosing‚Äù outcome phase, modeled as the punishment phase, subjected participants to a 5 s noise blast delivered by their opponent. Trials were characterized as retaliatory if they followed trials with high provocation (noise levels 3 or 4) and non-retaliatory if they followed trials with low provocation (levels 1 or 2). The 16-trial task contained eight retaliatory and eight non-retaliatory trials that were randomized across participants. Wins and losses were also randomly ordered across participants. Participants practiced the task first outside of the scanner to provide an opportunity for subjective evaluation of each noise level prior to neuroimaging assessment. \n",
      "--- Extract ---\n",
      "TaskName: retaliatory aggression task\n",
      "TaskDescription: a 16-trial game in which participants could respond to provocation by choosing whether or not to retaliate in the next round.\n",
      "\n",
      "PMCID: 8597975\n",
      "--- Annotator ---\n",
      "TaskName: antisaccade\n",
      "TaskDescription: We implemented an event-related design, mixing pro and antisaccades to faces and cars.   represents the time course of the task. Each trial started with a fixation cross displayed for periods ranging from 2000 to 8000¬†ms (sampled on an exponential distribution,  ), to introduce some jitter between trials onset and thereby improve design efficiency. A visual cue then appeared for 340¬†ms on the screen. The cue was a central disc (54‚Äâ√ó‚Äâ54 pixels, 1¬∞ visual angle) whose color (red or green) defined the nature of the saccadic task that should follow. A blank screen lasting 200¬†ms marked the transition between the visual cue (central disc) and the visual target stimulus (peripheral image) that was presented 10¬∞ to the left or 10¬∞ to the right of the screen center for 1000¬†ms. A green cue prompted the participants to look toward the appearing stimulus (prosaccade) and a red cue signaled that participants should look to the opposite side (to the mirror location) of the appearing stimulus (antisaccade). Participants were instructed to execute pro and antisaccades as quickly as possible. The experiment consisted of six 4-min long runs. Each run comprised 20 face and 20 car stimuli (40 trials in total) with 50% pro and 50% antisaccade instructions, randomized to the left and right hemi-field. We determined 6 different schedules (one for each run) using optseq2 (version 2.0, available at   http://surfer.nmr.mgh.harvard.edu/optseq  ), optimizing the order and timing of the 8 conditions (anti- or prosaccades toward faces or cars to the left or right side of the hemi field). The order of the 6 different schedules was counterbalanced across participants. We explained the task to the participants outside the scanner and had them perform a 2-min long practice session inside the scanner before the experiment started to ensure that the instruction was properly understood. Participants took breaks varying from a few seconds up to 2¬†min between runs, during which we reminded them of the instructions. \n",
      "--- Extract ---\n",
      "TaskName: Antisaccade task\n",
      "TaskDescription: Participants are asked to inhibit their reflexive eye movement to an abruptly appearing peripheral visual target and to reprogram a saccade in the opposite direction.\n",
      "\n",
      "PMCID: 8764488\n",
      "--- Annotator ---\n",
      "TaskName: Toulouse n‚Äêback Task (TNT)\n",
      "TaskDescription: The Toulouse n‚Äêback Task (TNT) was implemented in MATLAB (MathWorks) using the Psychophysics Toolbox extensions (Brainard,¬† ; Kleiner, Brainard, & Pelli,¬† ; Pelli & Vision,¬† ). The task is described in detail in a previous publication (Mandrick et al.,¬† ). The task was developed to combine a classical n‚Äêback task with mental arithmetic. Instead of memorizing and comparing unique items, as in the classical n‚Äêback task, the participants had to memorize and to compare the results of arithmetic operations, computed beforehand. Arithmetic operations were either additions or subtractions. All numbers were multiples of five (e.g., 15‚Äâ+‚Äâ40, 90‚Äì35). The arithmetic operations (trials) were presented for 2.5¬†s, followed by an interstimulus‚Äêinterval of 0.5¬†s. Volunteers were required to compute the result of the arithmetic operations and compare it with either a fixed number (0‚Äêback) or the result obtained two trials before (2‚Äêback). In the 0‚Äêback condition, the ‚Äútarget‚Äù fixed number was ‚Äú50.‚Äù Participants were therefore asked to press a specific button when the result of the operation was 50. In the 2‚Äêback condition, the participants were asked to press the button whenever the result of the arithmetic operation was identical to the one presented two trials ago (‚Äúmatch‚Äù), see Figure¬† . \n",
      "--- Extract ---\n",
      "TaskName: Toulouse nback task (TNT)\n",
      "TaskDescription: Participants performed a complex cognitive task that combined a classical nback task with mental arithmetic, requiring them to memorize and compare results of arithmetic operations.\n",
      "\n",
      "PMCID: 8857499\n",
      "--- Annotator ---\n",
      "TaskName: giving task\n",
      "TaskDescription: adolescents divided either a small or large number of coins (giving magnitude manipulation) between themselves and either a friend or unfamiliar peer (target manipulation) in an audience or anonymous context (peer presence manipulation)\n",
      "TaskDescription: #### Giving magnitude: giving small and large amounts \n",
      "  \n",
      "To assess the neural correlates of giving behavior, we used a modified fMRI version of the Dictator Game previously validated in adults ( ,  ). Participants divided 7 coins between themselves and another person - who could not reject the decision - in either a small or large giving condition. Giving was operationalized as the number of given coins. In the small giving condition, participants could give away 1, 2, or 3 out of 7 coins. In the large giving condition, participants could give away 4, 5, or 6 out of 7 coins. Participants could not give 0 coins, 7 coins, or make an equal split to warrant comparability between the small and large giving condition (see  ).   \n",
      "(A) The small and large giving conditions of the task, in which participants could give away 1, 2, or 3 coins, or 4, 5, or 6 coins, respectively (depicted in orange). Participants would keep the remainder of the 7 coins (depicted in yellow) to themselves. The name of the target (which could either be a friend or unfamiliar peer) was displayed at the top of the screen for each trial. (B) In two out of four blocks of the giving task, participants made anonymous giving choices. In the other two blocks, participants were aware that the peer audience depicted on the screen would observe their choices later in time (i.e., the audience condition). Note that the screens indicating whether blocks were audience or anonymous were only displayed at the start of each block, not during trials. Audience and anonymous blocks were presented in counterbalanced order. \n",
      "  Fig. 1   \n",
      "\n",
      "\n",
      "#### Familiarity of the target: friend and unfamiliar peer \n",
      "  \n",
      "The target of giving was either the participant‚Äôs closest friend (same-sex, similar-age) or an unfamiliar peer (same-sex, similar-age, anonymized participant of the same study). For each trial, the name of the target was displayed at the top of the screen (see  ). Participants were instructed that the coins they divided in each trial were worth actual money (i.e., 20 eurocents each) and that the computer would randomly select a few trials of the task to determine the payout of the participant, friend, and unfamiliar other. Accordingly, participants received a payment for themselves (  M   = ‚Ç¨.80,   SD   = ‚Ç¨.12) and their friend (  M   = ‚Ç¨.68,   SD   = ‚Ç¨.08), and experimenters transferred the payment to the unfamiliar other (i.e., another participant of the current study;   M   = ‚Ç¨.61,   SD   = ‚Ç¨.09). \n",
      "\n",
      "\n",
      "#### Peer presence: anonymous and audience giving \n",
      "  \n",
      "To assess the effects of peer presence on giving, the task consisted of two blocks in which participants made anonymous choices, and two blocks in which participants‚Äô choices were evaluated by peers later in time (see  ). The order of anonymous and audience blocks was counterbalanced across participants. In a practice session prior to the MRI session, participants viewed a video clip of six peers (three males and three females, aged 9‚Äì19) with neutral expressions. To the awareness of participants, these six peers were invited after study completion to observe and evaluate choices of participants. Trials in the anonymous blocks were not shown to anyone, as ‚Äì beknown to participants - experimenters covered the screen in the control room. \n",
      "--- Extract ---\n",
      "TaskName: Giving task\n",
      "TaskDescription: Participants divided either a small or large number of coins (giving magnitude manipulation) between themselves and either a friend or unfamiliar peer (target manipulation) in an audience or anonymous context (peer presence manipulation).\n",
      "\n",
      "PMCID: 9148994\n",
      "--- Annotator ---\n",
      "TaskName: spatial contextual memory task\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: spatial contextual memory task\n",
      "TaskDescription: The spatial contextual memory task consisted of an encoding, a subsequent consolidation, and a retrieval phase. The encoding task consisted of 64 trials where participants memorized objects and their positions. The consolidation phase was interrupted by a modulatory task that varied in extent to which consolidation processes were affected: interference task, reminder task, and control task.\n",
      "\n",
      "PMCID: 9261172\n",
      "--- Annotator ---\n",
      "TaskName: music listening\n",
      "TaskDescription: The fMRI task consisted of 24 trials altogether. In each trial, participants were first presented with a musical stimulus (lasting 20¬†s), then they were given the task of rating how familiar they found the music to be (familiarity rating lasted 2¬†s), and how much they liked the music (liking rating also lasted 2¬†s). Musical stimuli for the MRI task consisted of 24 different audio excerpts. Each auditory stimulus was from one of the following three categories: participant self-selected music (6/24 stimuli), other-selected (researcher-selected) music including well-known excerpts spanning multiple musical genres  (10/24 stimuli) and novel music spanning multiple genres (8/24 stimuli). A list of the researcher-selected musical selections is given in Supplementary Materials Table  . Stimuli were presented in a randomized order, and participants made ratings of familiarity and liking on the scales of 1 to 4: for familiarity: 1‚Äâ=‚Äâvery unfamiliar, 2‚Äâ=‚Äâunfamiliar, 3‚Äâ=‚Äâfamiliar, 4‚Äâ=‚Äâvery familiar; for liking: 1‚Äâ=‚Äâhate, 2‚Äâ=‚Äâneutral, 3‚Äâ=‚Äâlike, 4‚Äâ=‚Äâlove. Participants made these ratings by pressing a corresponding button on a button-box (Cambridge Research Systems) inside the scanner. Participants wore MR-compatible over-the-ear headphones (Cambridge Research Systems) over musician-grade silicone ear plugs during MRI data acquisition. The spatial mapping between buttons and the numerical categories of ratings were counterbalanced between participants to reduce any systematic association between familiarity or liking and the motor activity resulting from making responses. This fMRI task was completed before and after the MBI. \n",
      "--- Extract ---\n",
      "TaskName: Music Listening\n",
      "TaskDescription: Participants listened to musical stimuli and rated their familiarity and liking for each piece.\n",
      "\n",
      "PMCID: 9308012\n",
      "--- Annotator ---\n",
      "TaskName: Trust Game task\n",
      "TaskDescription: participants played with a computer, a stranger, and a close friend who accompanied them to the experiment.\n",
      "TaskDescription: MRI participants were told that they would be playing a game called the investment game in real time with their friend, the stranger and a computer partner. On a given trial of the task, the MRI participant would play with one of their three partners as indicated by a photo and name presented on the screen. Participants were instructed that they would start each trial with $8 and that they would have a choice between sending (investing) different proportions of that $8 to their partner on a given trial. The amounts that could be sent varied on a trial to trial basis, ranging from $0-$8. Participants would have up to 3 s to indicate via a button press on an MRI compatible response box which of the two investment options they preferred. Participants were instructed that whatever amount they chose to invest would be multiplied by a factor of 3 (i.e., an investment of $6 would become $18 for the partner), and that their partner could decide to split the multiplied amount evenly with them (reciprocate) or keep it all for themselves (defect). Upon entering their response, participants would see a screen that said ‚Äòwaiting‚Äô, (1.5 s) during which time they believed that their decision was being presented to their partner in another room in the research suite. After the waiting screen, a variable ISI was presented (mean = 1.42 s), and participants were then notified (2 s) whether their partner decided to split that amount evenly with them (reciprocate) or keep (defect) all of the money. Unbeknownst to participants, all outcomes were predetermined, and all partners were preprogrammed to reciprocate 50% of the time as per our previous work ( ). \n",
      "--- Extract ---\n",
      "TaskName: Trust Game\n",
      "TaskDescription: Participants played an economic trust game as the investor with three partners (friend, stranger, and computer) who played the role of investee.\n",
      "\n",
      "PMCID: 9454014\n",
      "--- Annotator ---\n",
      "TaskName: Food Cue Reactivity Task\n",
      "TaskDescription: It consisted of 44 food images [22 high energy-dense foods (HED), e.g., ice cream, cookies; 22 low energy-dense foods (LED), e.g., salad, fruit] matched on valence, arousal, image complexity, brightness, and hue, and 44 degraded images to serve as a visual baseline. Objective values of image properties were obtained with a photo editing program (GIMP, Berkeley, CA). Matching was confirmed by employee ratings. Degraded images were created from the food images using Image Shuffle (San Diego, CA). To improve signal in the primary task condition and contrast of interest, food images were presented twice each; degraded images were presented once. Each picture was presented for 1,750 ms followed by a fixation cross presented for 250‚Äì4,250 ms. Participants were asked to indicate whether they ‚Äúliked,‚Äù ‚Äúdisliked,‚Äù or felt ‚Äúneutral‚Äù about each image by pressing a corresponding button within 2,000 ms of image presentation; ratings and reaction times were logged via a fiber-optic response box. Total duration of the task was 5:54 min, and included an initial 9 s fixation period to allow for magnetization stabilization (excluded from analyses). \n",
      "--- Extract ---\n",
      "TaskName: Food Cue Reactivity Task\n",
      "TaskDescription: The Food Cue Reactivity Task was modified in-house from the Alcohol Cue Reactivity Task. It consisted of 44 food images [22 high energy-dense foods (HED), e.g., ice cream, cookies; 22 low energy-dense foods (LED), e.g., salad, fruit] matched on valence, arousal, image complexity, brightness, and hue, and 44 degraded images to serve as a visual baseline. Objective values of image properties were obtained with a photo editing program (GIMP, Berkeley, CA). Matching was confirmed by employee ratings. Degraded images were created from the food images using Image Shuffle (San Diego, CA). To improve signal in the primary task condition and contrast of interest, food images were presented twice each; degraded images were presented once. Each picture was presented for 1,750 ms followed by a fixation cross presented for 2504,250 ms. Participants were asked to indicate whether they liked, disliked, or felt neutral about each image by pressing a corresponding button within 2,000 ms of image presentation; ratings and reaction times were logged via a fiber-optic response box. Total duration of the task was 5:54 min, and included an initial 9 s fixation period to allow for magnetization stabilization (excluded from analyses). Prior to the scan session, participants practiced the task outside the scanner using non-food pictures and a computer keyboard.\n",
      "\n",
      "PMCID: 9837608\n",
      "--- Annotator ---\n",
      "TaskName: Synchronization task\n",
      "TaskDescription: To measure real-time synchronization among interacting participants, the study used a computer-based movement synchronization multiagent paradigm ( ). This game allows individuals to interact nonverbally by controlling the movement of circle-shaped figures with different colors. The displays are fully synchronized as the computers are connected via a closed network. During the game, each player faces a screen with a rectangle presented on it. The participants are instructed to imagine that the rectangle represents a room. At the beginning of the game, two circles appear on the screens, and each player is assigned one of them (blue, red). Participants are instructed to imagine that the circle represents them, as they are moving in the room. The participant in the scanner uses the response box, while the participant outside the scanner uses a keyboard to control the movement of the circles. \n",
      "\n",
      "The task includes three conditions. (i) Random condition‚Äîeach participant controls the movement of the circle that was assigned to them. The other circle‚Äôs movement is controlled by the computer and is randomized. The participants are aware that the other circle is controlled by a computer. (ii) Free condition‚Äîeach participant controls the movement of the circle that was assigned to them, and the other circle is controlled by the other participant. Participants are aware that the other circle is controlled by the other participant and are instructed to move their circle freely. (iii) Sync condition‚Äîthis condition is similar to the free condition; however, the participants are instructed to synchronize their movement to the best of their ability. The order of the conditions was maintained for all participants as was established in a previous study ( ) so that instructed synchrony will not affect the emergence of spontaneous synchrony right after it. \n",
      "--- Extract ---\n",
      "TaskName: movement synchronization\n",
      "TaskDescription: Participants were scanned while engaged in movement synchronization, using a novel dyadic interaction paradigm.\n",
      "\n",
      "PMCID: 9910278\n",
      "--- Annotator ---\n",
      "TaskName: emotion reappraisal\n",
      "TaskName: dietary self-control\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Emotion Reappraisal\n",
      "TaskName: Dietary Self-Control\n",
      "TaskDescription: Participants were presented with emotional stimuli and instructed to either view or reappraise the content to regulate their feelings. They rated their emotions after each trial.\n",
      "TaskDescription: Participants made choices about whether to eat presented food items, which were tailored based on individual taste and health ratings.\n",
      "\n",
      "PMCID: 9949505\n",
      "--- Annotator ---\n",
      "TaskName: Social incentive delay task\n",
      "TaskDescription: Each trial consists of an anticipation cue (circle, diamond or triangle), a jittered crosshair delay, a target (white square) signaling participants to press a button and feedback (e.g. reward/threat). Each cue and corresponding feedback depicted in the lower panel of the figure. \n",
      "--- Extract ---\n",
      "TaskName: Social Incentive Delay Task\n",
      "TaskDescription: Participants completed the Social Incentive Delay Task while undergoing fMRI to measure neural responses to anticipating social rewards and threats. Each trial began with a cue that signaled whether the upcoming image was a potential reward, threat, or neutral.\n",
      "\n",
      "PMCID: 10129386\n",
      "--- Annotator ---\n",
      "TaskName: verbal self-referential processing (SRP)\n",
      "TaskName: checking-in\n",
      "TaskDescription: participants were administered a set of trait adjectives from the Affective Norms of Emotion Words database ( ). The original 20 positive and 20 negative trait adjectives were translated to Swedish by the first author, slightly modified to make the words more relevant in Swedish, and divided into blocks of five words each, which were either all positive or all negative. To each block, one ‚Äòfluid‚Äô word and one ‚Äòconstant‚Äô word judged to be of the same valence as the rest of the block were added (total 16 words relating to fluidity/solidity). Examples of such words were ‚Äòbest√§ndig‚Äô (durable; positive-constant), ‚Äòdynamisk‚Äô (dynamic; positive-fluid), ‚Äòstatisk‚Äô (static; negative-constant), and ‚Äòflyktig‚Äô (volatile; negative-fluid). Half of the words, evenly distributed across positive and negative, were presented in uppercase, and the other half in lowercase letters. Each word was presented for 3 s, resulting in a total of 21‚Äâs per seven-word block. A fixation cross was presented for 4 s between each block. The blocks were presented three times, once in conjunction with each of three questions: ‚Äòdescribes me?‚Äô (Self condition), ‚Äòis positive?‚Äô (Valence condition), and ‚Äòis uppercase?‚Äô (Case condition). While Self was the condition of primary interest, Valence was included as a close control condition, and Case as a disparate control condition. Block order was the same across participants and was pseudo-randomized, so that no block type was presented two times in a row. The duration of this run was 10.48‚Äâminutes (540 TRs of 1.2 s each), and the field of view was 224‚Äâ√ó‚Äâ232‚Äâ√ó‚Äâ97 mm , comprising 44 dynamic slices. \n",
      "TaskDescription: This task was also presented through E-prime and consisted of simple math questions interspersed with presentations of shapes (circle, square, triangle, or arrow). Participants were instructed beforehand to ‚Äòfocus on the centre of your experience, the ‚Äúexperiencer‚Äù or ‚Äúobserver‚Äù‚Äô whenever the arrow was presented. Again, while Arrow was the condition of interest, Symbol was a close control condition, and Math a disparate control condition. The duration of math blocks (three questions per block) and arrow blocks were fixed to 12‚Äâs, whereas the duration of the other symbols was jittered (randomized between two and eight TRs), so that the average was 12‚Äâs per block of two symbols. The order of the three conditions (Math, Arrow, and Symbol) was the same for all participants and was pseudo-randomized, so that no block type was presented more than twice in a row. The duration of this run was 12‚Äâminutes (600 TRs of 1.2 s each), and the field of view was 224‚Äâ√ó‚Äâ232‚Äâ√ó‚Äâ97 mm , comprising 44 dynamic slices. \n",
      "--- Extract ---\n",
      "TaskName: Resting State\n",
      "TaskName: Self-Referential Processing (SRP) Task\n",
      "TaskName: Checking-in Task\n",
      "TaskDescription: Participants were instructed to keep their eyes closed and rest but not fall asleep and to not deliberately meditate.\n",
      "TaskDescription: During this task, participants were administered a set of trait adjectives from the Affective Norms of Emotion Words database, presented in blocks, with one fluid word and one constant word added to each block.\n",
      "TaskDescription: Participants were instructed to focus on the centre of their experience, the experiencer or observer whenever the arrow was presented.\n",
      "\n",
      "PMCID: 10458690\n",
      "--- Annotator ---\n",
      "TaskName: Classmates task\n",
      "TaskDescription: During the Classmates task, which was adapted from  , participants viewed yearbook photos of their peers from school. The yearbook photos (i.e., targets) used in the task were selected based on the sociometric data from the previous year (wave 1), because of the time required to process the sociometric data and create the scan task. However, we ran correlations between wave 1 and wave 2 for all four sociometric categories to establish that peer status was highly stable across years (i.e. High Popularity: r¬†=¬†0.67, p¬†<¬†.001; Low Popularity: r¬†=¬†0.76, p¬†<¬†.001; High Social Preference: r¬†=¬†0.80, p¬†<¬†.001; Low Social Preference: r¬†=¬†0.81, p¬†<¬†.001) in line with prior research ( ). To be selected as a target for the task, the peer needed to have a sociometric z-score between 1 and 5 (representing 1‚Äì5¬†SD above the mean on popularity/social preference in their school and grade) or between ‚àí¬†1 and ‚àí¬†5 (representing 1‚Äì5¬†SD below the mean on popularity/social preference in their school and grade). One version of the task was created for each grade level within each school (three middle schools, each with 2 grades, resulting in six versions total). The task had four conditions: High social preference (i.e., z-score between 1 and 5 on social preference), low social preference (i.e., z-score between ‚àí1 and ‚àí5 on social preference), high popularity (e.g., z-score between 1 and 5 on popularity), and low popularity (i.e., z-score between ‚àí1 and ‚àí5 on popularity). Within each condition, there were 10 targets, and we aimed for an equal number of boys and girls within each condition. Due to a data management error, z-scores for the targets from one of the schools (two of the six task versions) were incorrect and did not necessarily fall within the criteria (i.e., z-score between ¬±¬†1 and 5). Popularity and social preference scores were recalculated for the target images in these task versions. For one group of participants (n¬†=¬†20), there were a sufficient number of target images that still fit the criteria for each condition (High Popularity =¬†9, Low Popularity =¬†8, High Social Preference = 8, Low Social Preference =¬†10), so these participants were included. For the other group, there were an insufficient number of target images fitting the criteria (e.g., as few as 5) in the conditions, so these participants (n¬†=¬†15) were excluded. Across all versions of the task that were retained, the average z-score within each condition was approximately 2 (see supplement for details). There was no overlap in the targets across conditions, such that each target belonged in only one sociometric category and appeared in only one condition. No participants were included as targets so that no participant would see their own image. Target photos were obtained from school yearbooks from the previous school year, the same year the sociometric ratings were collected. Yearbook photos were digitized into JPEG images. \n",
      "--- Extract ---\n",
      "TaskName: Classmates Task\n",
      "TaskDescription: Participants viewed yearbook photos of their classmates from their school and grade, sorted by popularity based on sociometric ratings.\n",
      "\n",
      "PMCID: 10597625\n",
      "--- Annotator ---\n",
      "TaskName: single-cue conditioning paradigm\n",
      "TaskDescription: The experimental design was based on the one used by   and  . The main task consisted of three runs (i.e.¬†functional runs in the scanner) of 45 trials per run for a total of 135 trials across all three runs. Half the trials were CS-alone trials in which the US was omitted. In the other half of the trials (CS‚ÄìUS pairs), the CS co-terminated with the US. In each run, the 45 trials were presented as sequences of one-in-a-row (single), two-in-a-row (double) or three-in-a-row (triple) CS‚Äâ+‚ÄâUS trials and sequences of one-in-a-row (single), two-in-a-row (double) or three-in-a-row (triple) CS-alone trials¬†( ). The sequences were designed to conform to a binomial distribution of two equally probable events (CS-alone trials and CS‚Äâ+‚ÄâUS trials). In a given run, the various sequences were randomly shuffled, following a method adopted from Nicks (1959). Additionally, a CS-alone trial was added at the end of each run to measure expectancy ratings and brain activity related to the last sequence of the run. \n",
      "--- Extract ---\n",
      "TaskName: single-cue fear conditioning\n",
      "TaskDescription: Participants underwent a single-cue fear conditioning paradigm with a pseudo-random intermittent reinforcement schedule during functional magnetic resonance imaging.\n",
      "\n",
      "PMCID: 10615837\n",
      "--- Annotator ---\n",
      "TaskName: semantic judgment task\n",
      "TaskName: non-verbal tone judgment task\n",
      "TaskDescription: In both tasks, participants were required to decide whether an auditory stimulus matches with a presented image via yes/no-button press using the index and middle finger of their left hand. The left hand was used to shift motor activity related to the button press to the right hemisphere. The order of buttons was counterbalanced across participants. Tasks were presented in mini-blocks of four trials per task and blocks were separated by short rest intervals. Individual trials were 3.5¬†s long including presentation of auditory and visual stimulus, and button press by the participant. Each run included 88 stimuli with 32 items per condition of semantic judgment and 24 items of tone judgment. Participants completed two runs per session. \n",
      "--- Extract ---\n",
      "TaskName: semantic judgment task\n",
      "TaskName: tone judgment task\n",
      "TaskDescription: Participants were required to decide whether an auditory stimulus matches with a presented image via yes/no-button press using the index and middle finger of their left hand. The left hand was used to shift motor activity related to the button press to the right hemisphere. The order of buttons was counterbalanced across participants. Tasks were presented in mini-blocks of four trials per task and blocks were separated by short rest intervals. Individual trials were 3.5s long including presentation of auditory and visual stimulus, and button press by the participant.\n",
      "\n",
      "PMCID: 10637045\n",
      "--- Annotator ---\n",
      "TaskName: multiple-threat paradigm\n",
      "TaskDescription: On the study day, visceral and somatic pain thresholds were initially determined based on ratings of gradually increasing stimulus intensities, as previously accomplished.  Individual pain thresholds served as anchor for the subsequent calibration and matching procedures. During calibration, stimulus intensities for each pain modality were identified based on perceived pain intensity ratings within the target range of 60 to 80 mm on digitised visual analogue scales [VAS] with endpoints labelled ‚Äònot painful‚Äô [0] and ‚Äòextremely painful‚Äô [100], respectively. This procedure allows the determination of individual stimulation intensities [distension pressure for the visceral modality and temperature for the somatic modality] inducing adequately painful sensations for use in within-subject application of repeated pain stimuli during brain imaging. Afterwards, to ensure comparability across modalities, visceral and somatic perceived pain intensities were matched. For this purpose, visceral stimuli were presented together with thermal stimuli and participants were asked to compare the stimuli using Likert-type response options indicating more, less, or equally painful stimuli. If the rating showed a deviation, the intensity of thermal stimuli was successively adjusted until ratings indicated equal perception at least twice consecutively. As in our previous work,  stimulus durations were adjusted for each individual, aiming at matched durations of ascending [increasing pressure and temperature, respectively] and plateau phases [stable pressure and temperature, respectively] of visceral and thermal stimulation [total stimulus duration per stimulus: 20 s]. The stimulation intensities resulting from the matching procedure in patients with UC were comparable to those in healthy volunteers and patients with IBS, for both thermal stimuli [UC: 45.43‚ÄÖ¬±‚ÄÖ0.36¬∞C; HC: 45.20‚ÄÖ¬±‚ÄÖ0.30¬∞C; IBS: 43.83‚ÄÖ¬±‚ÄÖ0.70¬∞C] and rectal distensions [UC: 32.60‚ÄÖ¬±‚ÄÖ1.87 mmHg; HC: 38.16‚ÄÖ¬±‚ÄÖ2.25 mmHg; IBS: 34.78‚ÄÖ¬±‚ÄÖ2.15 mmHg]. \n",
      "--- Extract ---\n",
      "TaskName: Multiple-threat paradigm\n",
      "TaskDescription: Participants experienced visceral pain stimuli induced by pressure-controlled rectal distensions and somatic pain stimuli induced by cutaneous thermal pain on the forearm.\n",
      "\n",
      "PMCID: 10641579\n",
      "--- Annotator ---\n",
      "TaskName: Chatroom task\n",
      "TaskDescription: In this task, at the first visit, participants were asked to classify half of the 60 adolescents' photographs into the peers that they wanted to chat with (liked peers) and another half of the photographs into the peers that they did not want to chat with (unliked peers). At the second visit, in the fMRI scanner, participants received feedback indicating whether each of 60 adolescents were interested in chatting with them (acceptance) or not (rejection) or did not rate their interest (not rated), and reported how receiving this social feedback made them feel.\n",
      "TaskDescription: an experimental paradigm that simulates social evaluation ( ,  ). The task consists of a selection phase (out of scanner) and a feedback phase (in scanner) administered in two visits.\n",
      "TaskDescription: #### Visit 1 (Selection Phase) \n",
      "  \n",
      "Participants were told they were participating in a nationwide study about how teenagers communicate with each other on the internet, and that they would chat online with a peer selected for them based on similar interests from among participants at the other study sites. To enhance believability, participants created an online profile describing their interests and were told they would have their photograph taken. Participants then completed the selection phase whereby they viewed 60 photographs of mid- to late-adolescents (30 boys, 30 girls) and then placed 30 peers into an ‚Äúinterested‚Äù (liked) and 30 into a ‚Äúnot interested‚Äù (unliked) onscreen bin. Peer photographs were taken from stimulus sets used in past studies (e.g.,  ). Participants were told the other peers would indicate whether they wanted to chat with the participant using the same procedure. \n",
      "\n",
      "\n",
      "#### Visit 2 (Feedback Phase) \n",
      "  \n",
      "During visit two, participants completed the feedback phase of the task while undergoing an MRI scan. Participants were told they would chat online at the end of the visit with the peer selected for them. The fMRI feedback task included 60 trials. On each trial, for 2¬†s, the photograph of each peer for whom participants had previously indicated their interest was displayed, and a reminder appeared about whether participants had judged the peer as one of interest (liked) or not (unliked). An inter-stimulus interval of 2, 4, 6, or 8¬†s was included in equal numbers per duration length across the 60 trials (i.e., 15 trials). Next, for 1¬†s, participants viewed feedback indicating whether the presented peer wanted to interact with the participants (i.e., acceptance feedback; ‚Äú  He/she LIKED you  ‚Äù), did not want to interact with the participants (i.e., rejection feedback; ‚Äú  He/she DID NOT LIKE you  ‚Äù), or did not rate their interest of the participants (i.e., ‚Äúnot rated‚Äù feedback; ‚Äú  NOT RATED  ‚Äù). We modified a previous version of the chatroom fMRI task ( ,  ) to include a ‚Äúnot rated‚Äù condition to have a neutral comparison event for use in fMRI analyses rather than a general baseline. Feedback types were pseudo-randomized with an equal number of trials (i.e., 15 trials) yielding 6 event types that combine participants‚Äô selections and peer feedback condition (i.e., acceptance/rejection/not rated from liked peers, acceptance/rejection/not rated from unliked peers). After feedback was displayed for 1¬†s, a rating bar was presented and participants indicated with an MRI response box, ‚ÄúHow does this make you feel?‚Äù on a scale of 1¬†=¬†very bad to 5¬†=¬†great within a 3-second response duration. An inter-trial interval of 2, 4, 6, or 8¬†s was included in equal numbers per duration length across the 60 trials (i.e., 15 trials). After the fMRI scan, participants were debriefed and told that no social evaluations were actually performed and they would not chat with a peer at the end of the visit. No adverse reactions to the debriefing occurred. \n",
      "\n",
      "--- Extract ---\n",
      "TaskName: Chatroom fMRI task\n",
      "TaskDescription: Participants completed a social evaluation fMRI task where they received fictitious feedback (acceptance, rejection) from peers they liked or disliked.\n",
      "\n",
      "PMCID: 10656574\n",
      "--- Annotator ---\n",
      "TaskName: Classmates task\n",
      "TaskDescription: The task had four conditions: high social preference (i.e.   z  -score between 1 and 5 on social preference), low social preference (i.e.   z  -score between ‚àí1 and ‚àí5 on social preference), high popularity (e.g. z-score between 1 and 5 on popularity) and low popularity (i.e.   z  -score between ‚àí1 and ‚àí5 on popularity). Within each condition, there were 10 targets (i.e.¬†40 targets for each participant), and we aimed for an equal number of boys and girls within each condition. Due to a data management error,   z  -scores for the targets in two of the six task versions were incorrect and did not fall within the criteria (i.e.   z  -score between‚Äâ¬±‚Äâ1 and 5). Popularity and social preference (i.e. likability) scores were recalculated for the target images in these task versions. For one group of participants (n‚Äâ=‚Äâ20), there were a sufficient number of target images that fit the criteria for each condition (High Popularity‚Äâ=‚Äâ9, Low Popularity‚Äâ=‚Äâ8, High Social Preference‚Äâ=‚Äâ8, Low Social Preference‚Äâ=‚Äâ10), so these participants were included, resulting in five groups of participants and study stimuli. For the other groups, there were an insufficient number of target images (e.g. as little as¬†5) in the conditions, so these participants (n‚Äâ=‚Äâ15) were excluded. The faces in each condition were equally divided by female and male adolescents. We also attempted to build a racially/ethnically diverse paradigm such that each condition contains some faces representing individuals from minoritized groups. Research suggests that social preference (i.e. likability) and popularity are two distinct but correlated sociometric constructs in adolescents ( ). For our stimuli, the correlation between social preference and popularity scores ranged from 0.36 to 0.68 (Group 1:   r  ‚Äâ=‚Äâ0.36,   P  ‚Äâ<‚Äâ0.05; Group 2:   r  ‚Äâ=‚Äâ0.68,   P  ‚Äâ<‚Äâ0.001; Group 3:   r  ‚Äâ=‚Äâ0.56,   P  ‚Äâ<‚Äâ0.001; Group 4:   r  ‚Äâ=‚Äâ0.42,   P  ‚Äâ<‚Äâ0.01, Group 5:   r  ‚Äâ=‚Äâ0.46,   P  ‚Äâ<‚Äâ0.005). The correlation between social preference and popularity scores in the larger dataset from which study stimuli were drawn was 0.46,   P  ‚Äâ<‚Äâ0.001. Each target belonged in only one sociometric category and there was no overlap between targets in the social preference and popularity conditions. We did so to maximize the difference between neural responses to sociometric likability and popularity and increase the statistical power in our analyses. No adolescent participants in this neuroimaging study were included as face stimuli. The average   z  -score within each condition was approximately 2 (absolute value; see  ). We ensured equal distributions of sociometric scores across schools and conditions for the stimuli. \n",
      "\n",
      "The Classmates task consisted of 16 blocks, four blocks per condition, presented across two runs. The blocks were presented in a randomized order. Within each block, there were 10 targets chosen to comprise that condition. Peer faces within each block were shown in a fixed order using a randomization algorithm. Participants saw each face 4 times total (2 in each run), with each condition having 40 total trials each. To achieve the power for neuroimaging analysis on visual stimuli, we repeated each peer face across the conditions, in line with previous studies (e.g.  ;  ). We used a 1-back task to ensure that adolescent participants were paying attention to these visual stimuli during scanning, such that each block contained one target that appeared twice in a row. In addition, implicit inferences of social attributes from faces often require face identity recognition processes ( ,  ). A large body of work has shown that 1-back paradigms successfully elicit face identity recognition processes in which participants invoke a mental representation of face identity in the absence of percept ( ;  ;  ). Participants were instructed to press a button with their right pointer finger when a face repeated. Adolescents were not explicitly told to track peer status, which allows us to investigate how the brain spontaneously supports adolescents‚Äô awareness of peer-based social hierarchies in real-world networks. Each face was shown for 1750‚Äâms, and a fixation cross was jittered around an average of 2301‚Äâms (range: 565.8‚Äì4936.8‚Äâms; see¬† ). We did not include a control condition (e.g. teenagers with equal social attributes from other social networks) to account for other social attributes as it may regress out the phenomenon of our research interest, given that other social attributes (e.g. facial attractiveness, trustworthiness) are inherently correlated with social status in humans ( ). \n",
      "--- Extract ---\n",
      "TaskName: Classmates fMRI task\n",
      "TaskDescription: Adolescents viewed yearbook photos of their peers from their school and grade, selected based on sociometric ratings of social preference and popularity.\n",
      "\n",
      "PMCID: 10791126\n",
      "--- Annotator ---\n",
      "TaskName: Empathy-for-pain task\n",
      "TaskName: Emotional reactivity task\n",
      "TaskDescription:  In trials of the Self condition, participants passively received electrical stimuli. In the Other condition, participants experienced how another person (a confederate) received electrical stimuli. The stimuli were either painful or not painful. In the cue phase, an arrow indicated the recipient (downwards: Self; right: Other) and the intensity (blue: not painful; red: painful) of the next stimulus. In the stimulation phase, the stimulus was delivered. After half of the trials, participants were asked to rate the last stimulus. The confederate depicted has given informed consent that his photograph can be published\n",
      "TaskDescription: Participants were presented pictures with different content (violent or neutral) and different context (real or game context). After observing a block of pictures, participants rated their current unpleasantness on a visual analog scale from 0 to 100\n",
      "--- Extract ---\n",
      "TaskName: Empathy-for-pain task\n",
      "TaskName: Emotional reactivity task\n",
      "TaskDescription: Participants either received electric stimuli themselves (Self condition), or saw images of the confederate indicating that he was currently receiving electric stimulation (Other condition). The stimuli were either painful (Pain condition) or perceptible but not painful (No Pain condition).\n",
      "TaskDescription: Participants were shown pictures of either neutral or violent content (factor Content). Additionally, the pictures depicted either real scenes, or scenes taken from the video game participants played during the gaming sessions (factor Context).\n",
      "\n",
      "PMCID: 10990450\n",
      "--- Annotator ---\n",
      "TaskName: Montreal Imaging Stress Task (MIST)\n",
      "TaskDescription: During the 5‚ÄÖmin   stress condition   ( A), participants were asked to answer math problems of varying difficulty under time constraints whilst receiving trial-by-trial on screen performance feedback (‚Äòcorrect‚Äô in green, ‚Äòerror‚Äô in red, or ‚Äòtimeout‚Äô in yellow). To answer, participants were provided a button box and instructed to navigate left or right on a rotary-dial to the correct digit (between 0 and 9). In addition, a performance bar at the top of the screen continuously displayed the ‚Äòaverage‚Äô performance of previous participants (artificially set to 80%) as well as the participant‚Äôs current performance. Participants were instructed to attain or surpass the average performance of their peers. To induce a high failure rate, the participant‚Äôs response time limit got adjusted throughout the task to enforce a range of approximately 20% to 45% correct responses (Dedovic et al.,  ). Specifically, participants were given 10% less time after three consecutive correct responses and 10% more time after three consecutive incorrect or timeout responses. To further induce psychosocial stress, participants were presented with a 5 sec on screen summary of their current performance and were reminded that their ‚Äòperformance should be close to or better than the average performance‚Äô. This summary was presented at five timepoints during the stress condition. In addition, participants received scripted negative verbal feedback in between runs from a member of the study team saying: ‚ÄòYour performance is below average. In order for your data to be used, your performance should be close to or better than the average performance. Please try as hard as you can next round‚Äô. \n",
      "\n",
      "--- Extract ---\n",
      "TaskName: Montreal Imaging Stress Task\n",
      "TaskDescription: The Montreal Imaging Stress Task (MIST) is a well-validated acute psychosocial stress paradigm combining the stress-eliciting effects of high cognitive demands (solving math problems under time pressure) with negative social feedback (on screen and verbally via the experimenter).\n",
      "\n",
      "PMCID: 11063816\n",
      "--- Annotator ---\n",
      "TaskName: six-alternative forced-choice cued-recognition task\n",
      "TaskDescription: On each trial, a cue and six potential targets were presented simultaneously on the screen. The cue was presented in the middle of the screen with the six possible targets; one target and five foils form the same category (e.g., if the target was hammer, the five foils would be other randomly selected objects from the other events, regardless of closed- vs open-loop or delay vs no-delay status) presented in two rows of three below the cue ( ). Participants had a maximum of 6‚ÄÖs to respond with a button response that corresponded to the target position on the screen and were instructed to be as accurate as possible in the time given. The position of the correct target was randomly selected on each retrieval trial. The cue and six targets were presented until a response was made or when the maximum 6‚ÄÖs limit was reached (M response times  ‚Äâ¬±‚Äâ  SD‚Äâ=‚Äâ2.86‚ÄÖs‚Äâ¬±‚Äâ0.42‚ÄÖs). Missing responses (i.e., responses that fell outside the 6‚ÄÖs response window) were treated as incorrect trials for both accuracy and dependency (M percentage of missing responses  ‚Äâ¬±‚Äâ  SD‚Äâ=‚Äâ3.54%‚Äâ¬±‚Äâ3.22%). A 2‚Äâ√ó‚Äâ2 (loop‚Äâ√ó‚Äâdelay) ANOVA, where the dependent variable was the proportion of missing responses, showed no significant effects (  F  s‚Äâ<‚Äâ0.37). Thus, any differences in dependency across conditions are unlikely to be caused by assuming missing responses would have been incorrect. Note also that due to the six-alternative forced-choice recognition test, the chance of guessing correctly was relatively low (‚àº16.7%). \n",
      "--- Extract ---\n",
      "TaskName: Episodic memory retrieval\n",
      "TaskDescription: Participants learned events that comprised multiple overlapping pairs of event elements (e.g., person-location, object-location, location-person). Encoding occurred either immediately before or 24h before retrieval. Using fMRI during the retrieval of events, the study assessed whether episodic events continue to be reinstated holistically and whether hippocampal pattern completion continues to facilitate holistic reinstatement following a period of consolidation.\n",
      "\n",
      "PMCID: 11078806\n",
      "--- Annotator ---\n",
      "TaskName: stopwatch task\n",
      "TaskDescription: For each trial, participants were presented with a stopwatch that automatically started; they were asked to presss a button to stop the stopwatch within 50 ms of the 5-s time point. If participants stopped the stopwatch within this time window, the trial was deemed a success. In contrast, if participants failed to stop the stopwatch within this time window, the trial was deemed as a failure. To experimentally manipulate the success/failure outcome (without being affected by participants‚Äô performance), participants were shown random numbers on the display of the stopwatch after 3 s; thus, they could not accurately calibrate the correct timing. In some blocks, participants completed the task with a focus on approach goals, where they earned points when they succeeded but did not lose any points when they failed. In other blocks, participants completed the same task with a focus on avoidance goals, where they lost points when they failed but did not earn points when they succeeded. \n",
      "\n",
      "Each block started with a short instruction indicating the type of the upcoming block (approach = ‚ÄúWill win points if you succeed‚Äù; avoidance = ‚ÄúWill lose points if you fail‚Äù). Participants needed to press a button within 5 s to indicate that they understood the nature of the upcoming block; when participants did not press a button within 5 s, they were told that they needed to respond quickly and shown the same instruction again. This block instruction was followed by a jittered ISI (between 3 to 7 s) and experimental trials. Each trial started with a cue for 2 s. The cue indicated one of the three different points (3, 1, and 0). In the approach blocks, participants were told that they would earn the presented amount of points when they were successful in the trial, but they would not lose any points if they failed in the trial. In the avoidance blocks, participants were told that they would lose the presented amount of points when they failed in the trial, but they would not earn any points when they were successful. The cue was followed by a jittered ISI (between 3 to 7 s), which was replaced by a stopwatch that automatically started. Participants needed to press a button to stop the stopwatch within the time frame described earlier. Once participants pressed the button, the stopwatch stopped and participants found out whether they were sucessful or not. The outcome was shown for 3.2 s, followed by a jittered ITI (3‚Äì7 s) and the next trial. When participants failed to press a button within 6 s after the stoptwatch started, they were told to press the button sooner and a new trial started. The three different point conditions were included to increase the unpredictability of the task and thereby encourage participants to pay attention to the cue of each trial. \n",
      "--- Extract ---\n",
      "TaskName: stopwatch task\n",
      "TaskDescription: Participants completed a stopwatch task while being scanned in an MRI scanner. They were instructed that they could win or lose points during the stopwatch task depending on their performance, and their overall goal was to earn the total point of zero or larger than zero at the end of the experiments.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_print_by_pmcid(has_taskname_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Annotation improvements- need more structured annotations as its not always clear if there is 1-2 tasks when multiple sections were annotated for the same task\n",
    "- Prompt improvements- moving to structured extraction would help pair task name w/ description\n",
    "   - Better instruction would be to specify to describe task in 1-3 sentences and have a separate prompt that required more details design information. \n",
    "- Overall assessment- Descriptions are reasonable if sometimes short and vague. The short description is probably quite useful for decision making in a semi-automated meta-analysis context\n",
    "- Outside of scanner tasks as proper annotations / extractions\n",
    "\n",
    "\n",
    "In the future, only show Task Names / Description from non-abstract if that's not already the case\n",
    "TaskDescription\n",
    "TaskDesignDetails (verbatim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMCID: 3825257\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: reading\n",
      "TaskDescription: Participants read three excerpts from the official translation of Bill Bryson's 'A Short History of Everything' on a computer screen in an individual testing booth. Text was presented one sentence at a time and participants received experience sampling probes at random intervals.\n",
      "\n",
      "PMCID: 4110030\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Resting State fMRI\n",
      "TaskDescription: Participants completed three sessions of resting state fMRI. They were instructed to relax and keep their eyes closed during scanning.\n",
      "\n",
      "PMCID: 4386762\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Resting state fMRI\n",
      "TaskDescription: Subjects were instructed to keep their eyes closed, to not think of anything in particular and to refrain from falling asleep.\n",
      "\n",
      "PMCID: 4488375\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: The task used in this experiment was programmed using PsychoPy [ ]. The task featured a 0-back and a 1-back condition that continuously switched from one another throughout the experimental session (see  ). Our paradigm is broadly similar to the paradigm used by Smallwood and colleagues [ ] and was modified with the specific aim of maximising the differences between the 0-back and the 1-back conditions. In both conditions participants saw different pairs of shapes (Non-Targets, NT) appearing on the screen divided by a vertical line; the pairs could be: a circle and a square, a circle and a triangle, or a square and a triangle for a total of 6 possible pairs (two different left/right configurations for each). The pairs never had shapes of the same kind (e.g. a square and a square). In both tasks a block of NT was followed by target requiring participants to make a manual response. The target was a small stimulus presented in either blue or red and the colour was counterbalanced across participants. In the 0-back condition the target was flanked by one of two shapes and participants had to indicate by pressing the appropriate button which shape matched the target shape. In the 1-back condition, the target was flanked by two question marks and participants had to respond depending on which side the target shape was on the prior trial. For the behavioural study responses were made using the left and right arrow keys, for the neuroimaging study responses were made using a button box. Importantly, unlike the paradigm employed by Smallwood and colleagues [ ] this design ensures that the participants cannot know what response to make when presented with the to be encoded stimulus. \n",
      "\n",
      "--- Extract ---\n",
      "TaskName: 0-back\n",
      "TaskName: 1-back\n",
      "TaskDescription: Participants alternated between two tasks. One task involved observing non-coloured shapes presented at fixation waiting for the presentation of a coloured slide at which point they would indicate using a button press which side of the fixation cross a target shape was (0-back). In the other task participants had to encode the identity of shapes presented on screen and when prompted by a coloured slide to respond based on the position of a specific target shape on the prior trial (1-back).\n",
      "\n",
      "PMCID: 4526228\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Resting State\n",
      "TaskDescription: Participants were asked to lie still with eyes closed during the resting state fMRI acquisition.\n",
      "\n",
      "PMCID: 4547715\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: A) The task was conducted in an event-related design. Six trials from either the liked or the uninteresting male acquaintance were combined in a set. Each set contained pseudo-randomized four target trials (shown in red) with two control trials (shown in blue). (B) Each trial was created by a 2 √ó 3 factorial design. In a target trial, female participants judged the gift attractiveness (1: the least attractive, 9: the most attractive). In a control trial, the participants simply pressed the button when a white dot appeared by the side of the portrait. For details, see  . \n",
      "  \n",
      "In each target trial, a portrait of a giver, a picture of a gift, and a rating scale were displayed simultaneously ( ). The participants used a keypad to rate the attractiveness of the gift. The keypad was separated for the left and right hands. The left five fingers corresponded to the ratings 1‚Äì5 and the four right fingers, except the little finger, corresponded to the ratings 6‚Äì9. The participants had practiced to press buttons correctly before the task. Gift attractiveness and reaction time (RT) for judgments were recorded. We conducted control trials to subtract brain activity on simply viewing the giver and pressing the buttons. In each control trial, a white dot appeared either on the left or on the right side of the portrait ( ). The participants pressed the button with the left/right index finger if the dot appeared on the left/right. \n",
      "\n",
      "--- Extract ---\n",
      "TaskName: Gift attractiveness judgment\n",
      "TaskDescription: Participants judged the attractiveness of gifts received from two male acquaintances, one preferred and one uninteresting, on a 9-point scale.\n",
      "\n",
      "PMCID: 4914983\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Viewing and re-viewing of comedy movies\n",
      "TaskDescription: Three comedy clips were shown twice to 20 volunteers during functional magnetic resonance imaging (fMRI).\n",
      "\n",
      "PMCID: 5090046\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Synonym Judgement\n",
      "TaskName: Verbal Fluency\n",
      "TaskDescription: Participants performed a synonym judgement task to index the capacity to understand the meaning of an external stimulus.\n",
      "TaskDescription: Participants had 1min to generate as many unique words as possible belonging to a semantic category (category fluency) or starting with a specific letter (letter fluency).\n",
      "\n",
      "PMCID: 5552726\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Cognition assessment\n",
      "TaskDescription: Cognition assessment including the Montreal Cognitive Assessment Beijing Version, the Minimum Mental State Examination, the Digit Symbol Test, the Rey Auditory Verbal Learning Test, and the Verbal Memory Test.\n",
      "\n",
      "PMCID: 5607552\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: a degraded sentence is first experienced as very difficult to understand and, after a single presentation of its intact counterpart, the intelligibility of this same degraded sentence reaches near‚Äêperfect level.\n",
      "TaskDescription: functional run consisted of 9 D‚ÄêI‚ÄêD stimulus sets, each of which comprised three blocks of six sentences (see Figure¬† ). The blocks were 22 or 24¬†s in duration. To prevent an overlap between the blood oxygenation level dependent (BOLD) responses elicited by each sentence block, the blocks were separated by periods of 16¬†s without auditory stimulation. Subjects were instructed to listen attentively to the sentences, to maintain their gaze on a central fixation cross, and to avoid moving during the duration of the experiment. After 1¬†s following the end of each D‚ÄêI‚ÄêD stimulus set, a question appeared on the screen for 5¬†s prompting the subject to indicate by a button press (yes/no) whether the distorted sentences were easier to understand when presented after the intact sentences.\n",
      "--- Extract ---\n",
      "TaskName: Speech Comprehension\n",
      "TaskDescription: Subjects listened to acoustically distorted sentences, followed by intact versions, and then the distorted sentences were presented again.\n",
      "\n",
      "PMCID: 5662713\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: Participants watched 24 short videos while in the MRI scanner. The videos lasted on average 38‚Äâseconds (range 29‚Äì48‚Äâs) and were taken from short films or videos posted on   www.YouTube.com  . All videos depicted a short narrative and were presented without sound. The stories centered around one character (5 videos), two main characters (8 videos) or an interaction of multiple characters (11 videos). 13 videos took place outside, 8 videos took place inside of a building and three videos switched between these types of locations. The order of the videos was pseudorandomized across participants. All stimuli were pre-experimentally unfamiliar to the participants. \n",
      "--- Extract ---\n",
      "TaskName: Memory retrieval\n",
      "TaskDescription: Participants watched 24 short videos while in the MRI scanner. The task involved encoding the videos, immediate retrieval, and delayed retrieval after one week.\n",
      "\n",
      "PMCID: 5716095\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Empathy Task\n",
      "TaskDescription: Participants saw individuals expressing emotional distress in negative contexts and were instructed to perceive the target face as either their family member or themselves.\n",
      "\n",
      "PMCID: 5895040\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskName: n/a\n",
      "TaskDescription: In the perceptual task, subjects were asked to report the brighter of two stimuli on each trial\n",
      "TaskDescription: In the memory task, subjects began each miniblock by learning a set of nine consecutively presented stimuli\n",
      "--- Extract ---\n",
      "TaskName: perceptual task\n",
      "TaskName: memory task\n",
      "TaskDescription: In the perceptual task, subjects were asked to report the brighter of two stimuli on each trial.\n",
      "TaskDescription: In the memory task, subjects began each miniblock by learning a set of nine consecutively presented stimuli. A stimulus from this set was then presented on each subsequent trial (in randomized order) alongside a new stimulus. The subjects' task was to identify the studied stimulus.\n",
      "\n",
      "PMCID: 6102316\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Naturalistic viewing of social interactions\n",
      "TaskDescription: Participants watched a 20-min movie narrative encompassing verbal and non-verbal social interactions.\n",
      "\n",
      "PMCID: 6331309\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: MRI data acquisition\n",
      "TaskDescription: High-resolution T1-weighted images were acquired from 116 subjects: 27 RLS patients, 22 migraine patients, 22 patients with comorbid migraine and RLS, and 45 healthy controls.\n",
      "\n",
      "PMCID: 6382839\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: script-driven imagery\n",
      "TaskDescription: Participants were exposed to disorder-related and neutral narrative scripts while brain activation was measured with fMRI. They were encouraged to imagine the narrative scripts as vividly as possible.\n",
      "\n",
      "PMCID: 6391069\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: Auditory stimuli were presented in 10 separate runs, each lasting 4 min. One run consisted of 56 trials of mother‚Äôs voice, unfamiliar female voices, environmental sounds and catch trials, which were pseudo-randomly ordered within each run. Stimulus presentation order was the same for each subject. Each stimulus lasted 956 msec in duration. Prior to each run, child participants were instructed to play the ‚Äòkitty cat game‚Äô during the fMRI scan. While laying down in the scanner, children were first shown a brief video of a cat and were told that the goal of the cat game was to listen to a variety of sounds, including ‚Äòvoices that may be familiar,‚Äô and to push a button on a button box only when they heard kitty cat meows (catch trials). The function of the ‚Äòcatch trials‚Äô was to keep the children alert and engaged during stimulus presentation. During each run, four or five exemplars of each stimulus type (i.e. nonsense words samples of mother‚Äôs and unfamiliar female voices, environmental sounds), as well as three catch trials, were presented. At the end of each run, the children were shown another engaging video of a cat. Although the button box failed to register responses during data collection in four children with ASD and nine TD children, data analysis of the catch trails for 17 children with ASD and 12 TD children showed similar catch trial accuracies between TD (accuracy¬†=¬†91%) and ASD groups (accuracy¬†=¬†89%; two-sample   t  -test results:   t  (2) = 0.35, p¬†=¬†0.73). Across the ten runs, a total of 48 exemplars of each stimulus condition were presented to each subject (i.e. 144 total exemplars produced by each of the three vocal sources, including the child‚Äôs mother, unfamiliar female voice #1, and unfamiliar female voice #2). Vocal¬†stimuli were presented to participants in the scanner using Eprime V1.0 (Psychological Software Tools, 2002). Participants wore custom-built headphones designed to reduce the background scanner noise to ‚àº70 dBA ( ;  ). Headphone sound levels were calibrated prior to each data collection session, and all stimuli were presented at a sound level of 75 dBA. Participants were scanned using an event-related design. Auditory stimuli were presented during silent intervals between volume acquisitions to eliminate the effects of scanner noise on auditory discrimination. One stimulus was presented every 3576 ms, and the silent period duration was not jittered. The total silent period between stimulus presentations was 2620 ms, and consisted of a 300 ms silent period, 2000 ms for a volume acquisition, another 300 ms of silence, and a 20 ms silent interval that helped the stimulus computer maintain precise and accurate timing during stimulus presentation. \n",
      "--- Extract ---\n",
      "TaskName: Voice Processing\n",
      "TaskDescription: Participants listened to auditory stimuli including mothers voice, unfamiliar female voices, and environmental sounds while undergoing fMRI scanning.\n",
      "\n",
      "PMCID: 6397754\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskName: watched\n",
      "TaskDescription: For this task, all participants were told that they would watch a clip from a nature documentary featuring Canadian bighorn sheep, and at times various words would appear and move around on the screen, but they were told to ‚Äúavoid reading these distractor words whenever they appear.‚Äù On average, dieters find this task significantly more effortful, compared to a control condition in which they just watched the film and read the words as they pleased, Cohen‚Äôs   d  ¬†=¬†1.04 (calculated from¬† ). The video lasted for seven minutes, with a jittered presentation of 40 words (all one-syllable and of neutral valence, e.g.,¬†‚ÄúCHAIR‚Äù and ‚ÄúBOOK‚Äù) throughout the task epoch. \n",
      "--- Extract ---\n",
      "TaskName: Effortful self-control task\n",
      "TaskName: Food cue reactivity task\n",
      "TaskDescription: Participants were required to actively inhibit reading a series of words that appeared on the screen over the course of seven minutes.\n",
      "TaskDescription: Participants engaged in a food-cue reactivity task involving food commercials.\n",
      "\n",
      "PMCID: 6821801\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: During the event-related fMRI sessions, participants performed either purchasing or perceptual decisions in alternating blocks (Fig.¬† ). Each good was presented twice: once during the purchasing decision and once during the perceptual decision. Before each block started, both blocks were cued visually so participants knew in which block they were performing. Hedonic and utilitarian goods were presented in a random order. There were four blocks, and each block had 76 stimuli.   \n",
      "Participants performed two types of task in relation to identical stimuli (book cover: hedonic/utilitarian). (  A  ) For purchasing decisions, participants indicated their willingness to pay a specific price for that book. (  B  ) For perceptual choices, participants indicated the total number of people and lines on the book cover. \n",
      "  \n",
      "\n",
      "To capture explicit hedonic/utilitarian values, we asked participants how much they would be willing to pay for the goods during purchasing decisions (willingness-to-pay: WTP), as previously described . We considered the WTP scores as the economic value of goods, and the WTP scores for hedonic/utilitarian goods as hedonic/utilitarian values, respectively. We also assumed that participants automatically calculate WTP without the need for purchasing behavior during exposure to the goods and, therefore, WTP reflects the explicit and task-irrelevant economic values of the goods during purchasing decisions . \n",
      "\n",
      "We followed the Becker‚ÄìDegroot‚ÄìMarshak (BDM) auction method, which is widely used in economics to incentivize participants to reveal their true values for goods without actually providing all the selected goods . In brief, participants were given money (450 yen in our case) for purchasing decisions and state their WTP for each item. They placed bids on the goods from a range of prices (0 yen, 150 yen, 300 yen or 450 yen in our case) as the stated WTP. At the end of the experiment, one trial was randomly selected, and then the experimenter generated a random selling price (from a known distribution of 0 yen, 150 yen, 300 yen, and 450 yen with equal probability in our case). If the random selling price was less than or equal to their stated WTP, the participant could purchase the goods at the random selling price and kept the remaining money. If the selling price was greater than their WTP, the participant could not purchase the item and keeps the entire 450 yen. Therefore, participants‚Äô optimal strategy was to state their true value for goods on every trial. \n",
      "\n",
      "Participants also performed perceptual decisions for a perceptual task. They viewed the cover of the book, and answered questions relating to perceptual levels: the total number of people and lines of the title in the book (by 2, 3, 4, or 5). Each book included at least one person and one line of title. They saw the same books during both decisions, but the order was pseudo-randomized to eliminate the order effects. Therefore, this procedure allowed motor responses and sensory input to be matched in both purchasing and perceptual tasks. \n",
      "--- Extract ---\n",
      "TaskName: Purchasing decisions\n",
      "TaskName: Perceptual decisions\n",
      "TaskDescription: Participants indicated their willingness to pay a specific price for that book during purchasing decisions.\n",
      "TaskDescription: Participants indicated the total number of people and lines on the book cover during perceptual decisions.\n",
      "\n",
      "PMCID: 7426775\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Brain Age Prediction\n",
      "TaskDescription: Predicting chronological age from structural MRI scans using a deep learning model.\n",
      "\n",
      "PMCID: 7582181\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Imagining social interactions\n",
      "TaskDescription: Participants were asked to vividly imagine themselves in a novel self-relevant event that was ambiguous with regards to possible social acceptance or rejection.\n",
      "\n",
      "PMCID: 7859438\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: It measures predominantly memory encoding, but also perception and attention in both the auditory and visual domains within 10 min of fMRI acquisition time using simple instructions. To our knowledge, memory-encoding paradigms so far presented stimuli of one sensory condition or did face‚Äìname associative memory tasks (Sperling,  ; Barch et al.,  ; Nenert et al.,  ; Sidhu et al.,  ; Hayes et al.,  ) within a similar time frame. We optimized our task to allow mapping of a versatile number of contrasts that are relatively straightforward to interpret. To enable the separation of sensory-specific and sensory-unspecific activities (Wheeler et al.,  ; Daselaar et al.,  ; Langner et al.,  ), we used two sensory modalities, auditory and visual. Twenty-five percent of the total time consisted of passive rest blocks as baseline/rest condition (Gusnard and Raichle,  ). Each sensory condition contained two distinct sub-conditions to cover a wide range of information on visual and auditory system activations as well as joined activation for sensory-unspecific functions like overall memory. Within the visual condition, we chose to present faces and spatial scenes, motivated by work on face-selective and scene-selective brain regions (Kanwisher et al.,  ; Epstein and Kanwisher,  ; Gazzaley et al.,  ; Collins and Dickerson,  ). Further, those stimuli seemed to show differences in age-related reductions in neural dedifferentiation, which makes them interesting for longitudinal studies \n",
      "--- Extract ---\n",
      "TaskName: Memory Encoding Task\n",
      "TaskDescription: A functional MRI paradigm designed to map memory encoding across auditory and visual sensory conditions using a mixed design.\n",
      "\n",
      "PMCID: 8104963\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: The main experimental session was organized in three blocks. In the first block, participants saw a fixation cross (1 s) followed by one video (3 s) and were asked to estimate the amount of pain experienced by the person in the video, by moving a randomly-presented rectangular cursor on a Visual Analogue Scale (VAS) ranging from ‚Äòno pain at all‚Äô (corresponding to value 1) to ‚Äòthe worst pain imaginable‚Äô (corresponding to value 10). Following the rating (7 s), participants were exposed to two simultaneous feedbacks (3 s) that represented the judgement of the   Target   of the video and the average judgement of 20 emergency doctors that previously rated the same video-clips. These two feedbacks were displayed by cursors similar to that of the participant but of different colors (color codes were randomized across participants). Each kind of feedback was independent and orthogonal as respect to the other and described pain levels, which could be higher, equal or lower, compared to the participant. This led to a 3 √ó 3 design, with nine different conditions. Feedbacks were located on the scale according to a normal distribution with mean¬†+2.2, 0 and ‚àí2.2 scale points for values higher, equal and lower ratings respectively, and¬†¬±0.5 as standard deviation that helped in providing an effect of variability consistent with that of a human judgement. In case the feedbacks had to appear out of the physical limit of the scale, they were instead displayed at the further extremity within the scale. Each video appeared just once (total number¬†=¬†44) and was randomly assigned to one of the nine conditions. This led to eight conditions associated with five videos, and one condition associated with four videos (5  *  8¬†+¬†4¬†  =  ¬†44). The association between conditions and videos was different for each subject, thus preventing the presence of one condition systematically associated with less videos. \n",
      "--- Extract ---\n",
      "TaskName: Pain assessment\n",
      "TaskDescription: Participants appraised the pain of facial expression video-clips, and subsequently were confronted with two feedbacks: the self-report of the person in pain and the average opinion of 20 medical practitioners.\n",
      "\n",
      "PMCID: 8443248\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: Participants were explicitly instructed to recreate the feelings of the demonstrators shown in the videos as vividly and intensely as possible. Based on the validation and pilot study, the painful   expressions   for the genuine and pretended conditions were matched. We also counterbalanced the demonstrators appearing in the genuine and pretended conditions across participants, thus controlling for differences in behavioral and brain response that could be explained by differences between the stimulus sets. Note that, all video clips were validated and piloted multiple times to ensure the experimental effect (details can be found in the section above). \n",
      "\n",
      "The participant performed the fMRI experiment in two runs ( ). Each run was composed of two blocks showing genuine pain and two blocks showing pretended pain. In each block, the participant watched nine video clips containing both painful and neutral videos. To remind participants‚Äô the condition of the upcoming block, a label of 4 s duration appeared at the beginning of each block, showing either ‚Äògenuine‚Äô or ‚Äòpretended‚Äô (in German). Each trial started with a fixation cross (+) presented for 4‚Äì7 s (in steps of 1.5 s, mean = 5.5 s). After that, the video (duration = 2 s) was played. A short jitter was inserted after the video for 0.5‚Äì1.0 s (in steps of 0.05 s, mean = 0.75 s). After the jitter, the following three questions were displayed (in German) one after the other in a pseudo-randomized order: (1) How much pain did the person   express   on his/her face? (2) How much pain did the person   actually feel  ? (3) How   unpleasant did you feel   to watch the person in this situation? Beneath each question, a visual analog scale ranging from 0 (not at all) to 8 (unbearable) with nine tick-marks was positioned. The participant moved the marker along the scale by pressing the left or right keys on the button box, and they pressed the middle key to confirm their answer. The marker initially was always located at the midpoint (‚Äò4‚Äô) of the scale. When the confirmed key was pressed, the marker turned from black to red. All ratings lasted for 4 s even when the participant pressed the confirmed key before the end of this period. Between the two runs, the participant had a short break (1‚Äì2 min). \n",
      "--- Extract ---\n",
      "TaskName: Watch video clips of genuine vs. pretended pain\n",
      "TaskDescription: Participants watched video clips of people either feeling or pretending to feel pain.\n",
      "\n",
      "PMCID: 8564184\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Voxel-based morphometry\n",
      "TaskDescription: Parallelized whole-brain voxel-based morphometry analyses in two patient cohorts with chronic visceral pain (ulcerative colitis in remission and irritable bowel syndrome) and healthy individuals.\n",
      "\n",
      "PMCID: 8927597\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: 2-back updating task\n",
      "TaskName: classification task\n",
      "TaskDescription: Participants performed a 2-back updating task (2Back) and a classification task (Classification) with numbers or letters. The task involved classifying stimuli as odd/even or vowel/consonant and determining whether the current target was identical to the stimulus presented two trials before.\n",
      "\n",
      "PMCID: 8975992\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Watching a movie\n",
      "TaskName: Listening to audio-description\n",
      "TaskDescription: Participants watched the movie Forrest Gump and listened to its audio-description.\n",
      "\n",
      "PMCID: 9202476\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: match-to-sample (MTS) task\n",
      "TaskDescription: In this computerized task, subjects were presented with a complex figure in the middle of the screen. Then, a few patterns were shown in the periphery, from which one was matched with the presented pattern. In the first trials, two patterns were presented in the periphery, and it was increased to eight patterns in the final trials. A total of 48 trials were conducted for each subject.\n",
      "\n",
      "PMCID: 9729227\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Resting-state functional imaging\n",
      "TaskDescription: We used 110 7T resting-state functional MRI datasets from the Human Connectome Project S1200. Images were preprocessed and registered to the MNI152 space as specified in the Human Connectome Project protocol.\n",
      "\n",
      "PMCID: 10028637\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: auditory target detection task\n",
      "TaskName: narrative listening\n",
      "TaskName: resting state\n",
      "TaskDescription: Participants performed a computerized auditory target detection task (50 trials) to assess individual responsiveness differences during moderate anaesthesia. Sound was presented, and participants pressed a button upon hearing an auditory beep.\n",
      "TaskDescription: Participants listened to a plot-driven auditory narrative (5min) while in the scanner, with eyes closed, to engage perceptual or high-level attention processes.\n",
      "TaskDescription: A resting state (rs) scan was conducted for comparison with the narrative condition.\n",
      "\n",
      "PMCID: 10031743\n",
      "--- Annotator ---\n",
      "TaskName: n/a\n",
      "TaskDescription: n/a\n",
      "--- Extract ---\n",
      "TaskName: Neural Correlates of Formal Thought Disorder Dimensions in Psychosis\n",
      "TaskDescription: This study tested whether four FTD dimensions differ in their association with brain perfusion and brain structure.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_print_by_pmcid(has_notaskname_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubget",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
