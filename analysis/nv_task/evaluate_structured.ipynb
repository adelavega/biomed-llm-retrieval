{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load extracted data\n",
    "with open('../../outputs/nv_task/extractions/full_lb_nv_taskstructured-zeroshot_gpt-4o-mini-2024-07-18.json') as f:\n",
    "    nv_task_gpt_json = json.load(f)\n",
    "\n",
    "with open('../../outputs/nv_task/extractions/full_lb_nv_taskstructured-zeroshot_gpt-4o-2024-08-06.json') as f:\n",
    "    nv_task_gpt4_json = json.load(f)\n",
    "\n",
    "# Convert to dictionary and clean\n",
    "def _clean_extracted_data(data):\n",
    "    data_dict = {}\n",
    "    for item in data:\n",
    "        pmcid = item['pmcid']\n",
    "        data_dict[pmcid] = {}\n",
    "        for key, value in item.items():\n",
    "            value = None if value in ['null', []] else value\n",
    "            if key == 'Modality' and value:\n",
    "                value = [v.replace(' ', '') for v in value if v]\n",
    "\n",
    "            data_dict[pmcid][key] = value\n",
    "\n",
    "        for bad_key in ['Fmritasks', 'Behavioraltasks']:\n",
    "            if bad_key in data_dict[pmcid]:\n",
    "                data_dict[pmcid].pop(bad_key)\n",
    "\n",
    "        fMRITasks = data_dict[pmcid].get('fMRITasks') or []\n",
    "\n",
    "        for k in ['TaskName', 'TaskDescription']:\n",
    "            data_dict[pmcid][k] = []\n",
    "            for task in fMRITasks:\n",
    "                if not task['RestingState']:\n",
    "                    data_dict[pmcid][k].append(task.get(k, None))\n",
    "\n",
    "        has_resting_state = any(task['RestingState'] for task in fMRITasks)\n",
    "        data_dict[pmcid]['HasRestingState'] = has_resting_state\n",
    "\n",
    "        \n",
    "            \n",
    "    return data_dict\n",
    "nv_task_gpt = _clean_extracted_data(nv_task_gpt_json)\n",
    "nv_task_gpt4 = _clean_extracted_data(nv_task_gpt4_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "\n",
    "from labelrepo.projects.nv_task import load_annotations\n",
    "\n",
    "annotations = load_annotations()\n",
    "annotations = annotations[annotations['annotator_name'] != 'alice_chen']\n",
    "\n",
    "# Process annotations\n",
    "def _get_task_name(rows):\n",
    "    # Add TaskName, replacing 'None' and 'Unsure' with 'n/a'\n",
    "    rows = rows[rows.label_name == 'TaskName']\n",
    "    task_names = []\n",
    "    for _, row in rows.iterrows():\n",
    "        if row['None'] or row['Unsure']:\n",
    "            task_names.append('n/a')\n",
    "        else:\n",
    "            task_names.append(row['selected_text'])\n",
    "    return task_names\n",
    "\n",
    "# Convert to comparable dictionary\n",
    "annotations_summary = {}\n",
    "for pmcid, df in annotations.groupby('pmcid'):\n",
    "    HasRestingState = 'DesignType-RestingState' in df.label_name.values\n",
    "\n",
    "    s = {\n",
    "        'pmcid': pmcid,\n",
    "        'HasRestingState': HasRestingState,\n",
    "        'annotator_name': df.annotator_name.iloc[0],\n",
    "        'Exclude': next(\n",
    "            (label.split('-', 1)[1] for label in df.label_name if label.startswith('Exclude')), None\n",
    "        ),\n",
    "        'Modality': [\n",
    "            label.split('-', 1)[1] for label in df.label_name if label.startswith('Modality')\n",
    "        ] or None,\n",
    "    }\n",
    "\n",
    "    df_abstract = df[df.section == 'abstract']\n",
    "    abstract_tasks = _get_task_name(df_abstract)\n",
    "\n",
    "    df_body = df[df.section == 'body']\n",
    "    body_tasks = _get_task_name(df_body)\n",
    "\n",
    "    # Use body tasks if available, otherwise use abstract tasks\n",
    "    s['TaskName'] = body_tasks or abstract_tasks\n",
    "\n",
    "    for k in ['TaskDescription', 'Condition', 'ContrastDefinition']:\n",
    "        s[k] = df_body.loc[df_body.label_name == k, 'selected_text'].tolist() or None\n",
    "\n",
    "    annotations_summary[pmcid] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from thefuzz import process, fuzz\n",
    "\n",
    "def _clean(x):\n",
    "    _li =  [x.lower().replace('paradigm', '').replace('task', '').replace('‚Äê', '').strip() for x in (x or [])]\n",
    "\n",
    "    return [re.sub(r'\\s*\\([^)]*\\)', '', x) for x in _li]\n",
    "\n",
    "def _compare(x, y):\n",
    "    x = x or ''\n",
    "    y = y or ''\n",
    "    return x.replace('-', '').lower() == y.replace('-', '').lower()\n",
    "\n",
    "def _score_comparison(x, y):\n",
    "    res = _compare(x, y)\n",
    "    return 100 if res else 0\n",
    "\n",
    "def score_combinations(correct_labels, extracted_labels, scorer=fuzz.token_set_ratio):\n",
    "    correct_labels = _clean(correct_labels)\n",
    "    extracted_labels = _clean(extracted_labels)\n",
    "    \n",
    "    matched_labels = []\n",
    "    \n",
    "    while correct_labels and extracted_labels:\n",
    "        # Collect all matches and their scores\n",
    "        all_matches = []\n",
    "        for correct_label in correct_labels:\n",
    "            matches = process.extract(correct_label, extracted_labels, limit=None, scorer=scorer)\n",
    "            for matched_label, score in matches:\n",
    "                all_matches.append((correct_label, matched_label, score))\n",
    "\n",
    "        # Sort all matches by score in descending order\n",
    "        all_matches.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        # Take the highest match\n",
    "        best_match = all_matches[0]\n",
    "        correct_label, matched_label, score = best_match\n",
    "\n",
    "        # Append to results and remove matched labels\n",
    "        matched_labels.append(score)\n",
    "        correct_labels.remove(correct_label)\n",
    "        extracted_labels.remove(matched_label)\n",
    "\n",
    "    matched_labels = (sum(matched_labels) / len(matched_labels)) / 100 if matched_labels else pd.NA\n",
    "\n",
    "    return matched_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_comparisons(annotations_summary, extractions):\n",
    "    # Compare annotations to extractions\n",
    "    all_scores = {}\n",
    "\n",
    "    for pmcid, ann in annotations_summary.items():\n",
    "        gpt_subset = extractions[pmcid]\n",
    "        # Compare Modality\n",
    "        if ann['Modality']:\n",
    "            modality_score = score_combinations(ann['Modality'], gpt_subset['Modality'], scorer=_score_comparison)\n",
    "        else:\n",
    "            modality_score = None\n",
    "\n",
    "        all_scores[pmcid] = {\n",
    "            'Exclude': _compare(ann['Exclude'], gpt_subset['Exclude']),\n",
    "            'Modality': modality_score,\n",
    "            'HasRestingState': ann['HasRestingState'] == gpt_subset['HasRestingState']\n",
    "        }\n",
    "        \n",
    "        for k in ['TaskName', 'TaskDescription']:\n",
    "            all_scores[pmcid][k] = score_combinations(ann[k], gpt_subset[k])\n",
    "\n",
    "    # Calculate scores\n",
    "    n_articles = len(annotations_summary.keys())\n",
    "\n",
    "    all_scores_df = pd.DataFrame(all_scores).T\n",
    "\n",
    "    return all_scores_df\n",
    "\n",
    "all_scores_df = run_all_comparisons(annotations_summary, nv_task_gpt)\n",
    "all_scores_df4 = run_all_comparisons(annotations_summary, nv_task_gpt4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NA>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_combinations([], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_idx = [_p for _p, v in annotations_summary.items() if v['Exclude'] is not None]\n",
    "has_task_name = [_p for _p, v in annotations_summary.items() if 'n/a' not in v['TaskName']]\n",
    "has_task_noname = [_p for _p, v in annotations_summary.items() if 'n/a' in v['TaskName']]\n",
    "\n",
    "def compute_subset(df):\n",
    "    # Overall scores\n",
    "    mean_all = df.mean()\n",
    "    all_n = df.shape[0]\n",
    "\n",
    "    # Excluding articles with 'Exclude' label\n",
    "    no_exclude_res = df.loc[~df.index.isin(exclude_idx)]\n",
    "    no_exclude_res_n = no_exclude_res.shape[0]\n",
    "\n",
    "\n",
    "    # For papers with a clearly defined task name\n",
    "    has_task_name_res = df.loc[df.index.isin(has_task_name)]\n",
    "    has_task_name_n = has_task_name_res.shape[0]\n",
    "\n",
    "\n",
    "    # For papers with a task-based design but no annotated task name\n",
    "    has_task_name_noname = df.loc[df.index.isin(has_task_noname)]\n",
    "    has_task_name_noname_n = has_task_name_noname.shape[0]\n",
    "\n",
    "    # Combine results into a single dataframe\n",
    "    results = pd.concat([mean_all, no_exclude_res.mean(), has_task_name_res.mean(), has_task_name_noname.mean()], axis=1)\n",
    "    results.columns = ['All', 'No Exclude', 'Has Task Name', 'Has Task with No Name']\n",
    "\n",
    "    # Change dtype to float\n",
    "    results = results.astype(float)\n",
    "    results = results.round(2).T\n",
    "\n",
    "    combine_ns = pd.Series([all_n, no_exclude_res_n, has_task_name_n, has_task_name_noname_n], index=results.index, name='n').round(0)\n",
    "    results.insert(loc = 0, column = 'n', value = combine_ns)\n",
    "\n",
    "    return results\n",
    "\n",
    "results = compute_subset(all_scores_df)\n",
    "results4 = compute_subset(all_scores_df4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>Modality</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>TaskDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>104</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Exclude</th>\n",
       "      <td>95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Has Task Name</th>\n",
       "      <td>86</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Has Task with No Name</th>\n",
       "      <td>18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         n  Exclude  Modality  HasRestingState  TaskName  \\\n",
       "All                    104     0.94      0.99             0.91      0.78   \n",
       "No Exclude              95     1.00      0.99             0.91      0.78   \n",
       "Has Task Name           86     0.93      0.99             0.91      0.94   \n",
       "Has Task with No Name   18     1.00      1.00             0.94      0.18   \n",
       "\n",
       "                       TaskDescription  \n",
       "All                               0.77  \n",
       "No Exclude                        0.77  \n",
       "Has Task Name                     0.77  \n",
       "Has Task with No Name             0.77  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>Modality</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>TaskDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>104</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Exclude</th>\n",
       "      <td>95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Has Task Name</th>\n",
       "      <td>86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Has Task with No Name</th>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         n  Exclude  Modality  HasRestingState  TaskName  \\\n",
       "All                    104      1.0      0.99             0.94      0.75   \n",
       "No Exclude              95      1.0      0.99             0.94      0.75   \n",
       "Has Task Name           86      1.0      0.99             0.93      0.92   \n",
       "Has Task with No Name   18      1.0      1.00             1.00      0.18   \n",
       "\n",
       "                       TaskDescription  \n",
       "All                               0.69  \n",
       "No Exclude                        0.69  \n",
       "Has Task Name                     0.69  \n",
       "Has Task with No Name             0.73  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4mini = pd.DataFrame(nv_task_gpt).T\n",
    "gpt4 = pd.DataFrame(nv_task_gpt4).T\n",
    "gpt4mini['annotator_name'] = 'gpt-4o-mini'\n",
    "gpt4['annotator_name'] = 'gpt-4o'\n",
    "\n",
    "combined_df = pd.concat([pd.DataFrame(annotations_summary).T, gpt4mini, gpt4], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('max_colwidth', 300)\n",
    "    \n",
    "\n",
    "def _display(df, pmcids=None, exclude_cols=['pmcid', 'fMRITasks', 'BehavioralTasks', 'RestingState', 'RestingStateMetadata', 'Condition', 'ContrastDefinition', 'Restingstate', 'TaskDuration', 'TaskDesign']):\n",
    "    cols = list(set(df.keys()) - set(exclude_cols))\n",
    "    if pmcids:\n",
    "        df = df[df.pmcid.isin(pmcids)]\n",
    "    for _, _df in df.groupby('pmcid'):\n",
    "        display(_df[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff on Exclude \n",
    "\n",
    "diff_exclude = set(all_scores_df[all_scores_df['Exclude'] == False].index.tolist()).union(set(all_scores_df4[all_scores_df4['Exclude'] == False].index.tolist()))\n",
    "if diff_exclude:\n",
    "    _display(combined_df, diff_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on, we will exclude Excluded articles\n",
    "combined_df = combined_df[~combined_df.index.isin(exclude_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3825257</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825257</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>To investigate whether variations in the experience of reading across individuals have a basis in the brain's functional architecture, particularly focusing on the anterior medial prefrontal cortex (aMPFC) and posterior cingulate cortex (PCC) as part of the default mode network (DMN).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825257</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>To explore whether the intrinsic functional connectivity of key hubs of the default mode network (DMN) is predictive of individual differences in reading comprehension and task focus.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality TaskName Exclude HasRestingState  \\\n",
       "3825257    delavega_nv  [fMRI-BOLD]       []    None            True   \n",
       "3825257    gpt-4o-mini  [fMRI-BOLD]       []    None           False   \n",
       "3825257         gpt-4o  [fMRI-BOLD]       []    None           False   \n",
       "\n",
       "        TaskDescription  \\\n",
       "3825257            None   \n",
       "3825257              []   \n",
       "3825257              []   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                        StudyObjective  \n",
       "3825257                                                                                                                                                                                                                                                                                            NaN  \n",
       "3825257  To investigate whether variations in the experience of reading across individuals have a basis in the brain's functional architecture, particularly focusing on the anterior medial prefrontal cortex (aMPFC) and posterior cingulate cortex (PCC) as part of the default mode network (DMN).  \n",
       "3825257                                                                                                        To explore whether the intrinsic functional connectivity of key hubs of the default mode network (DMN) is predictive of individual differences in reading comprehension and task focus.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4110030</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4110030</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>To investigate sex differences and menstrual cycle effects in resting state functional connectivity of fronto-parietal cognitive control networks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4110030</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>Investigation of sex differences and menstrual cycle effects in resting state cognitive control networks using fMRI.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality TaskName Exclude HasRestingState  \\\n",
       "4110030    delavega_nv  [fMRI-BOLD]       []    None            True   \n",
       "4110030    gpt-4o-mini  [fMRI-BOLD]       []    None           False   \n",
       "4110030         gpt-4o  [fMRI-BOLD]       []    None            True   \n",
       "\n",
       "        TaskDescription  \\\n",
       "4110030            None   \n",
       "4110030              []   \n",
       "4110030              []   \n",
       "\n",
       "                                                                                                                                             StudyObjective  \n",
       "4110030                                                                                                                                                 NaN  \n",
       "4110030  To investigate sex differences and menstrual cycle effects in resting state functional connectivity of fronto-parietal cognitive control networks.  \n",
       "4110030                                Investigation of sex differences and menstrual cycle effects in resting state cognitive control networks using fMRI.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4386762</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386762</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>To investigate the relationship between ALFF and global FC with a voxel-based approach in the healthy and diseased brain, particularly in patients with Alzheimer's Disease (AD) and Mild Cognitive Impairment (MCI).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386762</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>To investigate the relationship between ALFF and global FC with a voxel-based approach in the healthy and diseased brain, specifically in degenerative dementia.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality TaskName Exclude HasRestingState  \\\n",
       "4386762    delavega_nv  [fMRI-BOLD]       []    None            True   \n",
       "4386762    gpt-4o-mini  [fMRI-BOLD]       []    None           False   \n",
       "4386762         gpt-4o  [fMRI-BOLD]       []    None            True   \n",
       "\n",
       "        TaskDescription  \\\n",
       "4386762            None   \n",
       "4386762              []   \n",
       "4386762              []   \n",
       "\n",
       "                                                                                                                                                                                                                StudyObjective  \n",
       "4386762                                                                                                                                                                                                                    NaN  \n",
       "4386762  To investigate the relationship between ALFF and global FC with a voxel-based approach in the healthy and diseased brain, particularly in patients with Alzheimer's Disease (AD) and Mild Cognitive Impairment (MCI).  \n",
       "4386762                                                       To investigate the relationship between ALFF and global FC with a voxel-based approach in the healthy and diseased brain, specifically in degenerative dementia.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4440210</th>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[emotional matching task]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[Subjects were shown triplets of geometric shapes (as neutral stimuli) and of threatening scenes as well as fearful faces (as emotional conditions) presented in alternating blocks of neutral and emotional stimuli.]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4440210</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Emotional Matching Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Subjects were shown triplets of geometric shapes (neutral stimuli) and of threatening scenes as well as fearful faces (emotional conditions) presented in alternating blocks of neutral and emotional stimuli.]</td>\n",
       "      <td>To investigate activations in the amygdala region to the presentation of emotional faces and to understand the origins of signal fluctuations and their heterogeneity in this region.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4440210</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                annotator_name     Modality                   TaskName  \\\n",
       "4440210  delavega-aliceoverlap  [fMRI-BOLD]  [emotional matching task]   \n",
       "4440210            gpt-4o-mini  [fMRI-BOLD]  [Emotional Matching Task]   \n",
       "4440210                 gpt-4o  [fMRI-BOLD]                         []   \n",
       "\n",
       "        Exclude HasRestingState  \\\n",
       "4440210    None            True   \n",
       "4440210    None           False   \n",
       "4440210    None           False   \n",
       "\n",
       "                                                                                                                                                                                                                TaskDescription  \\\n",
       "4440210  [Subjects were shown triplets of geometric shapes (as neutral stimuli) and of threatening scenes as well as fearful faces (as emotional conditions) presented in alternating blocks of neutral and emotional stimuli.]   \n",
       "4440210        [Subjects were shown triplets of geometric shapes (neutral stimuli) and of threatening scenes as well as fearful faces (emotional conditions) presented in alternating blocks of neutral and emotional stimuli.]   \n",
       "4440210                                                                                                                                                                                                                      []   \n",
       "\n",
       "                                                                                                                                                                                StudyObjective  \n",
       "4440210                                                                                                                                                                                    NaN  \n",
       "4440210  To investigate activations in the amygdala region to the presentation of emotional faces and to understand the origins of signal fluctuations and their heterogeneity in this region.  \n",
       "4440210                                                                                                                                                                                   None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9148994</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[spatial contextual memory task]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9148994</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Spatial Contextual Memory Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants performed a spatial contextual memory task consisting of an encoding, consolidation, and retrieval phase.]</td>\n",
       "      <td>This study examined age effects on consolidation-related neural mechanisms and their susceptibility to interference using functional magnetic resonance imaging data from younger and older healthy participants.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9148994</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>This study examined age effects on consolidation-related neural mechanisms and their susceptibility to interference using functional magnetic resonance imaging data from younger and older healthy participants.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality                          TaskName Exclude  \\\n",
       "9148994    delavega_nv  [fMRI-BOLD]  [spatial contextual memory task]    None   \n",
       "9148994    gpt-4o-mini  [fMRI-BOLD]  [Spatial Contextual Memory Task]    None   \n",
       "9148994         gpt-4o  [fMRI-BOLD]                                []    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "9148994            True   \n",
       "9148994           False   \n",
       "9148994            True   \n",
       "\n",
       "                                                                                                                  TaskDescription  \\\n",
       "9148994                                                                                                                      None   \n",
       "9148994  [Participants performed a spatial contextual memory task consisting of an encoding, consolidation, and retrieval phase.]   \n",
       "9148994                                                                                                                        []   \n",
       "\n",
       "                                                                                                                                                                                                            StudyObjective  \n",
       "9148994                                                                                                                                                                                                                NaN  \n",
       "9148994  This study examined age effects on consolidation-related neural mechanisms and their susceptibility to interference using functional magnetic resonance imaging data from younger and older healthy participants.  \n",
       "9148994  This study examined age effects on consolidation-related neural mechanisms and their susceptibility to interference using functional magnetic resonance imaging data from younger and older healthy participants.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9202476</th>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>[fMRI-BOLD, DiffusionMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202476</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD, DiffusionMRI]</td>\n",
       "      <td>[match-to-sample (MTS) task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[This computerized task assesses visual search and attention by presenting a complex figure and requiring subjects to match it with patterns shown in the periphery.]</td>\n",
       "      <td>The aim of the study was to evaluate correlation of sleep quality with visual attention and search, functional, and tracts properties of the DAN and VAN.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202476</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD, DiffusionMRI, StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>The aim of the study was to evaluate correlation of sleep quality with visual attention and search, functional, and tracts properties of the DAN and VAN.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                annotator_name                                  Modality  \\\n",
       "9202476  delavega-aliceoverlap                 [fMRI-BOLD, DiffusionMRI]   \n",
       "9202476            gpt-4o-mini                 [fMRI-BOLD, DiffusionMRI]   \n",
       "9202476                 gpt-4o  [fMRI-BOLD, DiffusionMRI, StructuralMRI]   \n",
       "\n",
       "                             TaskName Exclude HasRestingState  \\\n",
       "9202476                            []    None            True   \n",
       "9202476  [match-to-sample (MTS) task]    None           False   \n",
       "9202476                            []    None            True   \n",
       "\n",
       "                                                                                                                                                               TaskDescription  \\\n",
       "9202476                                                                                                                                                                   None   \n",
       "9202476  [This computerized task assesses visual search and attention by presenting a complex figure and requiring subjects to match it with patterns shown in the periphery.]   \n",
       "9202476                                                                                                                                                                     []   \n",
       "\n",
       "                                                                                                                                                    StudyObjective  \n",
       "9202476                                                                                                                                                        NaN  \n",
       "9202476  The aim of the study was to evaluate correlation of sleep quality with visual attention and search, functional, and tracts properties of the DAN and VAN.  \n",
       "9202476  The aim of the study was to evaluate correlation of sleep quality with visual attention and search, functional, and tracts properties of the DAN and VAN.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9729227</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9729227</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>To explore the subcortical anatomy of the ventral and dorsal attention networks by aligning individual resting-state functional maps in a common functional space.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9729227</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>To map the subcortical architecture of the Ventral and Dorsal Attention Networks using advanced methods of functional alignment applied to resting-state fMRI.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality TaskName Exclude HasRestingState  \\\n",
       "9729227    delavega_nv  [fMRI-BOLD]       []    None            True   \n",
       "9729227    gpt-4o-mini  [fMRI-BOLD]       []    None           False   \n",
       "9729227         gpt-4o  [fMRI-BOLD]       []    None           False   \n",
       "\n",
       "        TaskDescription  \\\n",
       "9729227            None   \n",
       "9729227              []   \n",
       "9729227              []   \n",
       "\n",
       "                                                                                                                                                             StudyObjective  \n",
       "9729227                                                                                                                                                                 NaN  \n",
       "9729227  To explore the subcortical anatomy of the ventral and dorsal attention networks by aligning individual resting-state functional maps in a common functional space.  \n",
       "9729227      To map the subcortical architecture of the Ventral and Dorsal Attention Networks using advanced methods of functional alignment applied to resting-state fMRI.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10028637</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10028637</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Auditory Target Detection Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[A computerized auditory target detection task aimed to assess individual responsiveness differences during moderate anaesthesia.]</td>\n",
       "      <td>To investigate whether variability or impairments in functional connectivity within and between the dorsal attention network (DAN), executive control network (ECN), and default mode network (DMN) underlie individual differences in responsiveness during propofol anaesthesia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10028637</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD, StructuralMRI]</td>\n",
       "      <td>[Narrative Understanding Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[Participants listened to a plot-driven auditory narrative while undergoing fMRI scans to investigate perceptual or high-level attention processes under anaesthesia.]</td>\n",
       "      <td>To investigate whether variability or impairments in functional connectivity (FC) within and between the dorsal attention network (DAN), executive control network (ECN), and default mode network (DMN) underlie individual differences in responsiveness during propofol anaesthesia.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         annotator_name                    Modality  \\\n",
       "10028637    delavega_nv                 [fMRI-BOLD]   \n",
       "10028637    gpt-4o-mini                 [fMRI-BOLD]   \n",
       "10028637         gpt-4o  [fMRI-BOLD, StructuralMRI]   \n",
       "\n",
       "                                  TaskName Exclude HasRestingState  \\\n",
       "10028637                             [n/a]    None            True   \n",
       "10028637  [Auditory Target Detection Task]    None           False   \n",
       "10028637    [Narrative Understanding Task]    None            True   \n",
       "\n",
       "                                                                                                                                                                 TaskDescription  \\\n",
       "10028637                                                                                                                                                                    None   \n",
       "10028637                                      [A computerized auditory target detection task aimed to assess individual responsiveness differences during moderate anaesthesia.]   \n",
       "10028637  [Participants listened to a plot-driven auditory narrative while undergoing fMRI scans to investigate perceptual or high-level attention processes under anaesthesia.]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                   StudyObjective  \n",
       "10028637                                                                                                                                                                                                                                                                                      NaN  \n",
       "10028637       To investigate whether variability or impairments in functional connectivity within and between the dorsal attention network (DAN), executive control network (ECN), and default mode network (DMN) underlie individual differences in responsiveness during propofol anaesthesia.  \n",
       "10028637  To investigate whether variability or impairments in functional connectivity (FC) within and between the dorsal attention network (DAN), executive control network (ECN), and default mode network (DMN) underlie individual differences in responsiveness during propofol anaesthesia.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10031743</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[StructuralMRI, DiffusionMRI, fMRI-CBF]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10031743</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[StructuralMRI, DiffusionMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>This study tested whether four FTD dimensions differ in their association with brain perfusion and brain structure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10031743</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[StructuralMRI, DiffusionMRI, fMRI-CBF]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>This study investigates the neural correlates of four formal thought disorder (FTD) dimensions in patients with schizophrenia spectrum disorders using multimodal brain imaging, including resting-state cerebral blood flow (CBF) and brain structure assessments.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         annotator_name                                 Modality TaskName  \\\n",
       "10031743    delavega_nv  [StructuralMRI, DiffusionMRI, fMRI-CBF]       []   \n",
       "10031743    gpt-4o-mini            [StructuralMRI, DiffusionMRI]       []   \n",
       "10031743         gpt-4o  [StructuralMRI, DiffusionMRI, fMRI-CBF]       []   \n",
       "\n",
       "         Exclude HasRestingState TaskDescription  \\\n",
       "10031743    None            True            None   \n",
       "10031743    None           False              []   \n",
       "10031743    None           False              []   \n",
       "\n",
       "                                                                                                                                                                                                                                                               StudyObjective  \n",
       "10031743                                                                                                                                                                                                                                                                  NaN  \n",
       "10031743                                                                                                                                                  This study tested whether four FTD dimensions differ in their association with brain perfusion and brain structure.  \n",
       "10031743  This study investigates the neural correlates of four formal thought disorder (FTD) dimensions in patients with schizophrenia spectrum disorders using multimodal brain imaging, including resting-state cerebral blood flow (CBF) and brain structure assessments.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Diff on has_resting_state\n",
    "\n",
    "diff_res = set(all_scores_df[all_scores_df['HasRestingState'] < 1].index.tolist()).union(set(all_scores_df4[all_scores_df['HasRestingState'] < 1].index.tolist()))\n",
    "\n",
    "if diff_res:\n",
    "    _display(combined_df, diff_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although with this prompt GPT (especially mini), sometimes fails to detect a resting state scan, at least it rarely mentions it as a task without labeling it as resting-state.\n",
    "\n",
    "Thus it would be fairly easy to exclude studies with no fMRITasks listed, and if any are resting state they can be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6331309</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331309</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>This study aimed to identify patterns of gray matter volume (GMV) alteration specific to and common among patients with RLS, migraine, and comorbid migraine and RLS.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331309</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>This study aimed to identify patterns of gray matter volume (GMV) alteration specific to and common among patients with RLS, migraine, and comorbid migraine and RLS.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name         Modality TaskName Exclude HasRestingState  \\\n",
       "6331309    delavega_nv  [StructuralMRI]       []    None           False   \n",
       "6331309    gpt-4o-mini  [StructuralMRI]       []    None           False   \n",
       "6331309         gpt-4o  [StructuralMRI]       []    None            True   \n",
       "\n",
       "        TaskDescription  \\\n",
       "6331309            None   \n",
       "6331309              []   \n",
       "6331309              []   \n",
       "\n",
       "                                                                                                                                                                StudyObjective  \n",
       "6331309                                                                                                                                                                    NaN  \n",
       "6331309  This study aimed to identify patterns of gray matter volume (GMV) alteration specific to and common among patients with RLS, migraine, and comorbid migraine and RLS.  \n",
       "6331309  This study aimed to identify patterns of gray matter volume (GMV) alteration specific to and common among patients with RLS, migraine, and comorbid migraine and RLS.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7426775</th>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7426775</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>To predict chronological age from structural magnetic resonance imaging scans using a deep learning framework and to identify brain regions contributing to this prediction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7426775</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>The study aims to use a Deep Learning framework to predict chronological age from structural MRI scans and identify brain regions contributing to this prediction, focusing on brain aging.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                annotator_name         Modality TaskName Exclude  \\\n",
       "7426775  delavega-aliceoverlap  [StructuralMRI]       []    None   \n",
       "7426775            gpt-4o-mini  [StructuralMRI]       []    None   \n",
       "7426775                 gpt-4o  [StructuralMRI]       []    None   \n",
       "\n",
       "        HasRestingState TaskDescription  \\\n",
       "7426775           False            None   \n",
       "7426775           False              []   \n",
       "7426775           False              []   \n",
       "\n",
       "                                                                                                                                                                                      StudyObjective  \n",
       "7426775                                                                                                                                                                                          NaN  \n",
       "7426775                 To predict chronological age from structural magnetic resonance imaging scans using a deep learning framework and to identify brain regions contributing to this prediction.  \n",
       "7426775  The study aims to use a Deep Learning framework to predict chronological age from structural MRI scans and identify brain regions contributing to this prediction, focusing on brain aging.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7582181</th>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7582181</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>To investigate the relationship between social reward and threat expectancies and regional grey matter volumes (rGMV) in healthy adults using voxel-based morphometry (VBM).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7582181</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>To investigate the unique and overlapping regional grey matter volume (rGMV) correlates of inter-individual differences in social threat and reward expectancies using voxel-based morphometry (VBM).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                annotator_name         Modality TaskName Exclude  \\\n",
       "7582181  delavega-aliceoverlap  [StructuralMRI]       []    None   \n",
       "7582181            gpt-4o-mini  [StructuralMRI]       []    None   \n",
       "7582181                 gpt-4o  [StructuralMRI]       []    None   \n",
       "\n",
       "        HasRestingState TaskDescription  \\\n",
       "7582181           False            None   \n",
       "7582181           False              []   \n",
       "7582181           False              []   \n",
       "\n",
       "                                                                                                                                                                                                StudyObjective  \n",
       "7582181                                                                                                                                                                                                    NaN  \n",
       "7582181                           To investigate the relationship between social reward and threat expectancies and regional grey matter volumes (rGMV) in healthy adults using voxel-based morphometry (VBM).  \n",
       "7582181  To investigate the unique and overlapping regional grey matter volume (rGMV) correlates of inter-individual differences in social threat and reward expectancies using voxel-based morphometry (VBM).  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8564184</th>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8564184</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>To elucidate structural brain alterations in patients with chronic visceral pain, specifically comparing patients with ulcerative colitis and irritable bowel syndrome to healthy controls, and to assess associations with symptom severity and chronic stress.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8564184</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Identify structural brain alterations in chronic visceral pain patients (UC and IBS) and investigate associations with visceral symptoms and chronic stress.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                annotator_name         Modality TaskName Exclude  \\\n",
       "8564184  delavega-aliceoverlap  [StructuralMRI]       []    None   \n",
       "8564184            gpt-4o-mini  [StructuralMRI]       []    None   \n",
       "8564184                 gpt-4o  [StructuralMRI]       []    None   \n",
       "\n",
       "        HasRestingState TaskDescription  \\\n",
       "8564184           False            None   \n",
       "8564184           False              []   \n",
       "8564184           False              []   \n",
       "\n",
       "                                                                                                                                                                                                                                                           StudyObjective  \n",
       "8564184                                                                                                                                                                                                                                                               NaN  \n",
       "8564184  To elucidate structural brain alterations in patients with chronic visceral pain, specifically comparing patients with ulcerative colitis and irritable bowel syndrome to healthy controls, and to assess associations with symptom severity and chronic stress.  \n",
       "8564184                                                                                                      Identify structural brain alterations in chronic visceral pain patients (UC and IBS) and investigate associations with visceral symptoms and chronic stress.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9202476</th>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>[fMRI-BOLD, DiffusionMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202476</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD, DiffusionMRI]</td>\n",
       "      <td>[match-to-sample (MTS) task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[This computerized task assesses visual search and attention by presenting a complex figure and requiring subjects to match it with patterns shown in the periphery.]</td>\n",
       "      <td>The aim of the study was to evaluate correlation of sleep quality with visual attention and search, functional, and tracts properties of the DAN and VAN.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202476</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD, DiffusionMRI, StructuralMRI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>The aim of the study was to evaluate correlation of sleep quality with visual attention and search, functional, and tracts properties of the DAN and VAN.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                annotator_name                                  Modality  \\\n",
       "9202476  delavega-aliceoverlap                 [fMRI-BOLD, DiffusionMRI]   \n",
       "9202476            gpt-4o-mini                 [fMRI-BOLD, DiffusionMRI]   \n",
       "9202476                 gpt-4o  [fMRI-BOLD, DiffusionMRI, StructuralMRI]   \n",
       "\n",
       "                             TaskName Exclude HasRestingState  \\\n",
       "9202476                            []    None            True   \n",
       "9202476  [match-to-sample (MTS) task]    None           False   \n",
       "9202476                            []    None            True   \n",
       "\n",
       "                                                                                                                                                               TaskDescription  \\\n",
       "9202476                                                                                                                                                                   None   \n",
       "9202476  [This computerized task assesses visual search and attention by presenting a complex figure and requiring subjects to match it with patterns shown in the periphery.]   \n",
       "9202476                                                                                                                                                                     []   \n",
       "\n",
       "                                                                                                                                                    StudyObjective  \n",
       "9202476                                                                                                                                                        NaN  \n",
       "9202476  The aim of the study was to evaluate correlation of sleep quality with visual attention and search, functional, and tracts properties of the DAN and VAN.  \n",
       "9202476  The aim of the study was to evaluate correlation of sleep quality with visual attention and search, functional, and tracts properties of the DAN and VAN.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_dt = [6331309, 7426775, 7582181, 8564184, 9202476]\n",
    "_display(combined_df, diff_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On studies that previously either had outside the scanner tasks listed as in the scanner, or resting state incorrectly tagged as task, in many cases they are now appropriately extracted (with 1-2 exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2241626</th>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[mental calculation task, language comprehension task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[In the present research, our goal was to define a simple fMRI test, less than 5 minutes long, that could delineate, in a subject-specific manner, those cerebral circuits. A functional sequence was added to each functional imaging session performed in our lab (Figure  ), taking advantage of the ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241626</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Multi-functional localizer]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[A fast event-related fMRI protocol designed to capture individual cerebral correlates of visual, auditory, and sensorimotor processes, reading, language comprehension, and mental calculation.]</td>\n",
       "      <td>To define a simple fMRI test, less than 5 minutes long, that could delineate, in a subject-specific manner, cerebral circuits associated with various cognitive processes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241626</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Multi-functional localizer paradigm]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[The task investigates various cognitive processes including auditory and visual perception, motor actions, reading, language comprehension, and mental calculation.]</td>\n",
       "      <td>To design a simple and fast fMRI acquisition protocol that captures individual cerebral networks associated with auditory and visual perception, motor actions, reading, language comprehension, and mental calculation.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                annotator_name     Modality  \\\n",
       "2241626  delavega-aliceoverlap  [fMRI-BOLD]   \n",
       "2241626            gpt-4o-mini  [fMRI-BOLD]   \n",
       "2241626                 gpt-4o  [fMRI-BOLD]   \n",
       "\n",
       "                                                       TaskName Exclude  \\\n",
       "2241626  [mental calculation task, language comprehension task]    None   \n",
       "2241626                            [Multi-functional localizer]    None   \n",
       "2241626                   [Multi-functional localizer paradigm]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "2241626           False   \n",
       "2241626           False   \n",
       "2241626           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "2241626  [In the present research, our goal was to define a simple fMRI test, less than 5 minutes long, that could delineate, in a subject-specific manner, those cerebral circuits. A functional sequence was added to each functional imaging session performed in our lab (Figure  ), taking advantage of the ...   \n",
       "2241626                                                                                                            [A fast event-related fMRI protocol designed to capture individual cerebral correlates of visual, auditory, and sensorimotor processes, reading, language comprehension, and mental calculation.]   \n",
       "2241626                                                                                                                                        [The task investigates various cognitive processes including auditory and visual perception, motor actions, reading, language comprehension, and mental calculation.]   \n",
       "\n",
       "                                                                                                                                                                                                                   StudyObjective  \n",
       "2241626                                                                                                                                                                                                                       NaN  \n",
       "2241626                                                To define a simple fMRI test, less than 5 minutes long, that could delineate, in a subject-specific manner, cerebral circuits associated with various cognitive processes.  \n",
       "2241626  To design a simple and fast fMRI acquisition protocol that captures individual cerebral networks associated with auditory and visual perception, motor actions, reading, language comprehension, and mental calculation.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4374765</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[MRS, fMRI-BOLD]</td>\n",
       "      <td>[fear inducing paradigm]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374765</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD, MRS]</td>\n",
       "      <td>[Fear Provocation Paradigm]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[The task involved presenting still pictures of spiders and other control animals to elicit fear-related BOLD responses.]</td>\n",
       "      <td>To investigate the relationship between GABA concentration and fear-related BOLD responses in the insula.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374765</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD, Other]</td>\n",
       "      <td>[Fear provoking paradigm]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants were shown images to provoke fear, including spiders, other animals, general negative, and neutral images from the IAPS.]</td>\n",
       "      <td>To investigate the relationship between GABA concentration and fear-related BOLD responses in the insula region.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name            Modality                     TaskName  \\\n",
       "4374765    delavega_nv    [MRS, fMRI-BOLD]     [fear inducing paradigm]   \n",
       "4374765    gpt-4o-mini    [fMRI-BOLD, MRS]  [Fear Provocation Paradigm]   \n",
       "4374765         gpt-4o  [fMRI-BOLD, Other]    [Fear provoking paradigm]   \n",
       "\n",
       "        Exclude HasRestingState  \\\n",
       "4374765    None           False   \n",
       "4374765    None           False   \n",
       "4374765    None           False   \n",
       "\n",
       "                                                                                                                                 TaskDescription  \\\n",
       "4374765                                                                                                                                     None   \n",
       "4374765                [The task involved presenting still pictures of spiders and other control animals to elicit fear-related BOLD responses.]   \n",
       "4374765  [Participants were shown images to provoke fear, including spiders, other animals, general negative, and neutral images from the IAPS.]   \n",
       "\n",
       "                                                                                                           StudyObjective  \n",
       "4374765                                                                                                               NaN  \n",
       "4374765         To investigate the relationship between GABA concentration and fear-related BOLD responses in the insula.  \n",
       "4374765  To investigate the relationship between GABA concentration and fear-related BOLD responses in the insula region.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5324609</th>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>[fMRI-BOLD, fMRI-BOLD]</td>\n",
       "      <td>[acquired equivalence task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[participants learned a set of visual discriminations via trial‚Äêand‚Äêerror (learning phase), and were subsequently tested on their ability to generalize what they had learned (generalization phase). Both learning and generalization occurred within a single scanning session and took place in the c...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5324609</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Learning and Generalization Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants learned a set of visual discriminations via trial-and-error and were subsequently tested on their ability to generalize what they had learned.]</td>\n",
       "      <td>To investigate the hippocampal role in gradually learning a set of spatial discriminations and subsequently generalizing them in an acquired equivalence task.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5324609</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Visual Discrimination and Generalization Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants learned a set of visual discriminations via trial-and-error and were subsequently tested on their ability to generalize what they had learned within a virtual reality environment.]</td>\n",
       "      <td>Investigate the hippocampal role in gradually learning a set of spatial discriminations and subsequently generalizing them in an acquired equivalence task.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                annotator_name                Modality  \\\n",
       "5324609  delavega-aliceoverlap  [fMRI-BOLD, fMRI-BOLD]   \n",
       "5324609            gpt-4o-mini             [fMRI-BOLD]   \n",
       "5324609                 gpt-4o             [fMRI-BOLD]   \n",
       "\n",
       "                                                TaskName Exclude  \\\n",
       "5324609                      [acquired equivalence task]    None   \n",
       "5324609               [Learning and Generalization Task]    None   \n",
       "5324609  [Visual Discrimination and Generalization Task]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "5324609           False   \n",
       "5324609           False   \n",
       "5324609           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "5324609  [participants learned a set of visual discriminations via trial‚Äêand‚Äêerror (learning phase), and were subsequently tested on their ability to generalize what they had learned (generalization phase). Both learning and generalization occurred within a single scanning session and took place in the c...   \n",
       "5324609                                                                                                                                                [Participants learned a set of visual discriminations via trial-and-error and were subsequently tested on their ability to generalize what they had learned.]   \n",
       "5324609                                                                                                           [Participants learned a set of visual discriminations via trial-and-error and were subsequently tested on their ability to generalize what they had learned within a virtual reality environment.]   \n",
       "\n",
       "                                                                                                                                                         StudyObjective  \n",
       "5324609                                                                                                                                                             NaN  \n",
       "5324609  To investigate the hippocampal role in gradually learning a set of spatial discriminations and subsequently generalizing them in an acquired equivalence task.  \n",
       "5324609     Investigate the hippocampal role in gradually learning a set of spatial discriminations and subsequently generalizing them in an acquired equivalence task.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6037859</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[synchrony judgment, temporal order judgment ]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[The fMRI procedure was similar to the behavioral experiment, except that a reduced stimulus set was presented: ‚àí333, 0, PSS, and +333. The PSS values were obtained individually from the pre-fMRI experiment separately for SJ and TOJ. To be as accurate as possible the individual PSS conditions we...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037859</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Synchrony Judgment (SJ), Temporal Order Judgment (TOJ)]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants decide whether audio and visual cues are in synch or out of synch., Participants decide which cue came first (or last).]</td>\n",
       "      <td>To investigate how behavioral differences between synchrony judgment (SJ) and temporal order judgment (TOJ) tasks are instantiated as neural differences using fMRI.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037859</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Synchrony Judgments (SJ) and Temporal Order Judgments (TOJ) Tasks]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Tasks designed to assess how two different judgments‚Äîwhether audiovisual cues are in synchrony (SJ) or which cue comes first (TOJ)‚Äîinvoke different neural activations.]</td>\n",
       "      <td>To investigate how behavioral differences between synchrony judgments (SJ) and temporal order judgments (TOJ) tasks are instantiated as neural differences using fMRI.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality  \\\n",
       "6037859    delavega_nv  [fMRI-BOLD]   \n",
       "6037859    gpt-4o-mini  [fMRI-BOLD]   \n",
       "6037859         gpt-4o  [fMRI-BOLD]   \n",
       "\n",
       "                                                                    TaskName  \\\n",
       "6037859                       [synchrony judgment, temporal order judgment ]   \n",
       "6037859             [Synchrony Judgment (SJ), Temporal Order Judgment (TOJ)]   \n",
       "6037859  [Synchrony Judgments (SJ) and Temporal Order Judgments (TOJ) Tasks]   \n",
       "\n",
       "        Exclude HasRestingState  \\\n",
       "6037859    None           False   \n",
       "6037859    None           False   \n",
       "6037859    None           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "6037859  [The fMRI procedure was similar to the behavioral experiment, except that a reduced stimulus set was presented: ‚àí333, 0, PSS, and +333. The PSS values were obtained individually from the pre-fMRI experiment separately for SJ and TOJ. To be as accurate as possible the individual PSS conditions we...   \n",
       "6037859                                                                                                                                                                       [Participants decide whether audio and visual cues are in synch or out of synch., Participants decide which cue came first (or last).]   \n",
       "6037859                                                                                                                                    [Tasks designed to assess how two different judgments‚Äîwhether audiovisual cues are in synchrony (SJ) or which cue comes first (TOJ)‚Äîinvoke different neural activations.]   \n",
       "\n",
       "                                                                                                                                                                 StudyObjective  \n",
       "6037859                                                                                                                                                                     NaN  \n",
       "6037859    To investigate how behavioral differences between synchrony judgment (SJ) and temporal order judgment (TOJ) tasks are instantiated as neural differences using fMRI.  \n",
       "6037859  To investigate how behavioral differences between synchrony judgments (SJ) and temporal order judgments (TOJ) tasks are instantiated as neural differences using fMRI.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6219793</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Social feedback paradigm]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Following previous studies [ ‚Äì ], negative social feedback was used to induce emotions for two reasons: (a) in daily life emotions are often caused by social stimuli [ , ] and (b) social feedback elicits emotional responses that are long enough to study emotion dynamics [ ]. The social feedback...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6219793</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Self-Distancing vs. Self-Immersion Perspective Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants were asked to adopt a self-immersed or self-distanced perspective while reading and thinking about negative social feedback.]</td>\n",
       "      <td>To replicate and extend previous findings on the psychological and neural mechanisms underlying emotion explosiveness and accumulation, specifically examining the effects of self-distancing versus self-immersion on these emotional dynamics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6219793</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Self-distancing perspective task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants adopt either a self-immersed or self-distanced perspective while reading and thinking about negative social feedback to assess changes in negative affect.]</td>\n",
       "      <td>To replicate and extend previous findings on the psychological and neural mechanisms underlying emotion explosiveness and accumulation, examining whether adopting a self-distanced (vs. self-immersed) perspective modulates activity in brain regions associated with these emotional dynamics.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality  \\\n",
       "6219793    delavega_nv  [fMRI-BOLD]   \n",
       "6219793    gpt-4o-mini  [fMRI-BOLD]   \n",
       "6219793         gpt-4o  [fMRI-BOLD]   \n",
       "\n",
       "                                                      TaskName Exclude  \\\n",
       "6219793                             [Social feedback paradigm]    None   \n",
       "6219793  [Self-Distancing vs. Self-Immersion Perspective Task]    None   \n",
       "6219793                     [Self-distancing perspective task]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "6219793           False   \n",
       "6219793           False   \n",
       "6219793           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "6219793  [Following previous studies [ ‚Äì ], negative social feedback was used to induce emotions for two reasons: (a) in daily life emotions are often caused by social stimuli [ , ] and (b) social feedback elicits emotional responses that are long enough to study emotion dynamics [ ]. The social feedback...   \n",
       "6219793                                                                                                                                                                  [Participants were asked to adopt a self-immersed or self-distanced perspective while reading and thinking about negative social feedback.]   \n",
       "6219793                                                                                                                                    [Participants adopt either a self-immersed or self-distanced perspective while reading and thinking about negative social feedback to assess changes in negative affect.]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                            StudyObjective  \n",
       "6219793                                                                                                                                                                                                                                                                                                NaN  \n",
       "6219793                                                   To replicate and extend previous findings on the psychological and neural mechanisms underlying emotion explosiveness and accumulation, specifically examining the effects of self-distancing versus self-immersion on these emotional dynamics.  \n",
       "6219793  To replicate and extend previous findings on the psychological and neural mechanisms underlying emotion explosiveness and accumulation, examining whether adopting a self-distanced (vs. self-immersed) perspective modulates activity in brain regions associated with these emotional dynamics.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6699247</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[EEG, fMRI-BOLD, MRS]</td>\n",
       "      <td>[picture‚Äìword verification task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[In this task, a simple line drawing of an object is presented (250‚ÄØms), and following a short delay (75‚ÄØms), a word appears. Subjects are instructed to respond by indicating whether the word was a semantic match (50% of trials) or non-match (unmatched, 50% of trials) to the preceding picture. P...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6699247</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Picture-Word Matching Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants performed a task where they indicated whether a word was a semantic match or non-match to a preceding picture.]</td>\n",
       "      <td>To link temporal patterns in ERPs to neuroanatomical patterns from fMRI and investigate relationships between N400 amplitude and neuroanatomical activation in schizophrenia patients and healthy controls.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6699247</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Picture-Word Matching Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[A task where participants indicate if a word presented after a picture is a semantic match or non-match to the picture.]</td>\n",
       "      <td>To link temporal patterns in ERPs to neuroanatomical patterns from fMRI and investigate relationships between N400 amplitude and neuroanatomical activation in schizophrenia patients and healthy controls.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name               Modality  \\\n",
       "6699247    delavega_nv  [EEG, fMRI-BOLD, MRS]   \n",
       "6699247    gpt-4o-mini            [fMRI-BOLD]   \n",
       "6699247         gpt-4o            [fMRI-BOLD]   \n",
       "\n",
       "                                 TaskName Exclude HasRestingState  \\\n",
       "6699247  [picture‚Äìword verification task]    None           False   \n",
       "6699247      [Picture-Word Matching Task]    None           False   \n",
       "6699247      [Picture-Word Matching Task]    None           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "6699247  [In this task, a simple line drawing of an object is presented (250‚ÄØms), and following a short delay (75‚ÄØms), a word appears. Subjects are instructed to respond by indicating whether the word was a semantic match (50% of trials) or non-match (unmatched, 50% of trials) to the preceding picture. P...   \n",
       "6699247                                                                                                                                                                                [Participants performed a task where they indicated whether a word was a semantic match or non-match to a preceding picture.]   \n",
       "6699247                                                                                                                                                                                    [A task where participants indicate if a word presented after a picture is a semantic match or non-match to the picture.]   \n",
       "\n",
       "                                                                                                                                                                                                      StudyObjective  \n",
       "6699247                                                                                                                                                                                                          NaN  \n",
       "6699247  To link temporal patterns in ERPs to neuroanatomical patterns from fMRI and investigate relationships between N400 amplitude and neuroanatomical activation in schizophrenia patients and healthy controls.  \n",
       "6699247  To link temporal patterns in ERPs to neuroanatomical patterns from fMRI and investigate relationships between N400 amplitude and neuroanatomical activation in schizophrenia patients and healthy controls.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6715348</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[fear conditioning and extinction paradigm]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6715348</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Fear Conditioning Paradigm]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants underwent a differential fear conditioning paradigm during fMRI to study cerebellar activation related to predictions and prediction errors.]</td>\n",
       "      <td>To address the contribution of the cerebellum in processing predictions and prediction errors during a fear conditioning paradigm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6715348</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Differential Fear Conditioning]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants underwent a differential fear conditioning paradigm using visual stimuli as conditioned stimuli (CS) and an electric shock as an unconditioned stimulus (US).]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality  \\\n",
       "6715348    delavega_nv  [fMRI-BOLD]   \n",
       "6715348    gpt-4o-mini  [fMRI-BOLD]   \n",
       "6715348         gpt-4o  [fMRI-BOLD]   \n",
       "\n",
       "                                            TaskName Exclude HasRestingState  \\\n",
       "6715348  [fear conditioning and extinction paradigm]    None           False   \n",
       "6715348                 [Fear Conditioning Paradigm]    None           False   \n",
       "6715348             [Differential Fear Conditioning]    None           False   \n",
       "\n",
       "                                                                                                                                                                      TaskDescription  \\\n",
       "6715348                                                                                                                                                                          None   \n",
       "6715348                   [Participants underwent a differential fear conditioning paradigm during fMRI to study cerebellar activation related to predictions and prediction errors.]   \n",
       "6715348  [Participants underwent a differential fear conditioning paradigm using visual stimuli as conditioned stimuli (CS) and an electric shock as an unconditioned stimulus (US).]   \n",
       "\n",
       "                                                                                                                             StudyObjective  \n",
       "6715348                                                                                                                                 NaN  \n",
       "6715348  To address the contribution of the cerebellum in processing predictions and prediction errors during a fear conditioning paradigm.  \n",
       "6715348                                                                                                                                None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7018765</th>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Testing Emotional Attunement and Mutuality (TEAM)]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[his task builds on past studies that have utilized error processing paradigms in the study of ER (e.g.,  ;  ;  ) by specifically examining dyadic error processing. Parent-adolescent dyads completed the TEAM task while simultaneously undergoing fMRI scanning. This task was developed to examine b...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7018765</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[TEAM task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[The TEAM task is designed to probe emotional reactivity and regulation in a dyadic context, specifically examining dyadic error processing.]</td>\n",
       "      <td>The current study examined brain activation related to parenting and emotion regulation in parent-adolescent dyads during concurrent fMRI scanning with a novel task, the Testing Emotional Attunement and Mutuality (TEAM) task.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7018765</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Testing Emotional Attunement and Mutuality (TEAM) task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[The TEAM task is designed to probe emotional reactivity and regulation in a dyadic context by examining brain activation in both parents and adolescents when the other dyad member makes a costly error.]</td>\n",
       "      <td>The study examined brain activation related to parenting and emotion regulation (ER) in parent-adolescent dyads during concurrent fMRI scanning with the TEAM task, aiming to study dyadic error processing and how it relates to parenting practices.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                annotator_name     Modality  \\\n",
       "7018765  delavega-aliceoverlap  [fMRI-BOLD]   \n",
       "7018765            gpt-4o-mini  [fMRI-BOLD]   \n",
       "7018765                 gpt-4o  [fMRI-BOLD]   \n",
       "\n",
       "                                                         TaskName Exclude  \\\n",
       "7018765       [Testing Emotional Attunement and Mutuality (TEAM)]    None   \n",
       "7018765                                               [TEAM task]    None   \n",
       "7018765  [Testing Emotional Attunement and Mutuality (TEAM) task]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "7018765           False   \n",
       "7018765           False   \n",
       "7018765           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "7018765  [his task builds on past studies that have utilized error processing paradigms in the study of ER (e.g.,  ;  ;  ) by specifically examining dyadic error processing. Parent-adolescent dyads completed the TEAM task while simultaneously undergoing fMRI scanning. This task was developed to examine b...   \n",
       "7018765                                                                                                                                                                [The TEAM task is designed to probe emotional reactivity and regulation in a dyadic context, specifically examining dyadic error processing.]   \n",
       "7018765                                                                                                  [The TEAM task is designed to probe emotional reactivity and regulation in a dyadic context by examining brain activation in both parents and adolescents when the other dyad member makes a costly error.]   \n",
       "\n",
       "                                                                                                                                                                                                                                                 StudyObjective  \n",
       "7018765                                                                                                                                                                                                                                                     NaN  \n",
       "7018765                       The current study examined brain activation related to parenting and emotion regulation in parent-adolescent dyads during concurrent fMRI scanning with a novel task, the Testing Emotional Attunement and Mutuality (TEAM) task.  \n",
       "7018765  The study examined brain activation related to parenting and emotion regulation (ER) in parent-adolescent dyads during concurrent fMRI scanning with the TEAM task, aiming to study dyadic error processing and how it relates to parenting practices.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8857499</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[giving task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[adolescents divided either a small or large number of coins (giving magnitude manipulation) between themselves and either a friend or unfamiliar peer (target manipulation) in an audience or anonymous context (peer presence manipulation), #### Giving magnitude: giving small and large amounts \\n ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8857499</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Giving Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants divided either a small or large number of coins between themselves and either a friend or unfamiliar peer in an audience or anonymous context.]</td>\n",
       "      <td>This pre-registered fMRI study investigated behavioral and neural correlates of adolescents' small versus large size giving in different social contexts related to target (i.e., giving to a friend or unfamiliar peer) and peer presence (i.e., anonymous versus audience giving).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8857499</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Modified Dictator Game]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants divided coins between themselves and another person in either a small or large giving condition, with target and peer presence manipulations.]</td>\n",
       "      <td>This pre-registered fMRI study investigated behavioral and neural correlates of adolescents small versus large size giving in different social contexts related to target and peer presence.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality                  TaskName Exclude  \\\n",
       "8857499    delavega_nv  [fMRI-BOLD]             [giving task]    None   \n",
       "8857499    gpt-4o-mini  [fMRI-BOLD]             [Giving Task]    None   \n",
       "8857499         gpt-4o  [fMRI-BOLD]  [Modified Dictator Game]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "8857499           False   \n",
       "8857499           False   \n",
       "8857499           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "8857499  [adolescents divided either a small or large number of coins (giving magnitude manipulation) between themselves and either a friend or unfamiliar peer (target manipulation) in an audience or anonymous context (peer presence manipulation), #### Giving magnitude: giving small and large amounts \\n ...   \n",
       "8857499                                                                                                                                                [Participants divided either a small or large number of coins between themselves and either a friend or unfamiliar peer in an audience or anonymous context.]   \n",
       "8857499                                                                                                                                                 [Participants divided coins between themselves and another person in either a small or large giving condition, with target and peer presence manipulations.]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                               StudyObjective  \n",
       "8857499                                                                                                                                                                                                                                                                                   NaN  \n",
       "8857499  This pre-registered fMRI study investigated behavioral and neural correlates of adolescents' small versus large size giving in different social contexts related to target (i.e., giving to a friend or unfamiliar peer) and peer presence (i.e., anonymous versus audience giving).  \n",
       "8857499                                                                                          This pre-registered fMRI study investigated behavioral and neural correlates of adolescents small versus large size giving in different social contexts related to target and peer presence.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9910278</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[emotion reappraisal, dietary self-control]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910278</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Emotion Regulation Task, Dietary Self-Control Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants were presented with positive, negative, and neutral stimuli and were asked to either view or reappraise the content to regulate their feelings., Participants made choices about whether to eat presented food items based on their healthiness and tastiness ratings.]</td>\n",
       "      <td>To investigate behavioral and neural associations in self-regulation across emotion regulation and dietary choice tasks in human participants.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910278</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Emotion Regulation Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Investigates reappraisal of emotional scenes (positive, negative) to assess self-regulation.]</td>\n",
       "      <td>To investigate behavioral and neural associations in self-regulation across emotion reappraisal and dietary choice domains in human participants.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality  \\\n",
       "9910278    delavega_nv  [fMRI-BOLD]   \n",
       "9910278    gpt-4o-mini  [fMRI-BOLD]   \n",
       "9910278         gpt-4o  [fMRI-BOLD]   \n",
       "\n",
       "                                                     TaskName Exclude  \\\n",
       "9910278           [emotion reappraisal, dietary self-control]    None   \n",
       "9910278  [Emotion Regulation Task, Dietary Self-Control Task]    None   \n",
       "9910278                             [Emotion Regulation Task]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "9910278           False   \n",
       "9910278           False   \n",
       "9910278           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                               TaskDescription  \\\n",
       "9910278                                                                                                                                                                                                                                                                                   None   \n",
       "9910278  [Participants were presented with positive, negative, and neutral stimuli and were asked to either view or reappraise the content to regulate their feelings., Participants made choices about whether to eat presented food items based on their healthiness and tastiness ratings.]   \n",
       "9910278                                                                                                                                                                                         [Investigates reappraisal of emotional scenes (positive, negative) to assess self-regulation.]   \n",
       "\n",
       "                                                                                                                                            StudyObjective  \n",
       "9910278                                                                                                                                                NaN  \n",
       "9910278     To investigate behavioral and neural associations in self-regulation across emotion regulation and dietary choice tasks in human participants.  \n",
       "9910278  To investigate behavioral and neural associations in self-regulation across emotion reappraisal and dietary choice domains in human participants.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11063816</th>\n",
       "      <td>delavega-other</td>\n",
       "      <td>[fMRI-BOLD, fMRI-BOLD]</td>\n",
       "      <td>[six-alternative forced-choice cued-recognition task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[On each trial, a cue and six potential targets were presented simultaneously on the screen. The cue was presented in the middle of the screen with the six possible targets; one target and five foils form the same category (e.g., if the target was hammer, the five foils would be other randomly s...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11063816</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Cued-Recognition Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants performed a six-alternative forced-choice cued-recognition task to retrieve overlapping elements from learned events.]</td>\n",
       "      <td>To assess whether episodic events continue to be reinstated holistically and whether hippocampal pattern completion continues to facilitate holistic reinstatement following a period of consolidation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11063816</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Holistic Episodic Memory Retrieval Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[A task to assess hippocampal pattern completion and its relationship to neocortical reinstatement across delays.]</td>\n",
       "      <td>To assess whether episodic events continue to be reinstated holistically and whether hippocampal pattern completion continues to facilitate holistic reinstatement following a period of consolidation.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          annotator_name                Modality  \\\n",
       "11063816  delavega-other  [fMRI-BOLD, fMRI-BOLD]   \n",
       "11063816     gpt-4o-mini             [fMRI-BOLD]   \n",
       "11063816          gpt-4o             [fMRI-BOLD]   \n",
       "\n",
       "                                                       TaskName Exclude  \\\n",
       "11063816  [six-alternative forced-choice cued-recognition task]    None   \n",
       "11063816                                [Cued-Recognition Task]    None   \n",
       "11063816              [Holistic Episodic Memory Retrieval Task]    None   \n",
       "\n",
       "         HasRestingState  \\\n",
       "11063816           False   \n",
       "11063816           False   \n",
       "11063816           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      TaskDescription  \\\n",
       "11063816  [On each trial, a cue and six potential targets were presented simultaneously on the screen. The cue was presented in the middle of the screen with the six possible targets; one target and five foils form the same category (e.g., if the target was hammer, the five foils would be other randomly s...   \n",
       "11063816                                                                                                                                                                         [Participants performed a six-alternative forced-choice cued-recognition task to retrieve overlapping elements from learned events.]   \n",
       "11063816                                                                                                                                                                                           [A task to assess hippocampal pattern completion and its relationship to neocortical reinstatement across delays.]   \n",
       "\n",
       "                                                                                                                                                                                                   StudyObjective  \n",
       "11063816                                                                                                                                                                                                      NaN  \n",
       "11063816  To assess whether episodic events continue to be reinstated holistically and whether hippocampal pattern completion continues to facilitate holistic reinstatement following a period of consolidation.  \n",
       "11063816  To assess whether episodic events continue to be reinstated holistically and whether hippocampal pattern completion continues to facilitate holistic reinstatement following a period of consolidation.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Diff on TaskName\n",
    "\n",
    "has_taskname_df = combined_df[combined_df.index.isin(has_task_name)]\n",
    "\n",
    "diff_taskname = set(all_scores_df[all_scores_df['TaskName'] < 0.8].index.tolist()).union(set(all_scores_df4[all_scores_df4['TaskName'] < 0.8].index.tolist()))\n",
    "\n",
    "if diff_taskname:\n",
    "    _display(has_taskname_df, diff_taskname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the incorrect cases have a reasonable guess. Not clear GPT4o is better than mini though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4488375</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[The task used in this experiment was programmed using PsychoPy [ ]. The task featured a 0-back and a 1-back condition that continuously switched from one another throughout the experimental session (see  ). Our paradigm is broadly similar to the paradigm used by Smallwood and colleagues [ ] and...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488375</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Shape Location Decision Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants made decisions about the location of shapes presented on screen either based on the current trial (0-back) or the prior trial (1-back).]</td>\n",
       "      <td>To explore the hypothesis that the role of the DMN in higher order cognition is to allow cognition to be shaped by information from stored representations rather than information in the immediate environment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488375</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[0-back and 1-back task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[A novel paradigm was used to test the DMN function involving a 0-back task where shapes are identified based on current trial and 1-back task based on prior trial.]</td>\n",
       "      <td>The study explores the hypothesis that the default mode network's role in higher order cognition is to allow cognition to be guided by information from stored representations rather than the immediate environment.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality                        TaskName Exclude  \\\n",
       "4488375    delavega_nv  [fMRI-BOLD]                           [n/a]    None   \n",
       "4488375    gpt-4o-mini  [fMRI-BOLD]  [Shape Location Decision Task]    None   \n",
       "4488375         gpt-4o  [fMRI-BOLD]        [0-back and 1-back task]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "4488375           False   \n",
       "4488375           False   \n",
       "4488375           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "4488375  [The task used in this experiment was programmed using PsychoPy [ ]. The task featured a 0-back and a 1-back condition that continuously switched from one another throughout the experimental session (see  ). Our paradigm is broadly similar to the paradigm used by Smallwood and colleagues [ ] and...   \n",
       "4488375                                                                                                                                                       [Participants made decisions about the location of shapes presented on screen either based on the current trial (0-back) or the prior trial (1-back).]   \n",
       "4488375                                                                                                                                        [A novel paradigm was used to test the DMN function involving a 0-back task where shapes are identified based on current trial and 1-back task based on prior trial.]   \n",
       "\n",
       "                                                                                                                                                                                                                StudyObjective  \n",
       "4488375                                                                                                                                                                                                                    NaN  \n",
       "4488375       To explore the hypothesis that the role of the DMN in higher order cognition is to allow cognition to be shaped by information from stored representations rather than information in the immediate environment.  \n",
       "4488375  The study explores the hypothesis that the default mode network's role in higher order cognition is to allow cognition to be guided by information from stored representations rather than the immediate environment.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4547715</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[A) The task was conducted in an event-related design. Six trials from either the liked or the uninteresting male acquaintance were combined in a set. Each set contained pseudo-randomized four target trials (shown in red) with two control trials (shown in blue). (B) Each trial was created by a 2...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547715</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Gift Attractiveness Judgment Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants judged the attractiveness of gifts received from male acquaintances, focusing on the gift rather than the giver.]</td>\n",
       "      <td>To investigate how preferences for the giver and the type of gift interactively determine the reward value of a gift, specifically examining the neural processing for understanding a gift's social meaning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547715</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Gift Preference Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants judged the attractiveness of gifts from liked and uninteresting male acquaintances.]</td>\n",
       "      <td>Investigating how preferences for a giver modulate the neural processing of a gift's social meaning.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality                             TaskName  \\\n",
       "4547715    delavega_nv  [fMRI-BOLD]                                [n/a]   \n",
       "4547715    gpt-4o-mini  [fMRI-BOLD]  [Gift Attractiveness Judgment Task]   \n",
       "4547715         gpt-4o  [fMRI-BOLD]               [Gift Preference Task]   \n",
       "\n",
       "        Exclude HasRestingState  \\\n",
       "4547715    None           False   \n",
       "4547715    None           False   \n",
       "4547715    None           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "4547715  [A) The task was conducted in an event-related design. Six trials from either the liked or the uninteresting male acquaintance were combined in a set. Each set contained pseudo-randomized four target trials (shown in red) with two control trials (shown in blue). (B) Each trial was created by a 2...   \n",
       "4547715                                                                                                                                                                              [Participants judged the attractiveness of gifts received from male acquaintances, focusing on the gift rather than the giver.]   \n",
       "4547715                                                                                                                                                                                                           [Participants judged the attractiveness of gifts from liked and uninteresting male acquaintances.]   \n",
       "\n",
       "                                                                                                                                                                                                        StudyObjective  \n",
       "4547715                                                                                                                                                                                                            NaN  \n",
       "4547715  To investigate how preferences for the giver and the type of gift interactively determine the reward value of a gift, specifically examining the neural processing for understanding a gift's social meaning.  \n",
       "4547715                                                                                                           Investigating how preferences for a giver modulate the neural processing of a gift's social meaning.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4914983</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4914983</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Viewing Comedy Movie Clips]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants watched three comedy clips during fMRI scanning to study brain activity related to humor.]</td>\n",
       "      <td>To investigate which brain areas respond similarly when watching relatively long (~5min) movie clips of comedy genre and how self-rated experienced humorousness explains similarities in hemodynamic activity in these areas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4914983</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Viewing Comedy Clips]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Subjects watched comedy clips during fMRI to study neural processes underlying humor.]</td>\n",
       "      <td>To investigate which brain areas respond similarly when watching comedy movie clips and to explore the relationship between experienced humorousness and hemodynamic activity.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality                      TaskName Exclude  \\\n",
       "4914983    delavega_nv  [fMRI-BOLD]                         [n/a]    None   \n",
       "4914983    gpt-4o-mini  [fMRI-BOLD]  [Viewing Comedy Movie Clips]    None   \n",
       "4914983         gpt-4o  [fMRI-BOLD]        [Viewing Comedy Clips]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "4914983           False   \n",
       "4914983           False   \n",
       "4914983           False   \n",
       "\n",
       "                                                                                                  TaskDescription  \\\n",
       "4914983                                                                                                      None   \n",
       "4914983  [Participants watched three comedy clips during fMRI scanning to study brain activity related to humor.]   \n",
       "4914983                   [Subjects watched comedy clips during fMRI to study neural processes underlying humor.]   \n",
       "\n",
       "                                                                                                                                                                                                                         StudyObjective  \n",
       "4914983                                                                                                                                                                                                                             NaN  \n",
       "4914983  To investigate which brain areas respond similarly when watching relatively long (~5min) movie clips of comedy genre and how self-rated experienced humorousness explains similarities in hemodynamic activity in these areas.  \n",
       "4914983                                                  To investigate which brain areas respond similarly when watching comedy movie clips and to explore the relationship between experienced humorousness and hemodynamic activity.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5607552</th>\n",
       "      <td>delavega-aliceoverlap</td>\n",
       "      <td>[fMRI-BOLD, fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[a degraded sentence is first experienced as very difficult to understand and, after a single presentation of its intact counterpart, the intelligibility of this same degraded sentence reaches near‚Äêperfect level., functional run consisted of 9 D‚ÄêI‚ÄêD stimulus sets, each of which comprised three b...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5607552</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[DID stimulus set]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[The task involved presenting distorted sentences followed by their intact counterparts, and then the distorted sentences again to assess changes in intelligibility.]</td>\n",
       "      <td>We examined which brain areas are involved in the comprehension of acoustically distorted speech using an experimental paradigm where the same distorted sentence can be perceived at different levels of intelligibility.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5607552</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[DID (Distorted-Intact-Distorted) stimulus set task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[The task involves presenting acoustically distorted sentences to participants, followed by their intact versions, and then the distorted sentences again.]</td>\n",
       "      <td>Examine brain areas involved in the comprehension of acoustically distorted speech and how previous exposure to intact sentences enhances intelligibility using fMRI.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                annotator_name                Modality  \\\n",
       "5607552  delavega-aliceoverlap  [fMRI-BOLD, fMRI-BOLD]   \n",
       "5607552            gpt-4o-mini             [fMRI-BOLD]   \n",
       "5607552                 gpt-4o             [fMRI-BOLD]   \n",
       "\n",
       "                                                     TaskName Exclude  \\\n",
       "5607552                                                 [n/a]    None   \n",
       "5607552                                    [DID stimulus set]    None   \n",
       "5607552  [DID (Distorted-Intact-Distorted) stimulus set task]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "5607552           False   \n",
       "5607552           False   \n",
       "5607552           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "5607552  [a degraded sentence is first experienced as very difficult to understand and, after a single presentation of its intact counterpart, the intelligibility of this same degraded sentence reaches near‚Äêperfect level., functional run consisted of 9 D‚ÄêI‚ÄêD stimulus sets, each of which comprised three b...   \n",
       "5607552                                                                                                                                       [The task involved presenting distorted sentences followed by their intact counterparts, and then the distorted sentences again to assess changes in intelligibility.]   \n",
       "5607552                                                                                                                                                  [The task involves presenting acoustically distorted sentences to participants, followed by their intact versions, and then the distorted sentences again.]   \n",
       "\n",
       "                                                                                                                                                                                                                     StudyObjective  \n",
       "5607552                                                                                                                                                                                                                         NaN  \n",
       "5607552  We examined which brain areas are involved in the comprehension of acoustically distorted speech using an experimental paradigm where the same distorted sentence can be perceived at different levels of intelligibility.  \n",
       "5607552                                                       Examine brain areas involved in the comprehension of acoustically distorted speech and how previous exposure to intact sentences enhances intelligibility using fMRI.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5662713</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants watched 24 short videos while in the MRI scanner. The videos lasted on average 38‚Äâseconds (range 29‚Äì48‚Äâs) and were taken from short films or videos posted on   www.YouTube.com  . All videos depicted a short narrative and were presented without sound. The stories centered around one...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5662713</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Memory Encoding and Retrieval Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants watched and retrieved short videos while being scanned in an MRI scanner.]</td>\n",
       "      <td>To investigate memory reinstatement for detailed lifelike memories between encoding, immediate retrieval and one-week delayed retrieval.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5662713</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Memory Retrieval Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants watched 24 short videos and retrieved the memories during two phases: immediate retrieval on the same day and delayed retrieval one week later, inside the MRI scanner.]</td>\n",
       "      <td>To investigate the memory reinstatement for detailed lifelike memories between encoding, immediate retrieval, and one-week delayed retrieval using functional MRI and representational similarity analyses (RSA).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality                              TaskName  \\\n",
       "5662713    delavega_nv  [fMRI-BOLD]                                 [n/a]   \n",
       "5662713    gpt-4o-mini  [fMRI-BOLD]  [Memory Encoding and Retrieval Task]   \n",
       "5662713         gpt-4o  [fMRI-BOLD]               [Memory Retrieval Task]   \n",
       "\n",
       "        Exclude HasRestingState  \\\n",
       "5662713    None           False   \n",
       "5662713    None           False   \n",
       "5662713    None           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "5662713  [Participants watched 24 short videos while in the MRI scanner. The videos lasted on average 38‚Äâseconds (range 29‚Äì48‚Äâs) and were taken from short films or videos posted on   www.YouTube.com  . All videos depicted a short narrative and were presented without sound. The stories centered around one...   \n",
       "5662713                                                                                                                                                                                                                     [Participants watched and retrieved short videos while being scanned in an MRI scanner.]   \n",
       "5662713                                                                                                                       [Participants watched 24 short videos and retrieved the memories during two phases: immediate retrieval on the same day and delayed retrieval one week later, inside the MRI scanner.]   \n",
       "\n",
       "                                                                                                                                                                                                            StudyObjective  \n",
       "5662713                                                                                                                                                                                                                NaN  \n",
       "5662713                                                                           To investigate memory reinstatement for detailed lifelike memories between encoding, immediate retrieval and one-week delayed retrieval.  \n",
       "5662713  To investigate the memory reinstatement for detailed lifelike memories between encoding, immediate retrieval, and one-week delayed retrieval using functional MRI and representational similarity analyses (RSA).  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5716095</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5716095</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Empathy Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants imagined the target person in distressing scenes as either themselves or their family member.]</td>\n",
       "      <td>The current study aimed to capture empathy processing in an interpersonal context within mother-adolescent dyads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5716095</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Empathy Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants imagined the target person in distressing scenes as either themselves or their family to capture empathy processing.]</td>\n",
       "      <td>Capture empathy processing in an interpersonal context and examine neural pattern similarity for self and family conditions in mother-adolescent dyads.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality        TaskName Exclude HasRestingState  \\\n",
       "5716095    delavega_nv  [fMRI-BOLD]           [n/a]    None           False   \n",
       "5716095    gpt-4o-mini  [fMRI-BOLD]  [Empathy Task]    None           False   \n",
       "5716095         gpt-4o  [fMRI-BOLD]  [Empathy Task]    None           False   \n",
       "\n",
       "                                                                                                                             TaskDescription  \\\n",
       "5716095                                                                                                                                 None   \n",
       "5716095                         [Participants imagined the target person in distressing scenes as either themselves or their family member.]   \n",
       "5716095  [Participants imagined the target person in distressing scenes as either themselves or their family to capture empathy processing.]   \n",
       "\n",
       "                                                                                                                                                  StudyObjective  \n",
       "5716095                                                                                                                                                      NaN  \n",
       "5716095                                        The current study aimed to capture empathy processing in an interpersonal context within mother-adolescent dyads.  \n",
       "5716095  Capture empathy processing in an interpersonal context and examine neural pattern similarity for self and family conditions in mother-adolescent dyads.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5895040</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a, n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[In the perceptual task, subjects were asked to report the brighter of two stimuli on each trial, In the memory task, subjects began each miniblock by learning a set of nine consecutively presented stimuli]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5895040</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Perceptual Task, Memory Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Subjects were asked to report the brighter of two stimuli on each trial., Subjects began each miniblock by learning a set of nine consecutively presented stimuli and then had to identify the studied stimulus.]</td>\n",
       "      <td>To investigate the neural substrates engaged when metacognitive judgments are made during perceptual and memory tasks matched for stimulus and performance characteristics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5895040</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Perceptual and Memory Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants performed perceptual and memory tasks to evaluate levels of confidence associated with their performance.]</td>\n",
       "      <td>Investigate whether metacognition relies on a domain-general resource or if self-evaluative processes are domain specific by examining neural substrates during perceptual and memory tasks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality                        TaskName Exclude  \\\n",
       "5895040    delavega_nv  [fMRI-BOLD]                      [n/a, n/a]    None   \n",
       "5895040    gpt-4o-mini  [fMRI-BOLD]  [Perceptual Task, Memory Task]    None   \n",
       "5895040         gpt-4o  [fMRI-BOLD]    [Perceptual and Memory Task]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "5895040           False   \n",
       "5895040           False   \n",
       "5895040           False   \n",
       "\n",
       "                                                                                                                                                                                                            TaskDescription  \\\n",
       "5895040      [In the perceptual task, subjects were asked to report the brighter of two stimuli on each trial, In the memory task, subjects began each miniblock by learning a set of nine consecutively presented stimuli]   \n",
       "5895040  [Subjects were asked to report the brighter of two stimuli on each trial., Subjects began each miniblock by learning a set of nine consecutively presented stimuli and then had to identify the studied stimulus.]   \n",
       "5895040                                                                                            [Participants performed perceptual and memory tasks to evaluate levels of confidence associated with their performance.]   \n",
       "\n",
       "                                                                                                                                                                                       StudyObjective  \n",
       "5895040                                                                                                                                                                                           NaN  \n",
       "5895040                   To investigate the neural substrates engaged when metacognitive judgments are made during perceptual and memory tasks matched for stimulus and performance characteristics.  \n",
       "5895040  Investigate whether metacognition relies on a domain-general resource or if self-evaluative processes are domain specific by examining neural substrates during perceptual and memory tasks.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6102316</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6102316</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Naturalistic Movie Viewing]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants watched a 20-minute movie excerpt that included various social interactions.]</td>\n",
       "      <td>To investigate the neural correlates of social interactions using a semiotic framework during naturalistic viewing of a movie narrative.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6102316</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Movie Watching Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants watched a film sequence showing a movie narrative with social interactions, which was coded into different interaction types based on Peirce's semiotic categories.]</td>\n",
       "      <td>To investigate how cognitive-semiotic categories differentiate neural processes during the observation of non-verbal social interactions in a naturalistic setting.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name  \\\n",
       "6102316    delavega_nv   \n",
       "6102316    gpt-4o-mini   \n",
       "6102316         gpt-4o   \n",
       "\n",
       "                                                                                                    Modality  \\\n",
       "6102316  [fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD, fMRI-BOLD]   \n",
       "6102316                                                                                          [fMRI-BOLD]   \n",
       "6102316                                                                                          [fMRI-BOLD]   \n",
       "\n",
       "                             TaskName Exclude HasRestingState  \\\n",
       "6102316                         [n/a]    None           False   \n",
       "6102316  [Naturalistic Movie Viewing]    None           False   \n",
       "6102316         [Movie Watching Task]    None           False   \n",
       "\n",
       "                                                                                                                                                                            TaskDescription  \\\n",
       "6102316                                                                                                                                                                                None   \n",
       "6102316                                                                                         [Participants watched a 20-minute movie excerpt that included various social interactions.]   \n",
       "6102316  [Participants watched a film sequence showing a movie narrative with social interactions, which was coded into different interaction types based on Peirce's semiotic categories.]   \n",
       "\n",
       "                                                                                                                                                              StudyObjective  \n",
       "6102316                                                                                                                                                                  NaN  \n",
       "6102316                             To investigate the neural correlates of social interactions using a semiotic framework during naturalistic viewing of a movie narrative.  \n",
       "6102316  To investigate how cognitive-semiotic categories differentiate neural processes during the observation of non-verbal social interactions in a naturalistic setting.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6382839</th>\n",
       "      <td>delavega-other</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6382839</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Script-driven imagery]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants were exposed to disorder-related and neutral narrative scripts while brain activation was measured with fMRI.]</td>\n",
       "      <td>The aim of the present fMRI study was to explore neural correlates of threat processing in a suited sample of PD patients by means of script-driven imagery based on a newly developed set of controlled disorder-related and neutral scenarios.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6382839</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Script-Driven Imagery Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants engaged in mental imagery of disorder-related and neutral narrative scripts to measure brain activation differences.]</td>\n",
       "      <td>Investigating brain activation in Panic Disorder (PD) patients during disorder-related script-driven imagery using fMRI.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         annotator_name     Modality                      TaskName Exclude  \\\n",
       "6382839  delavega-other  [fMRI-BOLD]                         [n/a]    None   \n",
       "6382839     gpt-4o-mini  [fMRI-BOLD]       [Script-driven imagery]    None   \n",
       "6382839          gpt-4o  [fMRI-BOLD]  [Script-Driven Imagery Task]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "6382839           False   \n",
       "6382839           False   \n",
       "6382839           False   \n",
       "\n",
       "                                                                                                                             TaskDescription  \\\n",
       "6382839                                                                                                                                 None   \n",
       "6382839         [Participants were exposed to disorder-related and neutral narrative scripts while brain activation was measured with fMRI.]   \n",
       "6382839  [Participants engaged in mental imagery of disorder-related and neutral narrative scripts to measure brain activation differences.]   \n",
       "\n",
       "                                                                                                                                                                                                                                           StudyObjective  \n",
       "6382839                                                                                                                                                                                                                                               NaN  \n",
       "6382839  The aim of the present fMRI study was to explore neural correlates of threat processing in a suited sample of PD patients by means of script-driven imagery based on a newly developed set of controlled disorder-related and neutral scenarios.  \n",
       "6382839                                                                                                                          Investigating brain activation in Panic Disorder (PD) patients during disorder-related script-driven imagery using fMRI.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6391069</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Auditory stimuli were presented in 10 separate runs, each lasting 4 min. One run consisted of 56 trials of mother‚Äôs voice, unfamiliar female voices, environmental sounds and catch trials, which were pseudo-randomly ordered within each run. Stimulus presentation order was the same for each subje...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6391069</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Voice Processing Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[The task involved processing unfamiliar voices and mothers' voices to assess brain activity and connectivity.]</td>\n",
       "      <td>To examine social information processing in children with autism spectrum disorder (ASD) by probing brain circuit function and connectivity in response to human vocal sounds.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6391069</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Voice Processing Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[This task examines brain responses to unfamiliar voices, mothers voice, and environmental sounds to understand voice processing in children with ASD.]</td>\n",
       "      <td>To examine neural responses to voice processing in children with ASD, focusing on the relationship between social communication abilities and brain activation in reward and salience regions.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality                 TaskName Exclude  \\\n",
       "6391069    delavega_nv  [fMRI-BOLD]                    [n/a]    None   \n",
       "6391069    gpt-4o-mini  [fMRI-BOLD]  [Voice Processing Task]    None   \n",
       "6391069         gpt-4o  [fMRI-BOLD]  [Voice Processing Task]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "6391069           False   \n",
       "6391069           False   \n",
       "6391069           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "6391069  [Auditory stimuli were presented in 10 separate runs, each lasting 4 min. One run consisted of 56 trials of mother‚Äôs voice, unfamiliar female voices, environmental sounds and catch trials, which were pseudo-randomly ordered within each run. Stimulus presentation order was the same for each subje...   \n",
       "6391069                                                                                                                                                                                              [The task involved processing unfamiliar voices and mothers' voices to assess brain activity and connectivity.]   \n",
       "6391069                                                                                                                                                      [This task examines brain responses to unfamiliar voices, mothers voice, and environmental sounds to understand voice processing in children with ASD.]   \n",
       "\n",
       "                                                                                                                                                                                         StudyObjective  \n",
       "6391069                                                                                                                                                                                             NaN  \n",
       "6391069                  To examine social information processing in children with autism spectrum disorder (ASD) by probing brain circuit function and connectivity in response to human vocal sounds.  \n",
       "6391069  To examine neural responses to voice processing in children with ASD, focusing on the relationship between social communication abilities and brain activation in reward and salience regions.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6397754</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a, watched]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[For this task, all participants were told that they would watch a clip from a nature documentary featuring Canadian bighorn sheep, and at times various words would appear and move around on the screen, but they were told to ‚Äúavoid reading these distractor words whenever they appear.‚Äù On average...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6397754</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Effortful Self-Control Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants were required to actively inhibit reading a series of words that appeared on the screen while watching a nature documentary.]</td>\n",
       "      <td>Investigate whether individual differences in recruitment of cognitive control regions during a difficult response inhibition task are associated with a failure to regulate neural responses to rewarding food cues in a subsequent task in a cohort of 27 female dieters.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6397754</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Effortful Self-Control Task, Food Cue Reactivity Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants were required to actively inhibit reading a series of words that appeared on the screen., Participants watched food commercials, assessing the brain's reward response.]</td>\n",
       "      <td>Investigate the relationship between individual differences in brain activity during self-control exertion and subsequent food cue exposure in dieters.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality  \\\n",
       "6397754    delavega_nv  [fMRI-BOLD]   \n",
       "6397754    gpt-4o-mini  [fMRI-BOLD]   \n",
       "6397754         gpt-4o  [fMRI-BOLD]   \n",
       "\n",
       "                                                        TaskName Exclude  \\\n",
       "6397754                                           [n/a, watched]    None   \n",
       "6397754                            [Effortful Self-Control Task]    None   \n",
       "6397754  [Effortful Self-Control Task, Food Cue Reactivity Task]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "6397754           False   \n",
       "6397754           False   \n",
       "6397754           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "6397754  [For this task, all participants were told that they would watch a clip from a nature documentary featuring Canadian bighorn sheep, and at times various words would appear and move around on the screen, but they were told to ‚Äúavoid reading these distractor words whenever they appear.‚Äù On average...   \n",
       "6397754                                                                                                                                                                  [Participants were required to actively inhibit reading a series of words that appeared on the screen while watching a nature documentary.]   \n",
       "6397754                                                                                                                       [Participants were required to actively inhibit reading a series of words that appeared on the screen., Participants watched food commercials, assessing the brain's reward response.]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                      StudyObjective  \n",
       "6397754                                                                                                                                                                                                                                                                          NaN  \n",
       "6397754  Investigate whether individual differences in recruitment of cognitive control regions during a difficult response inhibition task are associated with a failure to regulate neural responses to rewarding food cues in a subsequent task in a cohort of 27 female dieters.  \n",
       "6397754                                                                                                                      Investigate the relationship between individual differences in brain activity during self-control exertion and subsequent food cue exposure in dieters.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6821801</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[During the event-related fMRI sessions, participants performed either purchasing or perceptual decisions in alternating blocks (Fig.¬† ). Each good was presented twice: once during the purchasing decision and once during the perceptual decision. Before each block started, both blocks were cued v...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6821801</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Purchasing Decisions, Perceptual Decisions]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants indicated their willingness to pay a specific price for a book during fMRI scanning., Participants indicated the total number of people and lines on the book cover during fMRI scanning.]</td>\n",
       "      <td>To understand value processing of consumers using fMRI, specifically how consumers explicitly and task-irrelevantly encode the values of goods with affect-based and cognitively-based appeal and how trait reward sensitivity modulates this process.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6821801</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Purchasing and Perceptual Decision Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants perform purchasing decisions for explicit economic value and perceptual decisions for task-irrelevant value using identical book cover stimuli.]</td>\n",
       "      <td>The study aims to clarify how consumers explicitly and task-irrelevantly encode the values of hedonic and utilitarian goods, and how trait reward sensitivity modulates the task-irrelevant value signals of affectively- vs. cognitively-based goods using fMRI.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality  \\\n",
       "6821801    delavega_nv  [fMRI-BOLD]   \n",
       "6821801    gpt-4o-mini  [fMRI-BOLD]   \n",
       "6821801         gpt-4o  [fMRI-BOLD]   \n",
       "\n",
       "                                             TaskName Exclude HasRestingState  \\\n",
       "6821801                                         [n/a]    None           False   \n",
       "6821801  [Purchasing Decisions, Perceptual Decisions]    None           False   \n",
       "6821801     [Purchasing and Perceptual Decision Task]    None           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "6821801  [During the event-related fMRI sessions, participants performed either purchasing or perceptual decisions in alternating blocks (Fig.¬† ). Each good was presented twice: once during the purchasing decision and once during the perceptual decision. Before each block started, both blocks were cued v...   \n",
       "6821801                                                                                                     [Participants indicated their willingness to pay a specific price for a book during fMRI scanning., Participants indicated the total number of people and lines on the book cover during fMRI scanning.]   \n",
       "6821801                                                                                                                                               [Participants perform purchasing decisions for explicit economic value and perceptual decisions for task-irrelevant value using identical book cover stimuli.]   \n",
       "\n",
       "                                                                                                                                                                                                                                                            StudyObjective  \n",
       "6821801                                                                                                                                                                                                                                                                NaN  \n",
       "6821801             To understand value processing of consumers using fMRI, specifically how consumers explicitly and task-irrelevantly encode the values of goods with affect-based and cognitively-based appeal and how trait reward sensitivity modulates this process.  \n",
       "6821801  The study aims to clarify how consumers explicitly and task-irrelevantly encode the values of hedonic and utilitarian goods, and how trait reward sensitivity modulates the task-irrelevant value signals of affectively- vs. cognitively-based goods using fMRI.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7859438</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[It measures predominantly memory encoding, but also perception and attention in both the auditory and visual domains within 10 min of fMRI acquisition time using simple instructions. To our knowledge, memory-encoding paradigms so far presented stimuli of one sensory condition or did face‚Äìname a...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859438</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Memory Encoding Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[A task designed to measure memory encoding using auditory and visual stimuli.]</td>\n",
       "      <td>To introduce a new and time-efficient memory-encoding paradigm for functional magnetic resonance imaging (fMRI) optimized for mapping multiple contrasts using auditory and visual stimuli.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859438</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Memory Encoding Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[A memory encoding task involving auditory (environmental/vocal) and visual (scene/face) stimuli optimized for mapping multiple contrasts using a mixed design.]</td>\n",
       "      <td>A Functional MRI Paradigm for Efficient Mapping of Memory Encoding Across Sensory Conditions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality                TaskName Exclude  \\\n",
       "7859438    delavega_nv  [fMRI-BOLD]                   [n/a]    None   \n",
       "7859438    gpt-4o-mini  [fMRI-BOLD]  [Memory Encoding Task]    None   \n",
       "7859438         gpt-4o  [fMRI-BOLD]  [Memory Encoding Task]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "7859438           False   \n",
       "7859438           False   \n",
       "7859438           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "7859438  [It measures predominantly memory encoding, but also perception and attention in both the auditory and visual domains within 10 min of fMRI acquisition time using simple instructions. To our knowledge, memory-encoding paradigms so far presented stimuli of one sensory condition or did face‚Äìname a...   \n",
       "7859438                                                                                                                                                                                                                              [A task designed to measure memory encoding using auditory and visual stimuli.]   \n",
       "7859438                                                                                                                                             [A memory encoding task involving auditory (environmental/vocal) and visual (scene/face) stimuli optimized for mapping multiple contrasts using a mixed design.]   \n",
       "\n",
       "                                                                                                                                                                                      StudyObjective  \n",
       "7859438                                                                                                                                                                                          NaN  \n",
       "7859438  To introduce a new and time-efficient memory-encoding paradigm for functional magnetic resonance imaging (fMRI) optimized for mapping multiple contrasts using auditory and visual stimuli.  \n",
       "7859438                                                                                                 A Functional MRI Paradigm for Efficient Mapping of Memory Encoding Across Sensory Conditions  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8104963</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[The main experimental session was organized in three blocks. In the first block, participants saw a fixation cross (1 s) followed by one video (3 s) and were asked to estimate the amount of pain experienced by the person in the video, by moving a randomly-presented rectangular cursor on a Visua...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8104963</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Pain Assessment Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants appraised the pain of facial expression video-clips and subsequently were confronted with feedbacks from the person in pain and the average opinion of medical practitioners.]</td>\n",
       "      <td>To test whether senior medical students differed from younger colleagues and lay controls in the way they assess people's pain and take into consideration their feedback.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8104963</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Pain facial expression appraisal task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants evaluated the pain of facial expression video-clips and received feedback from the video's subject and medical practitioners.]</td>\n",
       "      <td>To investigate how senior medical students, younger colleagues, and lay controls differ in assessing people's pain and considering feedback, and how medical training affects sensitivity to pain faces and neural responses.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality                                 TaskName  \\\n",
       "8104963    delavega_nv  [fMRI-BOLD]                                    [n/a]   \n",
       "8104963    gpt-4o-mini  [fMRI-BOLD]                   [Pain Assessment Task]   \n",
       "8104963         gpt-4o  [fMRI-BOLD]  [Pain facial expression appraisal task]   \n",
       "\n",
       "        Exclude HasRestingState  \\\n",
       "8104963    None           False   \n",
       "8104963    None           False   \n",
       "8104963    None           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "8104963  [The main experimental session was organized in three blocks. In the first block, participants saw a fixation cross (1 s) followed by one video (3 s) and were asked to estimate the amount of pain experienced by the person in the video, by moving a randomly-presented rectangular cursor on a Visua...   \n",
       "8104963                                                                                                                 [Participants appraised the pain of facial expression video-clips and subsequently were confronted with feedbacks from the person in pain and the average opinion of medical practitioners.]   \n",
       "8104963                                                                                                                                                                 [Participants evaluated the pain of facial expression video-clips and received feedback from the video's subject and medical practitioners.]   \n",
       "\n",
       "                                                                                                                                                                                                                        StudyObjective  \n",
       "8104963                                                                                                                                                                                                                            NaN  \n",
       "8104963                                                     To test whether senior medical students differed from younger colleagues and lay controls in the way they assess people's pain and take into consideration their feedback.  \n",
       "8104963  To investigate how senior medical students, younger colleagues, and lay controls differ in assessing people's pain and considering feedback, and how medical training affects sensitivity to pain faces and neural responses.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8443248</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants were explicitly instructed to recreate the feelings of the demonstrators shown in the videos as vividly and intensely as possible. Based on the validation and pilot study, the painful   expressions   for the genuine and pretended conditions were matched. We also counterbalanced the...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443248</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Viewing Pain Expressions]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants watched video clips of individuals expressing genuine or pretended pain.]</td>\n",
       "      <td>To investigate the neural dynamics between the anterior insular cortex and right supramarginal gyrus in the context of genuine vs. pretended pain, focusing on affect sharing and self-other distinction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443248</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Pain Perception Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants watched video clips of people displaying either genuine or pretended facial expressions of pain.]</td>\n",
       "      <td>To investigate how the brain network involved in affect sharing and self-other distinction responds to genuine versus pretended pain, and to determine whether neural responses previously linked to affect sharing could result from the perception of salient affective displays.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality                    TaskName Exclude  \\\n",
       "8443248    delavega_nv  [fMRI-BOLD]                       [n/a]    None   \n",
       "8443248    gpt-4o-mini  [fMRI-BOLD]  [Viewing Pain Expressions]    None   \n",
       "8443248         gpt-4o  [fMRI-BOLD]      [Pain Perception Task]    None   \n",
       "\n",
       "        HasRestingState  \\\n",
       "8443248           False   \n",
       "8443248           False   \n",
       "8443248           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     TaskDescription  \\\n",
       "8443248  [Participants were explicitly instructed to recreate the feelings of the demonstrators shown in the videos as vividly and intensely as possible. Based on the validation and pilot study, the painful   expressions   for the genuine and pretended conditions were matched. We also counterbalanced the...   \n",
       "8443248                                                                                                                                                                                                                      [Participants watched video clips of individuals expressing genuine or pretended pain.]   \n",
       "8443248                                                                                                                                                                                              [Participants watched video clips of people displaying either genuine or pretended facial expressions of pain.]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                              StudyObjective  \n",
       "8443248                                                                                                                                                                                                                                                                                  NaN  \n",
       "8443248                                                                            To investigate the neural dynamics between the anterior insular cortex and right supramarginal gyrus in the context of genuine vs. pretended pain, focusing on affect sharing and self-other distinction.  \n",
       "8443248  To investigate how the brain network involved in affect sharing and self-other distinction responds to genuine versus pretended pain, and to determine whether neural responses previously linked to affect sharing could result from the perception of salient affective displays.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8927597</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8927597</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[2-back updating task (2Back)]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants were required to determine whether the current target was identical to the stimulus that appeared two trials before.]</td>\n",
       "      <td>To examine the neural mechanisms associated with retrieval of weak representations using functional magnetic resonance imaging and their potential relationships with creativity task performance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8927597</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[2-back updating task (2Back) and classification task (Classification)]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[A task paradigm including a 2-back updating task, which appeared after a classification task, to differentiate the retrieval processes for previously ignored vs. attended stimuli.]</td>\n",
       "      <td>To examine the neural mechanisms associated with retrieval of weak representations using functional magnetic resonance imaging and their potential relationships with creativity task performance.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality  \\\n",
       "8927597    delavega_nv  [fMRI-BOLD]   \n",
       "8927597    gpt-4o-mini  [fMRI-BOLD]   \n",
       "8927597         gpt-4o  [fMRI-BOLD]   \n",
       "\n",
       "                                                                        TaskName  \\\n",
       "8927597                                                                    [n/a]   \n",
       "8927597                                           [2-back updating task (2Back)]   \n",
       "8927597  [2-back updating task (2Back) and classification task (Classification)]   \n",
       "\n",
       "        Exclude HasRestingState  \\\n",
       "8927597    None           False   \n",
       "8927597    None           False   \n",
       "8927597    None           False   \n",
       "\n",
       "                                                                                                                                                                               TaskDescription  \\\n",
       "8927597                                                                                                                                                                                   None   \n",
       "8927597                                                    [Participants were required to determine whether the current target was identical to the stimulus that appeared two trials before.]   \n",
       "8927597  [A task paradigm including a 2-back updating task, which appeared after a classification task, to differentiate the retrieval processes for previously ignored vs. attended stimuli.]   \n",
       "\n",
       "                                                                                                                                                                                             StudyObjective  \n",
       "8927597                                                                                                                                                                                                 NaN  \n",
       "8927597  To examine the neural mechanisms associated with retrieval of weak representations using functional magnetic resonance imaging and their potential relationships with creativity task performance.  \n",
       "8927597  To examine the neural mechanisms associated with retrieval of weak representations using functional magnetic resonance imaging and their potential relationships with creativity task performance.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8975992</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8975992</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Naturalistic Stimuli Presentation]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants watched a Hollywood movie and listened to its audio-description to investigate spatial information processing.]</td>\n",
       "      <td>To investigate the perception of scene-related, spatial information embedded in two naturalistic stimuli (a movie and its audio-description) and to model hemodynamic activity based on annotations of selected stimulus features.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8975992</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Movie Watching Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[Participants watch a Hollywood movie (Forrest Gump) while hemodynamic activity is monitored.]</td>\n",
       "      <td>Investigating the perception of scene-related, spatial information embedded in naturalistic stimuli, specifically a Hollywood movie and its audio-description, to understand activation in the parahippocampal place area (PPA).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        annotator_name     Modality                             TaskName  \\\n",
       "8975992    delavega_nv  [fMRI-BOLD]                                [n/a]   \n",
       "8975992    gpt-4o-mini  [fMRI-BOLD]  [Naturalistic Stimuli Presentation]   \n",
       "8975992         gpt-4o  [fMRI-BOLD]                [Movie Watching Task]   \n",
       "\n",
       "        Exclude HasRestingState  \\\n",
       "8975992    None           False   \n",
       "8975992    None           False   \n",
       "8975992    None           False   \n",
       "\n",
       "                                                                                                                       TaskDescription  \\\n",
       "8975992                                                                                                                           None   \n",
       "8975992  [Participants watched a Hollywood movie and listened to its audio-description to investigate spatial information processing.]   \n",
       "8975992                                 [Participants watch a Hollywood movie (Forrest Gump) while hemodynamic activity is monitored.]   \n",
       "\n",
       "                                                                                                                                                                                                                             StudyObjective  \n",
       "8975992                                                                                                                                                                                                                                 NaN  \n",
       "8975992  To investigate the perception of scene-related, spatial information embedded in two naturalistic stimuli (a movie and its audio-description) and to model hemodynamic activity based on annotations of selected stimulus features.  \n",
       "8975992    Investigating the perception of scene-related, spatial information embedded in naturalistic stimuli, specifically a Hollywood movie and its audio-description, to understand activation in the parahippocampal place area (PPA).  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_name</th>\n",
       "      <th>Modality</th>\n",
       "      <th>TaskName</th>\n",
       "      <th>Exclude</th>\n",
       "      <th>HasRestingState</th>\n",
       "      <th>TaskDescription</th>\n",
       "      <th>StudyObjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10028637</th>\n",
       "      <td>delavega_nv</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[n/a]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10028637</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>[fMRI-BOLD]</td>\n",
       "      <td>[Auditory Target Detection Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[A computerized auditory target detection task aimed to assess individual responsiveness differences during moderate anaesthesia.]</td>\n",
       "      <td>To investigate whether variability or impairments in functional connectivity within and between the dorsal attention network (DAN), executive control network (ECN), and default mode network (DMN) underlie individual differences in responsiveness during propofol anaesthesia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10028637</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>[fMRI-BOLD, StructuralMRI]</td>\n",
       "      <td>[Narrative Understanding Task]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[Participants listened to a plot-driven auditory narrative while undergoing fMRI scans to investigate perceptual or high-level attention processes under anaesthesia.]</td>\n",
       "      <td>To investigate whether variability or impairments in functional connectivity (FC) within and between the dorsal attention network (DAN), executive control network (ECN), and default mode network (DMN) underlie individual differences in responsiveness during propofol anaesthesia.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         annotator_name                    Modality  \\\n",
       "10028637    delavega_nv                 [fMRI-BOLD]   \n",
       "10028637    gpt-4o-mini                 [fMRI-BOLD]   \n",
       "10028637         gpt-4o  [fMRI-BOLD, StructuralMRI]   \n",
       "\n",
       "                                  TaskName Exclude HasRestingState  \\\n",
       "10028637                             [n/a]    None            True   \n",
       "10028637  [Auditory Target Detection Task]    None           False   \n",
       "10028637    [Narrative Understanding Task]    None            True   \n",
       "\n",
       "                                                                                                                                                                 TaskDescription  \\\n",
       "10028637                                                                                                                                                                    None   \n",
       "10028637                                      [A computerized auditory target detection task aimed to assess individual responsiveness differences during moderate anaesthesia.]   \n",
       "10028637  [Participants listened to a plot-driven auditory narrative while undergoing fMRI scans to investigate perceptual or high-level attention processes under anaesthesia.]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                   StudyObjective  \n",
       "10028637                                                                                                                                                                                                                                                                                      NaN  \n",
       "10028637       To investigate whether variability or impairments in functional connectivity within and between the dorsal attention network (DAN), executive control network (ECN), and default mode network (DMN) underlie individual differences in responsiveness during propofol anaesthesia.  \n",
       "10028637  To investigate whether variability or impairments in functional connectivity (FC) within and between the dorsal attention network (DAN), executive control network (ECN), and default mode network (DMN) underlie individual differences in responsiveness during propofol anaesthesia.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Doesn't have TaskName (according to annotations)\n",
    "\n",
    "has_notaskname_df = combined_df[combined_df.index.isin(has_task_name) == False]\n",
    "\n",
    "diff_notaskname = set(all_scores_df[all_scores_df['TaskName'] < 0.8].index.tolist()).union(set(all_scores_df4[all_scores_df4['TaskName'] < 0.8].index.tolist()))\n",
    "\n",
    "if diff_notaskname:\n",
    "    _display(has_notaskname_df, diff_notaskname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times a reasonable task name is suggested, but sometimes a behavioral tasks is confused for scanner task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Focus on TaskDescription\n",
    "\n",
    "has_notaskname_df = combined_df[combined_df.index.isin(has_task_name) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "def _loop_print_row(df, fields=['TaskName', 'TaskDescription']):\n",
    "    for _, row in df.iterrows():\n",
    "        row[fields]\n",
    "        print(json.dumps(row[fields].to_dict(), indent=4))\n",
    "\n",
    "def _print_extracted_data(df):\n",
    "    for _, row in df.iterrows():        \n",
    "        # Print Behavioral Tasks\n",
    "        for task_type in ['BehavioralTasks', 'fMRITasks']:\n",
    "            _val = row[task_type]\n",
    "            if _val and isinstance(_val, list):\n",
    "                print(f'{task_type}:')\n",
    "                for task in _val:\n",
    "                    print(json.dumps(task, indent=4))\n",
    "        print('')\n",
    "\n",
    "def clean_print_by_pmcid(df):\n",
    "    for pmcid, _df in df.groupby('pmcid'):\n",
    "        print('PMCID:', pmcid)\n",
    "\n",
    "        for annotator_name, _extract in _df.groupby('annotator_name'):\n",
    "            print(f'--- {annotator_name} ---')\n",
    "            if 'delavega' in annotator_name:\n",
    "                _loop_print_row(_extract)\n",
    "            else:\n",
    "                print('\\tStudyObjectve:', _extract['StudyObjective'].iloc[0])\n",
    "                _print_extracted_data(_extract)\n",
    "            print('')\n",
    "\n",
    "\n",
    "        print('=============================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the following so instead of printing it appends to a list that can be used to generate a report\n",
    "import json \n",
    "def _loop_print_row(df, fields=['TaskName', 'TaskDescription']):\n",
    "    for _, row in df.iterrows():\n",
    "        row[fields]\n",
    "        print(json.dumps(row[fields].to_dict(), indent=4))\n",
    "\n",
    "def _print_extracted_data(df):\n",
    "    for _, row in df.iterrows():        \n",
    "        # Print Behavioral Tasks\n",
    "        for task_type in ['BehavioralTasks', 'fMRITasks']:\n",
    "            _val = row[task_type]\n",
    "            if _val and isinstance(_val, list):\n",
    "                print(f'{task_type}:')\n",
    "                for task in _val:\n",
    "                    print(json.dumps(task, indent=4))\n",
    "        print('')\n",
    "\n",
    "def clean_print_by_pmcid(df):\n",
    "    for pmcid, _df in df.groupby('pmcid'):\n",
    "        print('PMCID:', pmcid)\n",
    "\n",
    "        for annotator_name, _extract in _df.groupby('annotator_name'):\n",
    "            print(f'--- {annotator_name} ---')\n",
    "            if 'delavega' in annotator_name:\n",
    "                _loop_print_row(_extract)\n",
    "            else:\n",
    "                print('\\tStudyObjectve:', _extract['StudyObjective'].iloc[0])\n",
    "                _print_extracted_data(_extract)\n",
    "            print('')\n",
    "\n",
    "\n",
    "        print('=============================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMCID: 2241626\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"mental calculation task\",\n",
      "        \"language comprehension task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"In the present research, our goal was to define a simple fMRI test, less than 5 minutes long, that could delineate, in a subject-specific manner, those cerebral circuits. A functional sequence was added to each functional imaging session performed in our lab (Figure  ), taking advantage of the continuous flow of volunteers recruited for various protocols. Because we wanted to capture the maximal amount of functional information in the minimum amount of time, we designed the sequence according the following challenging constraints: \\n\\n\\u25aa the sequence had to be short, so as to disrupt as little as possible the main protocol. We choose 5 minutes for performing 100 trials. \\n\\n\\u25aa we aimed to obtain for each subject a description of different levels of functional architecture, from sensori-motor areas (perception and action) to more associative areas involved in reading, language processing and calculation. \\n\\n\\u25aa we aimed to capture in 5 minutes most of the individual networks related to each task. \\n\\n\\u25aa individual networks described in 5 min had to be reproducible over sessions and time. \\n\\nThe feasibility of using short stimulation designs (ranging from 10 to 25 min long) to reveal individual functional maps has been previously assessed for language mapping [ ], for visual areas [ ] and recently for a set of functional networks covering sensorimotor processes, working memory, executive functions and emotional processes [ ]. Beyond that point, the main goal of the current study was to use the data obtained with this fMRI protocol with individual subjects as the cornerstone of a large-scale hybrid database. Because the individual functional information that can be captured in such a short sequence should be considered with caution, we focus here on the design efficiency and within-subject. We then describe preliminary data obtained from 81 subjects scanned in a 3T scanner and address new methodological issues including statistical methods for analysis and visualization of inter-individual functional variability. Subsequent publications will exploit the potential of this database to focus on characterizing inter-individual variability. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To design a simple and fast fMRI acquisition protocol that captures individual cerebral networks associated with auditory and visual perception, motor actions, reading, language comprehension, and mental calculation.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Multi-functional localizer paradigm\",\n",
      "    \"TaskDescription\": \"The task investigates various cognitive processes including auditory and visual perception, motor actions, reading, language comprehension, and mental calculation.\",\n",
      "    \"DesignDetails\": \"Ten types of trials were mixed together in a fixed order with a fast event-related paradigm. There were 100 trials presented with a stochastic SOA ranging from 2400 ms to 3600 ms, optimized for both statistical detection and hemodynamic response estimation. Trials included visual and auditory stimuli, visual and auditory instructions for button presses, reading and listening to sentences, and solving subtraction problems. Visual stimuli were displayed in 1.3 sec, and auditory stimuli varied from 1.2 to 1.7 sec. Rest periods (black screen) served as null events. The entire sequence was designed to run in less than 5 minutes.\",\n",
      "    \"Conditions\": [\n",
      "        \"Passive viewing of flashing horizontal checkerboards\",\n",
      "        \"Passive viewing of flashing vertical checkerboards\",\n",
      "        \"Pressing left button (visual instructions)\",\n",
      "        \"Pressing right button (visual instructions)\",\n",
      "        \"Pressing left button (auditory instructions)\",\n",
      "        \"Pressing right button (auditory instructions)\",\n",
      "        \"Silent reading of sentences\",\n",
      "        \"Listening to sentences\",\n",
      "        \"Solving visual subtraction problems\",\n",
      "        \"Solving auditory subtraction problems\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"5 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To define a simple fMRI test, less than 5 minutes long, that could delineate, in a subject-specific manner, cerebral circuits associated with various cognitive processes.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Multi-functional localizer\",\n",
      "    \"TaskDescription\": \"A fast event-related fMRI protocol designed to capture individual cerebral correlates of visual, auditory, and sensorimotor processes, reading, language comprehension, and mental calculation.\",\n",
      "    \"DesignDetails\": \"The protocol consisted of ten types of trials presented in a fixed order, which appeared random to the subject. Each trial type included passive viewing of checkerboards, pressing buttons according to visual or auditory instructions, silently reading sentences, and solving subtraction problems. The sequence was optimized for statistical detection and hemodynamic response estimation, with a mean SOA of 3 seconds. A total of 100 trials were performed in a 5-minute session.\",\n",
      "    \"Conditions\": [\n",
      "        \"Passive viewing of horizontal checkerboards\",\n",
      "        \"Passive viewing of vertical checkerboards\",\n",
      "        \"Pressing left button according to visual instructions\",\n",
      "        \"Pressing right button according to visual instructions\",\n",
      "        \"Pressing left button according to auditory instructions\",\n",
      "        \"Pressing right button according to auditory instructions\",\n",
      "        \"Silently reading visual sentences\",\n",
      "        \"Listening to auditory sentences\",\n",
      "        \"Solving visual subtraction problems\",\n",
      "        \"Solving auditory subtraction problems\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"5 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 2686646\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"one-back task\"\n",
      "    ],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Investigate consistency and variability in fROIs in a set of 45 volunteers.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"One-back Task\",\n",
      "    \"TaskDescription\": \"Subjects were instructed to press a button if the stimulus was identical to the preceding stimulus using four categories of visual stimuli: written words, pictures of common objects, scrambled pictures of the same objects, and consonant letter strings.\",\n",
      "    \"DesignDetails\": \"A block design was used with each block consisting of 16 trials from a single category presented every second. A trial began with a 650ms fixation cross, followed by the stimulus for 350ms. In between, subjects viewed a fixation cross for 16s. Each category had 192 stimuli including targets. The order was counterbalanced across subjects over two runs.\",\n",
      "    \"Conditions\": [\n",
      "        \"Written words\",\n",
      "        \"Pictures of common objects\",\n",
      "        \"Scrambled pictures\",\n",
      "        \"Consonant letter strings\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Response time\",\n",
      "        \"Hit rate\",\n",
      "        \"False alarm rate\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"16.4 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To evaluate the consistency associated with functionally localising reading- and object-sensitive areas of left occipito-temporal cortex.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"One-back task\",\n",
      "    \"TaskDescription\": \"A task where subjects press a button if the stimulus is identical to the preceding stimulus, with four categories of visual stimuli.\",\n",
      "    \"DesignDetails\": \"Each block consisted of 16 trials from a single category presented one every second. A trial began with a 650ms fixation cross, followed by the stimulus for 350ms. In between blocks, subjects viewed a fixation cross for 16s. The stimuli were divided equally into two lists, with the order counter-balanced across subjects. In total there were 192 stimuli per category including targets.\",\n",
      "    \"Conditions\": [\n",
      "        \"Written words\",\n",
      "        \"Pictures of common objects\",\n",
      "        \"Scrambled pictures of the same objects\",\n",
      "        \"Consonant letter strings\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Hit rate\",\n",
      "        \"False alarms\",\n",
      "        \"D-prime scores\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"16.4 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 3078751\n",
      "--- delavega-other ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"movement observation paradigm\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"participants had to register auditory cues that determined what movement the dancer was to perform next, and watch the ensuing movements\",\n",
      "        \"participants watched a dancer performing according to cue, but occasionally making mistakes. Previous to playing the task, the participants were instructed on the cue-movement associations that rule the task (low chord: repetition; high chord: switch). They received a short training where they could choose either four or eight example movies that contained up to 19 cued movements, before they started the task. The movie of the dancer was displayed in the middle of an otherwise gray computer screen, using the Software Presentation 12.0 (Neurobehavioral Systems, San Francisco, CA, USA). Visual input did not extend further than 5\\u00b0 of visual angle. The movies were stopped in irregular intervals and participants had to indicate by button press, whether the dancer had performed correctly immediately before video offset. That is, participants had to indicate whether the very last movement had been correct, irrespective of possible earlier errors. Questions were indicated by a question mark (\\u201c?\\u201d) displayed in font size 24 for 1500\\u2009ms or until the first response. Participants had to press the arrow-to-the-left key (index finger) if they judged the last movement to have been according to cue or press the arrow-to-the-right key (middle finger) if they thought the movement had not been according to cue. Responses had to be given within a timeframe of 1500\\u2009ms and were followed by a valid feedback for 400\\u2009ms indicating correct, incorrect or delayed responses (\\u201c+\\u201d/\\u201c\\u2212\\u201d/\\u201c0\\u201d; Figure  ). \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Investigate the role of caudate nucleus in breaches of expectation using fMRI in a movement observation paradigm.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Behavioral Probe Session\",\n",
      "    \"TaskDescription\": \"Participants observed cue-based movement sequences, identifying errors in the dancer's performance as related to auditory cues.\",\n",
      "    \"DesignDetails\": \"In the probe session, participants viewed the dancer's movement sequences on a computer, indicating correctness of the dancer's final movement in a sequence. Question trials occurred at random intervals, requiring classification of the sequence as correct/incorrect based on cue adherence.\",\n",
      "    \"Conditions\": [\n",
      "        \"Correct Movement according to Cue\",\n",
      "        \"Incorrect Movement not according to Cue\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Accuracy\",\n",
      "        \"Response Time\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Movement Observation Paradigm\",\n",
      "    \"TaskDescription\": \"Participants watched movies of a dancer producing sequences according to auditory cues, with some sequences violating the cues to observe activation in the caudate nucleus.\",\n",
      "    \"DesignDetails\": \"During the fMRI session, participants watched movies of a dancer producing movements according to cues (88%) or violating cues (12%). Participants registered auditory cues to predict movements, having trained in a motor task prior to the fMRI session. The task was designed to distinguish caudate activity due to breaches of expectation from other factors like saliency and change.\",\n",
      "    \"Conditions\": [\n",
      "        \"Predicted Movement\",\n",
      "        \"Prediction-Violating Movement\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Caudate Nucleus Activity\",\n",
      "        \"BOLD Signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"not specified\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: The current study investigates whether caudate nucleus signals for breaches of expectation in a movement observation paradigm.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Movement Observation Paradigm\",\n",
      "    \"TaskDescription\": \"Participants watched movies of a dancer producing whole-body movements according to auditory cues, with some movements violating the expected sequence.\",\n",
      "    \"DesignDetails\": \"Participants were trained to produce a sequence of movements according to auditory cues. In the fMRI session, they watched movies of a dancer performing movements either according to the cue (88%) or not (12%). The task included 400 single movements, with 32 movements not according to cue. The design allowed for contrasting predicted with prediction-violating movements to test the violation hypothesis.\",\n",
      "    \"Conditions\": [\n",
      "        \"Cued movements\",\n",
      "        \"Prediction-violating movements\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 3825257\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To explore whether the intrinsic functional connectivity of key hubs of the default mode network (DMN) is predictive of individual differences in reading comprehension and task focus.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Reading Comprehension and Task Focus Assessment\",\n",
      "    \"TaskDescription\": \"Assess individual differences in maintaining focus and comprehension while reading texts.\",\n",
      "    \"DesignDetails\": \"Participants read three engaging non-fiction texts on a computer screen, presented one sentence at a time. Experience sampling probes assessed task focus at random intervals. Comprehension was assessed using open-ended questions after reading each text.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"Task focus rating\",\n",
      "        \"Comprehension score\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate whether variations in the experience of reading across individuals have a basis in the brain's functional architecture, particularly focusing on the anterior medial prefrontal cortex (aMPFC) and posterior cingulate cortex (PCC) as part of the default mode network (DMN).\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Reading Comprehension Assessment\",\n",
      "    \"TaskDescription\": \"Participants read three excerpts from a popular science book and answered open-ended questions to assess their comprehension.\",\n",
      "    \"DesignDetails\": \"Participants read three excerpts from 'A Short History of Everything' on a computer screen, with text presented one sentence at a time. Experience sampling probes were administered at random intervals (57 times for each text) to assess task focus. After each text, comprehension was assessed using open-ended questions. Each text was approximately 1200 words long, and participants took about 15 minutes to read each text and answer the questions.\",\n",
      "    \"Conditions\": null,\n",
      "    \"Taskmetrics\": [\n",
      "        \"Comprehension scores\",\n",
      "        \"Task focus ratings\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 4110030\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Investigation of sex differences and menstrual cycle effects in resting state cognitive control networks using fMRI.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Resting State\",\n",
      "    \"TaskDescription\": \"Resting state fMRI was used to measure functional connectivity in fronto-parietal networks to assess sex differences and menstrual cycle effects.\",\n",
      "    \"DesignDetails\": \"Participants completed three sessions of resting state fMRI over different time intervals. They were instructed to relax and keep their eyes closed during scanning. Data were collected using a 3T GE-Signa MRI scanner with specific parameters for anatomical and functional images.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"functional connectivity\",\n",
      "        \"independent component analysis\"\n",
      "    ],\n",
      "    \"RestingState\": true,\n",
      "    \"RestingStateMetadata\": \"Three sessions of resting state fMRI with eyes closed. Data collection parameters: anatomical T1-weighted image and gradient-echo echo-planar imaging for functional images with whole brain coverage.\",\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate sex differences and menstrual cycle effects in resting state functional connectivity of fronto-parietal cognitive control networks.\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 4115625\n",
      "--- delavega-other ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Voice localizer paradigm\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"The voice localizer includes a variety of different vocal sounds, not exclusively female or male voices, but also speech and non-speech of women, men and infants and non-vocal sounds (e.g., environmental sounds)\",\n",
      "        \"Subjects were instructed to close their eyes and passively listen to a large variety of sounds. Stimuli were presented in a simple block design and divided into vocal (20 blocks) and non-vocal (20 blocks) conditions. Vocal blocks contained only sounds of human vocal origin (excluding sounds without vocal fold vibration such as whistling or whispering) and consisted of speech (e.g., words, syllables, connected speech in different languages) or non-speech (e.g., coughs, laughs, sighs and cries). The vocal stimuli consisted of recordings from 7 babies, 12 adults, 23 children, and 5 elderly people. Half of the vocal sounds (speech and non-speech) consisted of vocalizations from adults and elderly people (women and men) with comparable proportions for both genders (~24% female, ~22% male). The other half of the vocal sounds consisted of infant vocalizations (speech and non-speech) which also included baby crying/laughing. Recorded non-vocal sounds included various environmental sounds (e.g., animal vocalizations, musical instruments, nature and industrial sounds). A total number of 40 blocks were presented. Each block lasted for 8 s with an inter-block interval of 2 s. Stimuli (16bit, mono, 22050 Hz sampling rate) were normalized for RMS and are available at   (Belin et al.,  ). \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: The aim of the study was to localize voice-related areas in both female and male listeners and to investigate whether brain maps may differ depending on the gender of the listener.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Functional Voice Localizer\",\n",
      "    \"TaskDescription\": \"The task aims to robustly estimate temporal voice areas (TVAs) by contrasting stimulation with vocal versus non-vocal sounds in both female and male listeners.\",\n",
      "    \"DesignDetails\": \"Subjects listened passively to sounds in a simple block design, divided into vocal (20 blocks) and non-vocal (20 blocks) conditions. Each block lasted for 8 seconds with a 2-second inter-block interval. Vocal blocks consisted of human vocal sounds, while non-vocal blocks included environmental sounds. A total of 40 blocks were presented.\",\n",
      "    \"Conditions\": [\n",
      "        \"Vocal\",\n",
      "        \"Non-Vocal\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"classification accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To localize voice-related areas in both female and male listeners and to investigate whether brain maps may differ depending on the gender of the listener.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Voice localizer paradigm\",\n",
      "    \"TaskDescription\": \"Subjects were instructed to close their eyes and passively listen to a large variety of sounds, including vocal and non-vocal stimuli.\",\n",
      "    \"DesignDetails\": \"The voice localizer included a simple block design with 40 blocks, divided into 20 vocal and 20 non-vocal conditions. Each block lasted for 8 seconds with an inter-block interval of 2 seconds. Vocal blocks contained sounds of human vocal origin, including speech and non-speech sounds from various age groups. Non-vocal sounds included environmental sounds.\",\n",
      "    \"Conditions\": [\n",
      "        \"Vocal\",\n",
      "        \"Non-vocal\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"Classification accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"10.28 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 4179768\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"single food choice task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"During the functional MRI scan, participants performed a food choice task (Figure  ). In this ask, participants made 100 choices. In every trial, they viewed one of the study stimuli (3000 ms, choice period) and subsequently had to indicate with a button press (1500 ms, button press period) whether they wanted to eat a portion of the snack or not. During the button press period the words \\u201cyes\\u201d and \\u201cno\\u201d were shown left/right (randomized) on the screen. After indicating their choice, a yellow box appeared around the yes or no. Participants were instructed to make their choice already during the period that the image was shown. To ensure that their choices were actually made in direct response to the food pictures, the button press period was so short that it only allowed them to locate whether they had to push the left or right button. The choice trials were interspersed with a random interval (2000 and 5000 ms). At the beginning, halfway (after 50 trials) and at the end an additional baseline period of 30,000 ms was included in the task. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate brain regions activated during food choices concerning high energy (HE) versus low energy (LE) snacks in weight-concerned women, and assess how these activations vary with tastiness and individual differences in self-regulatory success.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Single food choice task\",\n",
      "    \"TaskDescription\": \"Participants indicated whether they wanted to eat presented snack foods, specifically HE or LE snacks, while being scanned.\",\n",
      "    \"DesignDetails\": \"Participants performed 100 trials, each involving viewing a snack image for 3000 ms, followed by a 1500 ms button press period to indicate if they wanted to eat the snack. Choices were made during the imaging display period, with decisions confirmed by pressing a button during a short 1500 ms period. Random inter-trial intervals ranged from 2000 to 5000 ms. The task began, had a midpoint, and ended with additional 30,000 ms baseline periods to rest.\",\n",
      "    \"Conditions\": [\n",
      "        \"HE snack choice\",\n",
      "        \"LE snack choice\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Not explicitly stated\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate which brain regions are activated during food choices between high-energy (HE) and low-energy (LE) snacks in weight-concerned women, and how activation varies with the food's tastiness and individual differences in self-regulatory success.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Food Choice Task\",\n",
      "    \"TaskDescription\": \"Participants made choices about whether to eat high-energy or low-energy snacks while their brains were scanned using fMRI.\",\n",
      "    \"DesignDetails\": \"The task consisted of 100 trials where participants viewed one snack for 3000 ms and then had 1500 ms to indicate their choice with a button press. The trials were interspersed with random intervals of 2000 to 5000 ms, and baseline periods were included at the beginning, halfway, and end of the task.\",\n",
      "    \"Conditions\": [\n",
      "        \"High-Energy (HE) Snacks\",\n",
      "        \"Low-Energy (LE) Snacks\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Total duration of the task not specified.\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 4374765\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"fear inducing paradigm\"\n",
      "    ],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate the relationship between GABA concentration and fear-related BOLD responses in the insula region.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Fear provoking paradigm\",\n",
      "    \"TaskDescription\": \"Participants were shown images to provoke fear, including spiders, other animals, general negative, and neutral images from the IAPS.\",\n",
      "    \"DesignDetails\": \"The task involved presenting images in short blocks of 10 seconds, with 4 images each presented for 2.5 seconds. Blocks were followed by a fixation cross lasting between 7 to 13 seconds. Participants performed a covert task to detect human presence in 50% of the images via button press. Altogether, 10 blocks were presented for each condition.\",\n",
      "    \"Conditions\": [\n",
      "        \"SPIDERS > ANIMALS\",\n",
      "        \"IAPSnegative > IAPSneutral\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD response\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate the relationship between GABA concentration and fear-related BOLD responses in the insula.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Fear Provocation Paradigm\",\n",
      "    \"TaskDescription\": \"The task involved presenting still pictures of spiders and other control animals to elicit fear-related BOLD responses.\",\n",
      "    \"DesignDetails\": \"The images were presented in short blocks of 10 seconds, with 4 images (presented for 2.5 sec.) each. After half of the blocks, a fixation cross appeared for either 7, 9, 11 or 13 seconds. For each condition, 10 blocks were presented.\",\n",
      "    \"Conditions\": [\n",
      "        \"SPIDERS\",\n",
      "        \"ANIMALS\",\n",
      "        \"IAPSnegative\",\n",
      "        \"IAPSneutral\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Approximately 10 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 4386762\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate the relationship between ALFF and global FC with a voxel-based approach in the healthy and diseased brain, specifically in degenerative dementia.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Resting-state fMRI\",\n",
      "    \"TaskDescription\": \"Resting-state functional magnetic resonance imaging (rs-fMRI) to investigate brain function in the absence of a specific task.\",\n",
      "    \"DesignDetails\": \"Resting state fMRI data were acquired with subjects instructed to keep their eyes closed, not think of anything specific, and refrain from falling asleep. The data acquisition used an echo planar imaging sequence with parameters: TR = 2080 ms, TE = 30 ms, 32 slices, matrix = 64 x 64, in-plane resolution = 3x3 mm, slice thickness = 2.5 mm, flip angle = 70. Resting scans lasted 7 minutes and 20 seconds for a total of 220 volumes.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"Functional connectivity (FC)\",\n",
      "        \"Amplitude of Low-Frequency Fluctuations (ALFF)\"\n",
      "    ],\n",
      "    \"RestingState\": true,\n",
      "    \"RestingStateMetadata\": \"Subjects were instructed to keep their eyes closed, not think of anything specific, and refrain from falling asleep. Duration was 7 minutes and 20 seconds with 220 volumes collected.\",\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"440 seconds\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate the relationship between ALFF and global FC with a voxel-based approach in the healthy and diseased brain, particularly in patients with Alzheimer's Disease (AD) and Mild Cognitive Impairment (MCI).\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 4398371\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Soccer Paradigm\",\n",
      "        \"monetary incentive paradigm\"\n",
      "    ],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Investigate reward processing upon scoring a soccer goal in a two-versus-one situation and in comparison to a monetary incentive task.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Soccer Paradigm\",\n",
      "    \"TaskDescription\": \"Participants decide to either pass or shoot the ball in a two-versus-one soccer situation. Feedback is given as either scoring a goal ('GOAL!') or missing ('MISS!').\",\n",
      "    \"DesignDetails\": \"The fMRI paradigm involved 200 different 2v1 soccer situations. Participants decided to pass or shoot the ball. Each situation was shown twice with feedback preset as positive (GOAL!) or negative (MISS!). The interstimulus and intertrial intervals were 3000-6000 ms. Feedbacks were preset based on scoring probabilities from pre-testing. The task included 120 trials.\",\n",
      "    \"Conditions\": [\n",
      "        \"pass\",\n",
      "        \"shoot\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"brain activation\",\n",
      "        \"reward probability\",\n",
      "        \"reward reception\",\n",
      "        \"reward prediction error\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Monetary Paradigm\",\n",
      "    \"TaskDescription\": \"Participants guessed under which out of four boxes a circle was hidden, leading to winning or losing 10 euro cents based on their guess.\",\n",
      "    \"DesignDetails\": \"In each of the 150 trials, participants guessed the position of a circle hidden under one of four boxes. Winning probabilities ranged from 25% to 100%. A correct guess led to winning 10 euro cents; an incorrect guess to no win or a monetary loss of 10 euro cents. The interstimulus and intertrial intervals were randomized between 1500-4500 ms.\",\n",
      "    \"Conditions\": [\n",
      "        \"win\",\n",
      "        \"no win\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"brain activation\",\n",
      "        \"reward probability\",\n",
      "        \"reward reception\",\n",
      "        \"reward prediction error\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate reward processing upon scoring a soccer goal in a standard two-versus-one situation and in comparison to winning in a monetary incentive task.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Monetary Paradigm\",\n",
      "    \"TaskDescription\": \"Participants guessed under which of four boxes a circle was hidden, with varying probabilities of winning.\",\n",
      "    \"DesignDetails\": \"In each of the 150 trials, a correct guess led to a positive monetary feedback (win) of 10 euro cents, while a wrong guess led to no monetary win, and no guess resulted in a monetary loss of 10 euro cents. The ISI and ITI were randomized between 1500-4500 ms using Python 2.7.\",\n",
      "    \"Conditions\": [\n",
      "        \"Correct Guess\",\n",
      "        \"Wrong Guess\",\n",
      "        \"No Guess\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Monetary feedback\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Soccer Paradigm\",\n",
      "    \"TaskDescription\": \"Participants decide to either pass or shoot the ball in a soccer scenario, with feedback provided based on their choices.\",\n",
      "    \"DesignDetails\": \"The fMRI paradigm was programmed using in-house software. Each situation was randomly shown twice, with half of the feedbacks preset to be positive (GOAL!) and the other half negative (MISS!). The task consisted of 120 trials, with an interstimulus interval (ISI) and intertrial interval (ITI) of 3000-6000 ms, randomized using Python 2.7.\",\n",
      "    \"Conditions\": [\n",
      "        \"Pass\",\n",
      "        \"Shoot\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"max. 34 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 4440210\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"emotional matching task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"Subjects were shown triplets of geometric shapes (as neutral stimuli) and of threatening scenes as well as fearful faces (as emotional conditions) presented in alternating blocks of neutral and emotional stimuli.\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: None\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Matching Task\",\n",
      "    \"TaskDescription\": \"The task is designed to activate the amygdala by showing subjects triplets of geometric shapes (as neutral stimuli) and of threatening scenes as well as fearful faces (as emotional conditions) presented in alternating blocks of neutral and emotional stimuli.\",\n",
      "    \"DesignDetails\": \"The task involved presenting subjects with triplets of geometric shapes (as neutral stimuli) and threatening scenes as well as fearful faces (as emotional conditions). These were presented in alternating blocks of neutral and emotional stimuli. The Emotional Matching Task employed during the fMRI scanning was used to produce activations in the amygdala region.\",\n",
      "    \"Conditions\": [\n",
      "        \"Neutral stimuli\",\n",
      "        \"Emotional stimuli (threatening scenes, fearful faces)\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate activations in the amygdala region to the presentation of emotional faces and to understand the origins of signal fluctuations and their heterogeneity in this region.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Emotional Matching Task\",\n",
      "    \"TaskDescription\": \"Subjects were shown triplets of geometric shapes (neutral stimuli) and of threatening scenes as well as fearful faces (emotional conditions) presented in alternating blocks of neutral and emotional stimuli.\",\n",
      "    \"DesignDetails\": \"The task involved presenting triplets of geometric shapes and emotional faces in alternating blocks. The design included both neutral and emotional stimuli, with a total of 1420 volumes acquired during the task. The task was designed to activate the amygdala, and the emotional stimuli were presented in blocks to assess the brain's response to emotional cues.\",\n",
      "    \"Conditions\": [\n",
      "        \"Neutral Faces\",\n",
      "        \"Fearful Faces\",\n",
      "        \"Threatening Scenes\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Not specified\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 4517759\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Typing task\",\n",
      "        \"Reading task\",\n",
      "        \"Typing-movement task \",\n",
      "        \"Writing task\",\n",
      "        \"Reading task\",\n",
      "        \"Writing-movement task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"In the typing task, a word was presented on a screen and the subject was instructed to type the visually presented word on a keyboard (Fujitsu CP218230-02) that had been modified for use in the scanner by eliminating all ferro-magnetic components. The word list for the task consisted of 110 kana nouns obtained from the Nippon Telegraph and Telephone corporation (NTT) psycholinguistic database called \\\"Lexical Properties of Japanese\\\" [ ]. Words were matched on linguistic parameters including word frequency, imagability, number of syllables, and key positions on the QWERTY keyboard so as not to require unequal hand usage. All words consisted of four or five kana characters (seven to nine alphabetic characters) and did not include the Roman characters q, w, z, x, c, v, p, or l, because there are relatively few opportunities to press these keys in the Japanese language and these characters are located in the corners of the QWERTY keyboard ( ). No word was shown more than once in the experiment (i.e., the six scanning sessions containing the writing, reading, and the typing conditions). \\n   Positions of the keys required to type the words included in the word list.  \\nThe typing task required the subjects to type the letters corresponding to the red colored keys. The keys q, w, z, x, c, v, p, and l were excluded because there are few opportunities to press these keys for Japanese typists and these keys are located in the corners of the QWERTY keyboard. \\n  \\nBefore the typing task, a picture of a hand, which indicated that the subject should type, was presented for 2000 ms followed by six different words presented sequentially over a period of 18000 ms with no interval and subjects were instructed to type each word on the keyboard ( ). We adopted the most popular way of typing Japanese called \\u201calphabetical input\\u201d. In this method, a western-style keyboard is used, and the kana character \\u201cka\\u201d is produced by typing \\u201ck\\u201d followed by \\u201ca\\u201d [ ]. Although there are other ways of typing, all subjects were instructed to type using the alphabetical input method. \\n\\nThe keys pressed were recorded through the optical fibers placed in the keyboard buttons and the accuracy of typewriting was calculated offline. The percentage of left-hand and right-hand usage on the keyboard was calculated for each word to ensure that there was no bias in hand use. Behavioral data obtained from the typing experiment were processed in MATLAB 7.7.0 (MathWorks Inc., Natick, MA, USA). Accurate trials were defined as those for which subjects produced the correct key-pressing sequence for a given word. We were primarily interested in spelling errors as opposed to typing errors such as a slip in a keypress to the adjacent key, and we therefore did not consider key presses that were either one to the right or left of the correct key to be incorrect (e.g. for the letter \\u201cy\\u201d, \\u201ct\\u201d or \\u201cu\\u201d were considered correct). Typing speed (number of keys per second) was calculated offline to check the difference between consecutive key presses for each trial involving a motor control task. \\n\",\n",
      "        \"For the reading task, a picture of a book, which indicated that the subject should read, was presented for 2000 ms followed by six different words presented sequentially over a period of 18000 ms with no interval, and subjects were instructed to read each word silently and not to move their fingers ( ). The word list for the reading task was the same as that used in the writing task, and no word was shown more than once in the experiment. \",\n",
      "        \"In the typing-movement task, which was designed as a control, a double circle (\\u25ce) was presented for 2000-ms periods with a blank screen presented for 1000 ms between each period, and subjects were instructed to type randomly with both hands at a pre-learned speed during the presentation of the double circle ( ). Behavioral data were obtained in the same manner as for the typing task. \",\n",
      "        \"In the writing task, subjects were instructed to write words with their right index finger in the air. It is natural for Japanese people to write with the index finger in the air, and they often move their index finger to trace the imagined letter. Before the task, a picture of a hand, which indicated that the subject should write, was presented for 2000 ms followed by six different words presented sequentially over a period of 18000 ms with no interval and subjects were instructed to trace each word in the air with their index finger in Japanese phonograms (kana) ( ). The word list for this task was the same as that used in the typing condition. During the fMRI scan, finger movements were checked by examiners via a video monitor. \",\n",
      "        \"For the reading task, a picture of a book, which indicated that the subject should read, was presented for 2000 ms followed by six different words presented sequentially over a period of 18000 ms with no interval, and subjects were instructed to read each word silently ( ). The word list for the reading task was the same as that used in the typing condition. \",\n",
      "        \"In the writing-movement task, a double circle (\\u25ce) was presented and subjects were instructed to move their index finger randomly at a pre-learned speed, in the way they do when they write in the air. The double circle was presented for 2500-ms periods with a blank screen presented for 500 ms between each period ( ). The stimulus duration in the writing-movement task was longer than that in the typing-movement task to control for the difference in the time taken to complete these two tasks. During the fMRI scan, finger movements were checked by examiners via a video monitor. \\n\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate the neural substrate of typewriting Japanese words and detect the difference between the neural substrate of typewriting and handwriting.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Typing Task\",\n",
      "    \"TaskDescription\": \"Participants type visually presented Japanese words on a modified keyboard compatible with MRI scanning.\",\n",
      "    \"DesignDetails\": \"In the typing task, a word was presented on a screen and the subject was instructed to type the visually presented word on a keyboard (Fujitsu CP218230-02) that had been modified for use in the scanner by eliminating all ferro-magnetic components. The word list for the task consisted of 110 kana nouns obtained from the Nippon Telegraph and Telephone corporation (NTT) psycholinguistic database called \\\"Lexical Properties of Japanese\\\". Words were matched on linguistic parameters including word frequency, imagability, number of syllables, and key positions on the QWERTY keyboard so as not to require unequal hand usage. All words consisted of four or five kana characters (seven to nine alphabetic characters) and did not include the Roman characters q, w, z, x, c, v, p, or l, because there are relatively few opportunities to press these keys in the Japanese language and these characters are located in the corners of the QWERTY keyboard. Before the typing task, a picture of a hand, which indicated that the subject should type, was presented for 2000 ms followed by six different words presented sequentially over a period of 18000 ms with no interval and subjects were instructed to type each word on the keyboard.\",\n",
      "    \"Conditions\": [\n",
      "        \"Typing Task\",\n",
      "        \"Reading Task\",\n",
      "        \"Typing-Movement Task\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Accuracy\",\n",
      "        \"Response time\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Not explicitly stated\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate the neural substrate of typewriting Japanese words and to detect the difference between the neural substrate of typewriting and handwriting.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Typing Task\",\n",
      "    \"TaskDescription\": \"Subjects were instructed to type visually presented Japanese words on a keyboard.\",\n",
      "    \"DesignDetails\": \"The typing experiment used an fMRI block design. Each session consisted of three different tasks presented pseudo-randomly in five blocks of 20-s duration. All tasks had a fixation condition. In the typing-movement task, subjects were instructed to type randomly with both hands when the double circle symbol was present on the monitor.\",\n",
      "    \"Conditions\": [\n",
      "        \"Typing\",\n",
      "        \"Reading\",\n",
      "        \"Typing-Movement\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Accuracy\",\n",
      "        \"Typing Speed\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"20 seconds\"\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Writing Task\",\n",
      "    \"TaskDescription\": \"Subjects were instructed to write words with their right index finger in the air.\",\n",
      "    \"DesignDetails\": \"The writing experiment used an fMRI block design that was almost the same as the typing experiment. Each session consisted of three different tasks presented pseudo-randomly in five blocks of 20-s duration. In the writing-movement task, subjects were instructed to move their right index finger randomly when the double circle symbol was present on the monitor.\",\n",
      "    \"Conditions\": [\n",
      "        \"Writing\",\n",
      "        \"Reading\",\n",
      "        \"Writing-Movement\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"20 seconds\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 4526228\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: The study examines the relationship between physical activity and cardiorespiratory fitness (CRF) with brain functional integrity in older adults, using spontaneous moment-to-moment variability in the BOLD signal as an index of functional brain health.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Resting State\",\n",
      "    \"TaskDescription\": \"Participants were asked to lie still with eyes closed, and spontaneous brain activity was measured.\",\n",
      "    \"DesignDetails\": \"Resting state fMRI data were collected for 6 minutes using a fast echo-planar imaging (EPI) sequence with Blood Oxygenation Level Dependent (BOLD) contrast. Participants lay still with eyes closed. The imaging parameters included TR = 2s, TE = 25ms, flip angle = 80 degrees, 3.4 x 3.4 mm in-plane resolution, and 35 4mm-thick slices acquired in ascending order.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal variability (SD)\"\n",
      "    ],\n",
      "    \"RestingState\": true,\n",
      "    \"RestingStateMetadata\": {\n",
      "        \"duration\": \"6 minutes\",\n",
      "        \"instructions\": \"lie still with eyes closed\"\n",
      "    },\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"6 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To determine how the level of physical fitness (measured as CRF) and physical activity (measured via accelerometer) are related to functional brain health measured as moment-to-moment variability in the BOLD signal (SD) in healthy older adults.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Resting State fMRI\",\n",
      "    \"TaskDescription\": \"Participants underwent resting state fMRI to measure spontaneous brain activity.\",\n",
      "    \"DesignDetails\": \"Resting state fMRI data were collected during a single session on a 3T Siemens Tim Trio system. T2*-weighted resting state images were acquired with a fast echo-planar imaging (EPI) sequence with Blood Oxygenation Level Dependent (BOLD) contrast for 6 minutes, with a TR of 2 seconds and TE of 25 ms. Participants were instructed to lie still with their eyes closed during the scan.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal variability (SD)\"\n",
      "    ],\n",
      "    \"RestingState\": true,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"6 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 4530666\n",
      "--- delavega-other ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"implicit emotion processing task \",\n",
      "        \"explicit emotion identification task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"In the implicit task, participants were presented with each stimulus twice in a pseudorandom fashion, resulting in a total of 16 presentations of each emotion over two runs.\",\n",
      "        \"In the explicit task, participants were shown each stimulus once, resulting in eight presentations per emotion over one run.\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: to investigate facial affect processing and the processing of one's own face through measures of emotion identification, functional magnetic resonance imaging (fMRI) and eyetracking\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Explicit Emotion Identification Task\",\n",
      "    \"TaskDescription\": \"Participants were asked to identify emotions from face stimuli, correlating fMRI and eyetracking data with behavioral data.\",\n",
      "    \"DesignDetails\": \"Faces were displayed for 8000 ms on a white background. Following the presentation of each face stimulus, a forced-choice screen appeared asking participants to identify the emotion displayed from a list of emotions. Participants were given as much time as needed to respond.\",\n",
      "    \"Conditions\": [\n",
      "        \"Anger\",\n",
      "        \"Disgust\",\n",
      "        \"Fear\",\n",
      "        \"Happiness\",\n",
      "        \"Sadness\",\n",
      "        \"Neutral\",\n",
      "        \"Own Face\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Response accuracy\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Implicit Emotion Processing Task\",\n",
      "    \"TaskDescription\": \"Participants performed an implicit emotion processing task that involved gender identification of stimuli while undergoing fMRI and eyetracking.\",\n",
      "    \"DesignDetails\": \"Participants were presented with face stimuli from a standard set of black-and-white images, displaying seven emotions: anger, disgust, fear, happiness, sadness, surprise, and neutral. The stimuli were presented pseudorandomly across emotions for 8000 ms on a white background, followed by a black fixation cross for 3000-4300 ms. Participants were instructed to make a gender response with their right hand by clicking one of two buttons only when the fixation cross appeared.\",\n",
      "    \"Conditions\": [\n",
      "        \"Anger\",\n",
      "        \"Disgust\",\n",
      "        \"Fear\",\n",
      "        \"Happiness\",\n",
      "        \"Sadness\",\n",
      "        \"Neutral\",\n",
      "        \"Own Face\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"Response accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate facial affect processing and the processing of one's own face in individuals with anorexia nervosa (AN) through measures of emotion identification, functional magnetic resonance imaging (fMRI), and eyetracking.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Implicit Emotion Processing Task\",\n",
      "    \"TaskDescription\": \"Participants were presented with face stimuli to identify gender while undergoing fMRI and eyetracking.\",\n",
      "    \"DesignDetails\": \"Participants were presented with each stimulus twice in a pseudorandom fashion, resulting in a total of 16 presentations of each emotion over two runs. The implicit task involved displaying faces for 8000 ms followed by a fixation cross for 3000-4300 ms. Each of the two runs began with a long period of fixation for 15000 ms.\",\n",
      "    \"Conditions\": [\n",
      "        \"Anger\",\n",
      "        \"Disgust\",\n",
      "        \"Fear\",\n",
      "        \"Happiness\",\n",
      "        \"Sadness\",\n",
      "        \"Surprise\",\n",
      "        \"Neutral\",\n",
      "        \"Own Face\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"Fixation count\",\n",
      "        \"Fixation duration\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Not specified\"\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Explicit Emotion Identification Task\",\n",
      "    \"TaskDescription\": \"Participants identified the emotion displayed in the face stimuli after viewing them.\",\n",
      "    \"DesignDetails\": \"In the explicit task, participants were shown each stimulus once, resulting in eight presentations per emotion over one run. Faces were displayed for 8000 ms on a white background, followed by a forced-choice screen asking participants to identify the emotion from a list.\",\n",
      "    \"Conditions\": [\n",
      "        \"Anger\",\n",
      "        \"Disgust\",\n",
      "        \"Fear\",\n",
      "        \"Happiness\",\n",
      "        \"Sadness\",\n",
      "        \"Surprise\",\n",
      "        \"Neutral\",\n",
      "        \"Own Face\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Emotion identification accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Not specified\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 5090046\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To examine the diagnostic value of measuring functional connectivity at rest in understanding individual differences in semantic cognition.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Verbal Fluency Task\",\n",
      "    \"TaskDescription\": \"Participants had 1 minute to generate as many unique words as possible belonging to a semantic category or starting with a specific letter.\",\n",
      "    \"DesignDetails\": \"Semantic fluency was assessed for eight categories split in two blocks (e.g., animals, fruits). Letter fluency was assessed for three letter cues (e.g., A, F, S). Block order was counterbalanced and responses were recorded.\",\n",
      "    \"Conditions\": [\n",
      "        \"Category Fluency\",\n",
      "        \"Letter Fluency\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Number of correct words\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Synonyms Task\",\n",
      "    \"TaskDescription\": \"Participants selected the word closest in meaning to a probe word from three options.\",\n",
      "    \"DesignDetails\": \"The Synonyms Task comprised 96 trials split into six conditions according to lexical frequency (high and low) and imageability (high, medium and low). A probe word was displayed with three choices, responses were collected using a numeric keyboard.\",\n",
      "    \"Conditions\": [\n",
      "        \"High Frequency\",\n",
      "        \"Low Frequency\",\n",
      "        \"High Imageability\",\n",
      "        \"Low Imageability\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Z-scored reaction times\",\n",
      "        \"Z-scored accuracy\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Resting State Task\",\n",
      "    \"TaskDescription\": \"Participants underwent resting state functional magnetic resonance imaging (fMRI) to explore the neurocognitive architecture of semantic cognition.\",\n",
      "    \"DesignDetails\": \"Participants underwent a 9-minute eyes-open resting state functional scan using a gradient single-shot echo planar imaging (EPI) sequence with repetition time (TR) 3000ms, echo time (TE) minimum full, 180 volumes, flip angle 90, voxel size 3x3x3 mm, matrix size 64x64, field of view (FOV) 192x192 mm, slice thickness 3mm and 60 slices with an interleaved (bottom up) acquisition order.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": null,\n",
      "    \"RestingState\": true,\n",
      "    \"RestingStateMetadata\": \"9-minute eyes-open resting state functional scan\",\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"9 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: This study explored the neurocognitive architecture that supports semantic cognition by examining how individual variation in functional brain organisation predicts comprehension and semantic generation.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Synonym Judgement Task\",\n",
      "    \"TaskDescription\": \"Participants were asked to select the word closest in meaning to a probe word from a set of three options.\",\n",
      "    \"DesignDetails\": \"The task comprised 96 trials split into six conditions according to lexical frequency (high and low) and imageability (high, medium and low). Each trial started with a fixation cross for 1s, followed by a trial which remained on screen until the participant responded. A probe word was presented at the top of the screen with the target and two unrelated distracters on the bottom row. Responses were collected using the numeric keyboard.\",\n",
      "    \"Conditions\": [\n",
      "        \"High Frequency\",\n",
      "        \"Low Frequency\",\n",
      "        \"High Imageability\",\n",
      "        \"Medium Imageability\",\n",
      "        \"Low Imageability\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Accuracy\",\n",
      "        \"Reaction Time\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Verbal Fluency Task\",\n",
      "    \"TaskDescription\": \"Participants had 1 minute to generate as many unique words as possible belonging to a semantic category or starting with a specific letter.\",\n",
      "    \"DesignDetails\": \"Semantic fluency was assessed for eight categories split in two blocks (Block A: animals, fruits, birds, type of dogs; Block B: vehicles, tools, household objects, boats). Letter fluency was assessed for three letter cues (Block C: A, F, S). Block order was counterbalanced across participants and the order of cues within each block was randomized. Participants' verbal responses were collected and transcribed and scored off-line.\",\n",
      "    \"Conditions\": [\n",
      "        \"Category Fluency\",\n",
      "        \"Letter Fluency\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Number of Correct Words Generated\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Resting State fMRI\",\n",
      "    \"TaskDescription\": \"Participants underwent resting state functional magnetic resonance imaging (fMRI) to assess functional connectivity at rest.\",\n",
      "    \"DesignDetails\": \"The imaging session started with a 9min eyes-open resting state functional scan using a gradient single-shot echo planar imaging (EPI) sequence with repetition time (TR) 3000ms, echo time (TE) minimum full, 180 volumes, flip angle 90, voxel size 3x3x3mm, matrix size 64x64, field of view (FOV) 192x192mm, slice thickness 3mm and 60 slices with an interleaved (bottom up) acquisition order.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": null,\n",
      "    \"RestingState\": true,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"9 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 5324609\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"acquired equivalence task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"participants learned a set of visual discriminations via trial\\u2010and\\u2010error (learning phase), and were subsequently tested on their ability to generalize what they had learned (generalization phase). Both learning and generalization occurred within a single scanning session and took place in the context of a first\\u2010person virtual reality environment (see Fig.  ). On each trial, a scene was presented depicting two buildings positioned equidistantly from a start location. One building concealed a pile of gold (the \\u201creward\\u201d). The location of this gold was determined by the configuration of wall textures rendered onto the towers of each building. Participants were required to select the rewarded building (within 3 s). Following a response, video feedback was presented (7 s) before an inter\\u2010trial interval (1 s). \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Investigate the hippocampal role in gradually learning a set of spatial discriminations and subsequently generalizing them in an acquired equivalence task.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Visual Discrimination and Generalization Task\",\n",
      "    \"TaskDescription\": \"Participants learned a set of visual discriminations via trial-and-error and were subsequently tested on their ability to generalize what they had learned within a virtual reality environment.\",\n",
      "    \"DesignDetails\": \"During scanning, participants learned a set of visual discriminations. Each trial presented a scene with two buildings, one of which concealed a pile of gold (reward), determined by building's wall textures. Participants selected the rewarded building within 3s, followed by 7s video feedback, then a 1s inter-trial interval. Three discriminations were learned, with 48 trials per discrimination presented in a pseudorandom order.\",\n",
      "    \"Conditions\": [\n",
      "        \"Trained discrimination\",\n",
      "        \"Generalization test between equivalence classes\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate the hippocampal role in gradually learning a set of spatial discriminations and subsequently generalizing them in an acquired equivalence task.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Learning and Generalization Task\",\n",
      "    \"TaskDescription\": \"Participants learned a set of visual discriminations via trial-and-error and were subsequently tested on their ability to generalize what they had learned.\",\n",
      "    \"DesignDetails\": \"During scanning, participants engaged in a learning phase where they learned three visual discriminations, each involving two buildings with different wall textures. Each trial lasted 3 seconds for the decision, followed by 7 seconds of feedback and a 1-second inter-trial interval. There were 48 trials per discrimination presented in a pseudorandom order. The generalization phase involved 12 test trials where participants made novel discriminations based on equivalence class membership.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"response accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 5404760\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"revised Social Norm Processing Task (SNPT-R)\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"the SNPT-R contains stories describing neutral social situations, stories on unintentional social norm violations, and stories depicting intentional social norm violations. However, in contrast to earlier versions of the SNPT [ \\u2013 ], the SNPT-R uses only personal stories in order to maximize personal involvement of the participants while reading the stories \",\n",
      "        \"During the story-reading phase (1), participants read stories consisting of a stem sentence and an ending sentence, describing either a neutral social situation, a situation in which a social norm was unintentionally transgressed or situation in which a social norm was violated intentionally. Participants were instructed to imagine themselves in the situation described. In the rating phase (2), participants rated all stories on embarrassment and inappropriateness. \\n  \\nIn the story-reading phase, participants read short stories written in second person. Each story consisted of two sentences, a stem sentence (duration: 3 s) and an ending sentence (duration: 6 s). The stories described either a situation in which no social norm was transgressed (\\u201cneutral condition\\u201d), a situation in which a social norm was unintentionally transgressed (\\u201cunintentional condition\\u201d) or a situation in which a social norm was intentionally transgressed (\\u201cintentional condition\\u201d). It is important to note that the unintentional (\\u201cYou are baking with friends. You use salt instead of sugar without realizing.\\u201d) and intentional (\\u201cYou are baking with friends. You use salt instead of sugar as a joke.\\u201d) condition differ only in the intention of the actor, while we aimed to keep the actual outcome of the action (for example, a distasteful cake) in general the same (although the outcome of some intentional stories could be considered to be more severe in comparison to the outcome of the matching unintentional story, inherent to the verb used to describe intentionality; we refer the reader to   for a sensitivity analysis). \\n\\nThe stories in the SNPT-R were developed in collaboration with Karina S. Blair, author of previous work on the SNPT [ ]. All stories described everyday social situations, in which the protagonist was accompanied by at least one other person, and the stories outlined relative innocuous violations of conventional social norms, in which no severe harm was done to others. The stories were heterogeneous with respect to the context (for example, in the presence of one friend or in public space like an airport) and the nature of the social norm transgression (for example, breaking decency rules versus hurting somebody), in order to increase the external validity of the paradigm. Stories were developed to be suitable for a broad audience and age-range (for children from age 8). However, given the fact that the stories of the SNPT-R were all personal (written in \\u2018you\\u2019 form) in order to maximize personal involvement of participants, some small changes were necessary in stories describing age- or genderspecific elements. Therefore, four age- and gender specific versions of the task were developed: for boys < 18 years of age (version 1), girls < 18 years of age (version 2), men \\u2265 18 years of age (version 3) and women \\u2265 18 years of age (version 4). For example, the school environment (< 18 years) was replaced for a work environment (\\u2265 18 years of age), and \\u2018bikini bottoms\\u2019 (females) for \\u2018swimming trunks\\u2019 (males). However, these changes were only minimal (see   and osf.io/m8r76 for a full list of stories included in the SNPT-R [ ]). \\n\\nIn line with the SNPT described by Blair et al. [ ], twenty-six stem sentences were developed, with three different types of ending. Therefore, the SNPT-R consisted of 78 short stories in total. These stories were presented in a pseudo-random order using E-Prime software (version 2.0.10, Psychology Software Tools; script available at osf.io/m8r76 [ ]), separated by a fixation cross (jittered duration between 2\\u20137 s, determined using Optseq software ( ), mean duration fixation: 3.5 s) and divided into two consecutive blocks of 39 stories (duration each block: 8 min 44 s). Participants were instructed to imagine themselves in the social situations described and to press a button with their right index finger after reading the stem sentence of each story. A button press within 3 s resulted in visual feedback to the participant (a green checkmark presented beneath the sentence). This element was added to the paradigm in order to be able to check whether participants engaged with the task. Prior to the start of the experiment, all participants were familiarized with the story-reading phase by performing a short version of the task (using 5 stories). \\n\\nIn the (unannounced) rating phase of the task ( ), participants read all stories again and were asked to rate them on a 5-point Likert scale on embarrassment (ranging from 1, not embarrassing at all, to 5, extremely embarrassing) and inappropriateness (ranging from 1, not inappropriate at all, to 5, extremely inappropriate), similar as in the SNPT described by Blair and colleagues [ ]. These tasks were also presented using E-Prime software (version 2.0.10, Psychology Software Tools; scripts available at osf.io/m8r76 [ ]). \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate behavioral and neural correlates of processing social norm violations using the revised Social Norm Processing Task (SNPT-R).\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Social Norm Processing Task - Revised (SNPT-R)\",\n",
      "    \"TaskDescription\": \"The task involves reading stories that describe situations with intentional, unintentional, and neutral social norm violations, followed by rating them based on inappropriateness and embarrassment.\",\n",
      "    \"DesignDetails\": \"The task consists of two phases: a story-reading phase where participants read short stories (each with a stem and an ending sentence) in neutral, unintentional, or intentional norm violation conditions, and a rating phase where stories are rated for embarrassment and inappropriateness. Stories are presented in a pseudo-random order, and participants give ratings on a Likert scale. The fMRI component involves scanning during the story-reading phase only.\",\n",
      "    \"Conditions\": [\n",
      "        \"Neutral\",\n",
      "        \"Unintentional Norm Violation\",\n",
      "        \"Intentional Norm Violation\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"Ratings of embarrassment\",\n",
      "        \"Ratings of inappropriateness\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"16 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate behavioral and neural responses to intended and unintended social norm violations using the revised Social Norm Processing Task (SNPT-R).\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Social Norm Processing Task (SNPT-R)\",\n",
      "    \"TaskDescription\": \"Participants read stories describing neutral social situations, unintentional social norm violations, and intentional social norm violations, and rated them on embarrassment and inappropriateness.\",\n",
      "    \"DesignDetails\": \"The task consists of two phases: a story-reading phase and a rating phase. In the story-reading phase, participants read 78 short stories (26 stem sentences with three types of endings) presented in a pseudo-random order, separated by a jittered fixation cross. Each story consists of a stem sentence (3 seconds) and an ending sentence (6 seconds). The task is divided into two blocks of 39 stories, each lasting approximately 8 minutes and 44 seconds.\",\n",
      "    \"Conditions\": [\n",
      "        \"Neutral\",\n",
      "        \"Unintentional\",\n",
      "        \"Intentional\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Embarrassment ratings\",\n",
      "        \"Inappropriateness ratings\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Approximately 17 minutes and 28 seconds\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 5552726\n",
      "--- delavega-other ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Evaluate cognition changes in severe asymptomatic carotid artery stenosis patients undergoing CAS, using pASL MRI and R-fMRI.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Montreal Cognitive Assessment Beijing Version\",\n",
      "    \"TaskDescription\": \"Assess global cognition in patients with severe asymptomatic carotid artery stenosis.\",\n",
      "    \"DesignDetails\": \"Administered 7 days before and 3 months after CAS to evaluate cognition.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"scores\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Minimum Mental State Examination (MMSE)\",\n",
      "    \"TaskDescription\": \"Assess global cognition in patients with severe asymptomatic carotid artery stenosis.\",\n",
      "    \"DesignDetails\": \"Administered 7 days before and 3 months after CAS to evaluate cognition.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"scores\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Digit Symbol Test\",\n",
      "    \"TaskDescription\": \"Translate numbers to symbols within a given time for cognitive assessment.\",\n",
      "    \"DesignDetails\": \"Performed outside scanner, score based on correct translations within 90s.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"correct translations\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Rey Auditory Verbal Learning Test\",\n",
      "    \"TaskDescription\": \"Evaluate memory and verbal learning ability by recalling words.\",\n",
      "    \"DesignDetails\": \"Participants recall words across five trials, scored on immediate and delayed recall.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"word recall\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Verbal Memory Test\",\n",
      "    \"TaskDescription\": \"Assess verbal memory through tasks such as the digit span.\",\n",
      "    \"DesignDetails\": \"Participants perform forward and backward digit span tasks outside scanner.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"digit span\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Resting-state functional MRI\",\n",
      "    \"TaskDescription\": \"Assess functional connectivity and cerebral perfusion post-CAS.\",\n",
      "    \"DesignDetails\": \"Preprocessed using DPABI, analyzed for ALFF and FC; ROI in PCC and correlation with cognition scores.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"ALFF\",\n",
      "        \"functional connectivity\"\n",
      "    ],\n",
      "    \"RestingState\": true,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To evaluate cognition in severe asymptomatic carotid artery stenosis patients undergoing carotid artery stenting (CAS) and explore the mechanisms underlying cognitive change using pASL MRI and R-fMRI.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Montreal Cognitive Assessment Beijing Version\",\n",
      "    \"TaskDescription\": \"This task assesses global cognition.\",\n",
      "    \"DesignDetails\": \"Participants completed the MoCA to evaluate cognitive function, with scores recorded before and after the CAS intervention.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"cognition scores\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Minimum Mental State Examination\",\n",
      "    \"TaskDescription\": \"This task evaluates cognitive impairment.\",\n",
      "    \"DesignDetails\": \"The MMSE was administered to assess cognitive function, with scores recorded before and after the CAS intervention.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"cognition scores\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Digit Symbol Test\",\n",
      "    \"TaskDescription\": \"This task measures processing speed and attention.\",\n",
      "    \"DesignDetails\": \"Subjects translated numbers to symbols within a given time, with correct translations recorded within 90 seconds.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"response time\",\n",
      "        \"accuracy\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Rey Auditory Verbal Learning Test\",\n",
      "    \"TaskDescription\": \"This task assesses memory and verbal learning ability.\",\n",
      "    \"DesignDetails\": \"Participants recalled words over five repetitions, with immediate and delayed recall recorded.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"number of words recalled\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Verbal Memory Test\",\n",
      "    \"TaskDescription\": \"This task evaluates verbal memory.\",\n",
      "    \"DesignDetails\": \"Participants were assessed on their ability to recall words, with scores recorded before and after the CAS intervention.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"memory scores\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Resting-state functional MRI\",\n",
      "    \"TaskDescription\": \"This task measures brain activity at rest to assess functional connectivity and spontaneous neuronal activity.\",\n",
      "    \"DesignDetails\": \"The resting-state fMRI was acquired using EPI sequence with a repetition time of 2000ms and echo time of 30ms. The first 10 volumes of each time series were abandoned to allow for stabilization. Images were corrected for slice timing and realigned, then normalized into standard MNI space and smoothed with an 8-mm FWHM isotropic Gaussian kernel.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"functional connectivity\",\n",
      "        \"amplitude of low-frequency fluctuation (ALFF)\"\n",
      "    ],\n",
      "    \"RestingState\": true,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 5662181\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Color-Word Stroop task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"The children performed a modified version of the Color-Word Stroop task designed to be used inside the fMRI scanner [ ]. During each trial, participants were presented with a word on a black screen written in one of four colors (red, yellow, green, or blue) and were instructed to subvocalize the color of the stimulus, regardless of the meaning of the written word, and simultaneously press an arbitrary button on a button box. Subvocalization, in which participants say the color of the ink in their heads rather than out loud, is a commonly used approach in developmental neuroimaging studies of cognitive control [ , ] because it helps minimize movement-related artifacts. On the other hand, the technique makes it inherently difficult to ensure that the participant is correctly completing the task. Therefore, extensive pre-scanning training was conducted and in-scanner response times between the conditions were evaluated to have on objective measure of whether the participant was indeed performing the task. Prior to scanning, participants practiced the task in a lab testing room. First, the child was presented with 3 blocks (1 congruent, 2 incongruent with 4 trials in each) and was instructed to say the color of the written word aloud. Second, the child was presented with 1 incongruent block (4 trials total) and was instructed to say the color of the word in his or her head and simultaneously press a button. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate the effects of music training on the cognitive control network in children using an fMRI Stroop task.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Color-Word Stroop Task (Behavioral)\",\n",
      "    \"TaskDescription\": \"The task measured reaction time and accuracy by requiring participants to name the ink color of a word while ignoring the word itself, performed outside the scanner.\",\n",
      "    \"DesignDetails\": \"Participants completed six separate blocks of 12 trials each. Blocks were either all congruent or all incongruent. Each trial was presented for 1700ms. Participants vocalized responses and pressed space bar simultaneously.\",\n",
      "    \"Conditions\": [\n",
      "        \"Congruent\",\n",
      "        \"Incongruent\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Accuracy\",\n",
      "        \"Response time\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Hearts and Flowers task\",\n",
      "    \"TaskDescription\": \"A test of working memory, response inhibition, and task switching, requiring a response on the same or opposite side of an image.\",\n",
      "    \"DesignDetails\": \"The task consisted of three levels: congruent trials, incongruent trials, and mixed congruent and incongruent trials, with participants responding to images with a directional button within 750ms.\",\n",
      "    \"Conditions\": [\n",
      "        \"Congruent\",\n",
      "        \"Incongruent\",\n",
      "        \"Mixed\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Response time\",\n",
      "        \"Accuracy\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Flanker Fish task\",\n",
      "    \"TaskDescription\": \"A task assessing selective attention, task switching, working memory, and inhibitory control using a child-friendly Flanker task.\",\n",
      "    \"DesignDetails\": \"Children responded to the direction of the fish using a response button, with tasks cued by fish color for standard and reverse conditions.\",\n",
      "    \"Conditions\": [\n",
      "        \"Congruent\",\n",
      "        \"Incongruent\",\n",
      "        \"Mixed\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Accuracy\",\n",
      "        \"Response time\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Color-Word Stroop Task\",\n",
      "    \"TaskDescription\": \"The task aimed to measure cognitive control by requiring participants to name the ink color of a word while ignoring the word itself.\",\n",
      "    \"DesignDetails\": \"During scanning, children completed two functional runs, each containing six blocks. Three blocks consisted of congruent trials where the stimulus color matched the word, and three blocks consisted of incongruent trials where they did not match. Each block contained 12 trials, each presented for 1700ms with a 300ms interval between trials, followed by a 16-second rest period, for a total scan time of 240 seconds.\",\n",
      "    \"Conditions\": [\n",
      "        \"Congruent\",\n",
      "        \"Incongruent\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Response time\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"240 seconds per run\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To clarify the connections between music training and executive function in developing children using neuroimaging.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Color-Word Stroop Task\",\n",
      "    \"TaskDescription\": \"A modified version of the Color-Word Stroop task designed to be used inside the fMRI scanner, where participants were instructed to subvocalize the color of the stimulus while pressing a button.\",\n",
      "    \"DesignDetails\": \"During scanning, children completed two functional runs, each containing six blocks. Three blocks consisted of only congruent trials, and three blocks consisted of only incongruent trials. Each block contained 12 trials, each presented for 1700ms with a 300ms interval between each trial. The participants were instructed to respond as quickly and as accurately as possible. Each block was followed by a 16 second rest period for a total scan time of 240 seconds (120 TRs).\",\n",
      "    \"Conditions\": [\n",
      "        \"Congruent\",\n",
      "        \"Incongruent\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"Response time\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"240 seconds\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 5776089\n",
      "--- delavega-other ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Motor Task\",\n",
      "        \"Somatosensory Stimulation\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"he fMRI session was composed of nine rest\\u2013task cycles with 30 s for each period. Eyes were kept closed during scanning. The motor task consisted of repetitive alternating dorsiflexion and relaxation of the right foot (with range reaching 15\\u00b0). Foot movements were paced following an audio cue that was sounded every 1.5 s. These small range of motion and medium speed were applied to avoid large head motions. In each task period, the last audio cue is a verbal command \\u201cstop\\u201d. The verbal command, \\u201cready, right foot movement, go\\u201d was delivered within a 2 s duration that is 3 s ahead of each task period. These audio cues and commands were recorded in advance and transmitted via the intercom system of the MR scanner. Each subject was trained prior to MR scanning to perform the motor task as gently as possible. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: The study examines the activation and connectivity of the brain under different conditions using fMRI, specifically looking at the effects of movement and simultaneous stimulation of the agonist muscle on brain network properties.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Motor Task with Somatosensory Stimulation\",\n",
      "    \"TaskDescription\": \"The task involves repetitive alternating dorsiflexion and relaxation of the right foot, with and without simultaneous somatosensory stimulation to examine brain activation and connectivity.\",\n",
      "    \"DesignDetails\": \"The fMRI session consists of nine rest-task cycles, each 30 seconds long. The motor task includes dorsiflexion and relaxation of the right foot, paced with a 1.5-second audio cue. Stimulation is applied in six of the nine task periods, targeting either the agonist muscle or a control area. Audio cues include 'ready', 'go', and 'stop' commands before each task period.\",\n",
      "    \"Conditions\": [\n",
      "        \"Right ankle dorsiflexion (Task)\",\n",
      "        \"Ankle dorsiflexion with agonist stimulation (Task+AgonistStim)\",\n",
      "        \"Ankle dorsiflexion with control stimulation (Task+ControlStim)\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"brain activation patterns\",\n",
      "        \"functional connectivity\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"570 seconds\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To examine the activation and connectivity of the brain under three different conditions: (1) right ankle dorsiflexion (Task) as the baseline; (2) ankle dorsiflexion coupled with simultaneous stimulation to the agonist muscle (Task+AgonistStim); and (3) to a control area without agonist or antagonist muscles (Task+ControlStim).\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Motor Task\",\n",
      "    \"TaskDescription\": \"The motor task consisted of repetitive alternating dorsiflexion and relaxation of the right foot, paced by an audio cue.\",\n",
      "    \"DesignDetails\": \"The fMRI session was composed of nine rest-task cycles, each lasting 30 seconds. The total duration of the fMRI experiment was 570 seconds. Each task period was preceded by a verbal command delivered within a 2-second duration, 3 seconds ahead of each task period. The last audio cue was a verbal command to stop.\",\n",
      "    \"Conditions\": [\n",
      "        \"Task\",\n",
      "        \"Task+AgonistStim\",\n",
      "        \"Task+ControlStim\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"brain activation\",\n",
      "        \"functional connectivity\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"570 seconds\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 5973829\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Food task\",\n",
      "        \"Altruism task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"\\u00a0The non-social fMRI task was a modified version of an established food task ( ). On every trial, subjects chose between one of 90 food items presented on-screen (4 s) and a default food chosen prior to scanning ( ). Subjects responded by pressing one of four buttons corresponding to \\u2018strong yes\\u2019, \\u2018yes\\u2019, \\u2018no\\u2019, \\u2018strong no\\u2019 (displayed at the bottom of the screen), using a button box placed in their right hand. The assignment of choice preferences to buttons was fixed throughout the task and the right-left orientation of the scale was counterbalanced across subjects. Inter-trial intervals varied from 1 to 4 s (average of 2 s), during which a white fixation cross was presented against a black background. After scanning, one trial was randomly drawn to determine what the subject would eat before leaving the lab. If subjects failed to respond within the 4 s of the selected trial either the on-screen or the default option was randomly chosen. \\n\\nSubjects made food choices under three conditions:   Respond Naturally   (\\u2018respond as you naturally would\\u2019, [NC]),   Focus on Health   (\\u2018focus on the healthiness of the food when making the choice\\u2019, [HC]), or   Focus on Taste   (\\u2018focus on the tastiness of the food when making the choice\\u2019, [TC]) (see Appendix 1 \\u2013 Instructions for regulatory conditions in both choice tasks for instructions). Importantly, subjects were explicitly instructed to always make the decision based on their preference, regardless of the condition. Every condition comprised nine blocks (with 10 trials per block), resulting in a total of 90 trials per condition. Prior to every block, detailed instructions appeared for 4 s. In addition, during food display, a short description (\\u2018Respond Naturally\\u2019, \\u2018Focus on Health\\u2019, \\u2018Focus on Taste\\u2019) appeared at the top of the screen to remind participants of the current instruction. Each of the nine functional scanning runs contained one block of every condition (i.e., three task blocks per run), with the order of conditions randomized across runs and subjects. The only exception was the first task block, which was pre-assigned to \\u2018natural\\u2019 for every subject. Practice trials as well as a short quiz prior to scanning ensured that subjects understood the instructions for each condition and were comfortable with the timing of the task. \\n\\nFood items varied in their perceived tastiness and healthiness and included healthy snacks (e.g., apples, broccoli) and junk foods (e.g., candy bars, chips). Items were selected based on subjects ratings in a self-paced computerized task prior to scanning that assessed perceived tastiness (5-point Likert scale, \\u2018very untasty\\u2019 to\\u00a0\\u2018very tasty\\u2019) and healthiness (5-point Likert scale, \\u2018very unhealthy\\u2019 to \\u2018very healthy\\u2019) of 200 food items ( ;  ). Ninety food items were selected from this larger set to cover the range of health and taste ratings in a roughly uniform manner. In addition, for each subject we chose one default food that was perceived as neutral for taste and health. Each food item was presented once in each of three choice conditions, with presentation order randomized across blocks, functional runs, and subjects. To ensure the motivational saliency of the food items, subjects were asked to refrain from eating 4 hr prior to testing. Stimulus presentation was implemented using high-resolution color pictures (72 dpi) and Psychophysics Toolbox Version 3 ( ) together with Matlab (2014a). \",\n",
      "        \"The altruism task was an fMRI compatible version of the dictator game modified from ( ). On every trial, subjects were presented with a monetary proposal that affected their own ($Self) and another persons\\u2019 ($Other) monetary payoff ( ). Subjects had 4 s to chose between the on-screen proposal and a constant default allocation ($20 to both) by pressing one of the four response buttons (\\u2018strong yes\\u2019, \\u2018yes\\u2019, \\u2018no\\u2019, \\u2018strong no\\u2019; direction counter-balanced across subjects). Payouts for self and other ranged from $0 to $40 and always involved a tradeoff between self and other (i.e. prizes for one individual were equal or less than the default, while prizes for the other individual exceeded the default). Thus, subjects always had to choose between acting altruistically (benefitting the other at a cost to oneself) or selfishly (benefitting oneself at a cost to the other) on every trial. At the end of the experiment, one trial was randomly selected and implemented according to the subjects\\u2019 choice. If subjects failed to respond within 4 s for this trial, both individuals received $0. \\n\\nSimilar to the food task, subjects performed the task under three different conditions:   Respond Naturally   (\\u2018respond as you naturally would\\u2019, [NC]),   Focus on Ethics   (\\u2018focus on doing the right thing and consider the ethical or moral implications of your choice\\u2019, [EC]), or   Focus on Partner   (\\u2018focus on your partner\\u2019s feelings and how the other person is affected by your choice\\u2019, [PC]). Subjects were reminded to always make their choice based on their preference, regardless of the condition. Conditions were implemented in separate blocks of 10 trials each, with the beginning of a new block signaled by a short reminder instruction (4 s). Matching the food task, subjects performed 9 blocks of each condition (i.e., 90 trials per condition and a total of 270 trials), with the block order counter-balanced across subjects and functional runs, with the exception that the first two blocks were always natural choice trials. Choices in these NC blocks were used to estimate a logistic regression [Choice\\u00a0=\\u00a0w  * $Self\\u00a0+\\u00a0w  * $Other] and used for a subject-specific selection of 30% of proposals most likely to elicit generous behavior and 30% of proposals likely to elicit selfish behavior. The remaining 40% of trials were randomly chosen from the full proposal space. Practice trials and a quiz prior to scanning verified that subjects were capable and comfortable to make the choice within 4 s. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To identify neural loci of regulation-related shifts in value representations across goals and domains (dietary or altruistic choice) using behavioral computational modeling and multivariate decoding of fMRI responses.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Food Task\",\n",
      "    \"TaskDescription\": \"A modified food task where subjects choose between food items varying in tastiness and healthiness, and a default option, under different regulatory conditions.\",\n",
      "    \"DesignDetails\": \"Subjects chose between 90 on-screen food items and a default, with responses made in Natural [NC], Focus on Health [HC], and Focus on Taste Conditions [TC]. There were 270 trials, with each condition comprising 90 trials (9 blocks of 10 trials per condition).\",\n",
      "    \"Conditions\": [\n",
      "        \"Natural [NC]\",\n",
      "        \"Focus on Health [HC]\",\n",
      "        \"Focus on Taste Conditions [TC]\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"choice behavior\",\n",
      "        \"decision value\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Altruism Task\",\n",
      "    \"TaskDescription\": \"An fMRI version of the dictator game where subjects decided between monetary proposals affecting their own and another's payoff under different regulatory conditions.\",\n",
      "    \"DesignDetails\": \"Subjects made choices on 270 trials (90 per condition) between monetary proposals. Choices were made under Natural [NC], Focus on Ethics [EC], and Focus on Partner [PC] conditions in 9 blocks of 10 trials each.\",\n",
      "    \"Conditions\": [\n",
      "        \"Natural [NC]\",\n",
      "        \"Focus on Ethics [EC]\",\n",
      "        \"Focus on Partner [PC]\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"monetary payoff\",\n",
      "        \"altruistic choice\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To identify neural loci of regulation-related shifts in value representations across goals and domains (dietary or altruistic choice).\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Food Task\",\n",
      "    \"TaskDescription\": \"Subjects chose between on-screen food items that varied in tastiness and healthiness and a neutral default food.\",\n",
      "    \"DesignDetails\": \"Choices were made in three conditions: Respond Naturally [NC], Focus on Health [HC], and Focus on Taste [TC]. Each condition comprised nine blocks (with 10 trials per block), resulting in a total of 90 trials per condition. Prior to every block, detailed instructions appeared for 4 s. During food display, a short description appeared at the top of the screen to remind participants of the current instruction.\",\n",
      "    \"Conditions\": [\n",
      "        \"Respond Naturally [NC]\",\n",
      "        \"Focus on Health [HC]\",\n",
      "        \"Focus on Taste [TC]\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"choice behavior\",\n",
      "        \"reaction times\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"4 seconds per trial\"\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Altruism Task\",\n",
      "    \"TaskDescription\": \"Subjects chose between on-screen proposals that affected the payoff of themselves and an anonymous partner and a default option.\",\n",
      "    \"DesignDetails\": \"Choices were made in three conditions: Respond Naturally [NC], Focus on Ethics [EC], and Focus on Partner [PC]. Each condition comprised nine blocks (with 10 trials per block), resulting in a total of 90 trials per condition. Matching the food task, subjects performed 9 blocks of each condition (i.e., 90 trials per condition and a total of 270 trials), with the block order counter-balanced across subjects and functional runs.\",\n",
      "    \"Conditions\": [\n",
      "        \"Respond Naturally [NC]\",\n",
      "        \"Focus on Ethics [EC]\",\n",
      "        \"Focus on Partner [PC]\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"choice behavior\",\n",
      "        \"reaction times\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"4 seconds per trial\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6024199\n",
      "--- delavega-other ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"involuntary memory\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"During the re-encoding runs, each sound-picture pair and unpaired sound was presented. Participants rated on a 7-point scale how emotional the stimuli were. During the recall runs, the 55 paired and 40 unpaired sounds (randomly intermixed) were presented, panned 15\\u00b0 to either the left or the right using specialized audio software (Audacity,  ). Participants indicated the side on which the sound was louder. We presented each sound for 4\\u202fs, followed by a 1.5-second response period, during which the screen was blank and participants completed the sound localization judgment. A fixation period followed (jittered with a mean of 4\\u202fs, range: 1.5\\u202fs\\u201310.4\\u202fs). At the beginning of the run, we instructed participants to wait until after the sound had ended to respond. Immediately following the scanning session, participants completed a post-scan questionnaire to assess their memory for the scenes. Participants heard all 95 sounds. After the presentation of each sound, participants reported: (1) whether they had remembered a picture when the sound was played during the sound localization task (yes/no), (2) how hard they tried to perform the sound localization task correctly (1\\u202f=\\u202fdid not try at all, 7\\u202f=\\u202ftried very hard), (3) how hard they tried to recall a picture during the scan (1\\u202f=\\u202fdid not try at all, 7\\u202f=\\u202ftried very hard), and (4) how vivid the memory of the picture was during the scan (1\\u202f=\\u202fnot at all vivid, 7\\u202f=\\u202fvery vivid). Lastly, we presented the 55 paired sounds a final time and participants provided a description of the picture originally paired with the sound. Two independent judges rated these descriptions for accuracy. In the case of disagreement, a third judge broke the tie. As described below, only paired sounds that elicited a memory without effort and unpaired sounds that did not elicit a memory and had no retrieval effort were analyzed. \\n\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To examine the spatiotemporal dynamics of neural activity underlying negative and neutral involuntary memory retrieval in individuals with PTSD compared to trauma-exposed controls.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Involuntary Memory Task\",\n",
      "    \"TaskDescription\": \"Participants were presented with paired and unpaired sounds that served as cues for negative and neutral scenes. Their task was to recall the picture with low effort and perform a sound localization judgment.\",\n",
      "    \"DesignDetails\": \"Participants encoded sound-picture pairs prior to scanning. During scanning, they re-encoded sounds and pictures and performed localization judgments. Sounds were presented in runs, with each sound played for 4 seconds followed by a 1.5-second response period and a fixation period. Paired and unpaired sounds were intermixed randomly. Memory and emotion models were used to analyze neural responses, focusing on paired sounds eliciting memories with low effort.\",\n",
      "    \"Conditions\": [\n",
      "        \"Paired sounds\",\n",
      "        \"Unpaired sounds\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To examine the spatiotemporal dynamics of the neural activity underlying negative and neutral involuntary memory retrieval in individuals with PTSD compared to trauma-exposed controls.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Involuntary Memory Task\",\n",
      "    \"TaskDescription\": \"Participants performed an involuntary memory task while undergoing fMRI scanning, using environmental sounds as cues for associated pictures.\",\n",
      "    \"DesignDetails\": \"The task involved two sessions: a prescan encoding session where participants linked sounds to pictures, and an fMRI session where they rated the emotionality of stimuli and performed sound localization tasks. The fMRI task included paired and unpaired sounds presented in a randomized order, with a response period following each sound.\",\n",
      "    \"Conditions\": [\n",
      "        \"Paired sounds\",\n",
      "        \"Unpaired sounds\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"Emotion ratings\",\n",
      "        \"Vividness ratings\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"4 seconds per sound, with a 1.5-second response period and a jittered fixation period.\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6037859\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"synchrony judgment\",\n",
      "        \"temporal order judgment \"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"The fMRI procedure was similar to the behavioral experiment, except that a reduced stimulus set was presented: \\u2212333, 0, PSS, and +333. The PSS values were obtained individually from the pre-fMRI experiment separately for SJ and TOJ. To be as accurate as possible the individual PSS conditions were selected as the closest COA level to that of the PSS value derived from the pre-fMRI data fits. Although COA levels in the pre-fMRI experiment were restricted to \\u00b1333, \\u00b1267, \\u00b1200, \\u00b1133, \\u00b167, and 0 ms, COA values for the PSS condition in the fMRI experiment could be any COA level between 0 and \\u00b1333 in 16 ms increments i.e., one frame at a time. This use of an individually determined stimulus level (PSS) is similar to the approach used by Binder ( ) to determine stimuli levels, but did not use the simultaneity threshold approach based on separate sound-first and flash-first trials. For TOJ-unable participants, we used average results from a behavioral study using identical stimuli (Petrini et al.,  ). An optimized mixed block/event-related design was used to enable investigation of differences between the tasks at both transient and sustained levels of processing. \\n\\nEach of two functional runs (~22 min each) consisted of 32 stimulation blocks (half SJ and half TOJ, randomized) and after every two stimulation blocks there was a 16 s fixation block (Figure  ). Within a stimulation block (25 s) there were 9 events: 5 stimuli (each 3 s) separated by 4 fixation events (1, 2, 3, or 4 s in pseudorandom order). Each COA condition was presented a total of 40 times (20 per run) per task. To minimize the correlation between the transient (stimuli) and sustained (stimulation block) regressors the number of times an individual COA condition was presented within a single stimulation block was manipulated as follows: in a run, a COA level was presented 0 times during 4 stimulation blocks, once in 6 blocks, twice in 4 blocks and 3 times in 2 blocks, i.e., a total of 20 presentations for each COA level and task. One thousand sequences with different randomizations of the order of events and blocks were created and the best chosen by balancing efficiency and correlation. In the chosen sequence, the mean correlation between sustained and transient regressors was 0.47, which enabled reliable estimation of both types of BOLD response (Otten et al.,  ). \\n\\nAuditory stimuli were presented via Sensimetrics S14 insert headphones at approximately 85 dB. The visual cue was back-projected (Panasonic PT-D7700E DLP; 1,024 \\u00d7 768 pixel resolution, 60 Hz refresh rate) onto a screen behind the participant's head, visible via a mirror mounted on the MR head coil with an approximate viewing distance of 65 cm. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate how behavioral differences between synchrony judgments (SJ) and temporal order judgments (TOJ) tasks are instantiated as neural differences using fMRI.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Synchrony Judgments (SJ) and Temporal Order Judgments (TOJ) Tasks\",\n",
      "    \"TaskDescription\": \"Tasks designed to assess how two different judgments\\u2014whether audiovisual cues are in synchrony (SJ) or which cue comes first (TOJ)\\u2014invoke different neural activations.\",\n",
      "    \"DesignDetails\": \"A mixed block/event-related design was utilized to investigate the neural responses associated with two tasks: SJs and TOJs. The tasks are performed using audiovisual stimuli, with different cue onset asynchronies (COAs). For the fMRI experiment, a subset of synchronous and asynchronous conditions was used to allow investigation of both transient and sustained neural activity. Each of two functional runs consisted of alternating blocks of each task separated by fixation blocks. Stimulus blocks included several COA conditions presented in pseudorandom order to permit the isolation of transient and sustained neural effects. Each task block lasted 25 seconds and constituted 9 events: 5 stimuli and 4 fixation events of varying pseudorandom durations. Event-related impulse responses were modeled for transient effects, while sustained effects used boxcar functions.\",\n",
      "    \"Conditions\": [\n",
      "        \"Synchronous condition\",\n",
      "        \"Largest audio-leading condition\",\n",
      "        \"Largest video-leading condition\",\n",
      "        \"Individually defined PSS\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"task-related deactivation\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Mixed\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate how behavioral differences between synchrony judgment (SJ) and temporal order judgment (TOJ) tasks are instantiated as neural differences using fMRI.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Synchrony Judgment (SJ)\",\n",
      "    \"TaskDescription\": \"Participants decide whether audio and visual cues are in synch or out of synch.\",\n",
      "    \"DesignDetails\": \"The SJ task was conducted in a two-alternative forced-choice format with 20 blocks, half of which were SJ and the other half TOJ. Each block contained 10 trials per cue onset asynchrony (COA) level. The order of blocks was randomized, and instructions were displayed for 4 seconds before each block.\",\n",
      "    \"Conditions\": [\n",
      "        \"Synchronous\",\n",
      "        \"Asynchronous\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"Response accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Mixed\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"25 seconds\"\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Temporal Order Judgment (TOJ)\",\n",
      "    \"TaskDescription\": \"Participants decide which cue came first (or last).\",\n",
      "    \"DesignDetails\": \"The TOJ task was also conducted in a two-alternative forced-choice format with the same structure as the SJ task. Participants responded to 20 blocks, half of which were SJ and half TOJ, with 10 trials per COA level. The order of blocks was randomized, and instructions were displayed for 4 seconds before each block.\",\n",
      "    \"Conditions\": [\n",
      "        \"Synchronous\",\n",
      "        \"Asynchronous\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"Response accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Mixed\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"25 seconds\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6137311\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Yellow Light Game (YLG)\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"The YLG (Op de Macks   et al  .,  ) is an adaption of the widely used Stoplight Task (Gardner and Steinberg,  ; Chein   et al  .,  ;) that examines risk-taking at the behavioral and neural level. In the YLG, participants were asked to drive a virtual car from the driver\\u2019s point of view along a straight track, during which they encountered several intersections with yellow lights ( ). They were instructed that the goal of the game was to get through all of the intersections in the shortest amount of time. At each intersection, participants had to indicate by button press whether they wanted to accelerate and go through the yellow light (go decision) or brake before arriving at the intersection (stop decision). Deciding to accelerate through the intersection\\u2014a go decision\\u2014constitutes a risky decision, and could result either in a successful go associated with no delay (i.e. if there was no other car passing through the intersection), or a delay of 5\\u00a0s in the event of a crash (i.e. if there was another car passing through the intersection). A successful go was shown on the screen with a blue tilde and a positive chiming sound ( ), whereas a crash was shown as a cracked car window, honking car and crash sound ( ).\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To compare within-person peer and parent influences on risky decision-making during adolescence using fMRI.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Yellow Light Game (YLG)\",\n",
      "    \"TaskDescription\": \"A computerized driving task where participants make safe or risky decisions at intersections with yellow lights, aiming to reach destinations quickly.\",\n",
      "    \"DesignDetails\": \"The YLG involved driving a virtual car and encountering intersections with yellow lights, requiring participants to decide to 'go' (risky) or 'stop' (safe). If participants chose to 'go,' it could lead to no delay or a crash delay of 5s. 'Stop' decisions resulted in a 2.5s delay. Participants completed the task while being observed by either peers or parents, undergoing two runs for each social context condition in the scanner. Each run included 20 intersections, totaling 40 trials per condition.\",\n",
      "    \"Conditions\": [\n",
      "        \"Peer Presence\",\n",
      "        \"Parent Presence\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"Percentage of go decisions\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To compare within-person peer and parent influences on risky decision-making during adolescence.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Yellow Light Game (YLG)\",\n",
      "    \"TaskDescription\": \"A computerized driving task where participants make safe or risky decisions at intersections with yellow lights.\",\n",
      "    \"DesignDetails\": \"Participants completed two runs for each social context condition (Peer Presence and Parent Presence), with each run consisting of 20 intersections, totaling 40 trials per condition. The onset of the yellow light was 1500ms after the previous trial, and the probability of crashing was kept at 50% for each run. Participants were trained on the task with two practice runs before the scan.\",\n",
      "    \"Conditions\": [\n",
      "        \"Peer Presence\",\n",
      "        \"Parent Presence\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"percentage of go decisions\",\n",
      "        \"percentage of stop decisions\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Not specified\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6175904\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"autobiographical reminiscence task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"participants were instructed that they would be viewing images from the experience sampling experiment they recently completed and told that each image would be displayed for 8 s. Participants were asked to \\u201c\\u2026 try to remember the event depicted in the picture, and try to relive your experience mentally\\u201d. After the remembrance period for each event, participants were asked if they remembered the event (\\u201cyes\\u201d or \\u201cno\\u201d) and how vividly they recalled the event (\\u201clots of detail\\u201d or \\u201cvery little detail\\u201d). Participants were given 2.5 s to respond to each of those questions using a button box held in their right hand. The images were presented in random order, and the task was split into eight runs with 15 images in each run. With each image presented for 8 s and each question for presented 2.5 s with a 0.5 s interstimulus interval, each trial took a total of 14 s. The intertrial interval was jittered uniformly between 4 and 10 s, allowing for a true event-related design.\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate the brain networks involved in personal semantics during vivid autobiographical memory recollection, focusing on subjective content representations in the precuneus.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"End-of-day Tagging Task\",\n",
      "    \"TaskDescription\": \"Participants reviewed and tagged images from their lifelogging device to capture subjective contents.\",\n",
      "    \"DesignDetails\": \"Each night, participants segmented their stream of images from the day and tagged each episode with descriptors from a provided list. Tags included places, activities, and people.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"Number of tagged events\",\n",
      "        \"Tag selection\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Week Discrimination Task\",\n",
      "    \"TaskDescription\": \"Participants discriminated the week in which events occurred based on images.\",\n",
      "    \"DesignDetails\": \"Conducted midway through and at the end of lifelogging, participants identified whether images were from the first or second week.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"Accuracy\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Autobiographical Reminiscence Task\",\n",
      "    \"TaskDescription\": \"Participants relived personal experiences cued by images from their own lives inside an fMRI scanner.\",\n",
      "    \"DesignDetails\": \"Participants viewed 120 images from their lifelogging data in the fMRI scanner. Each image was displayed for 8 seconds, followed by two questions with a 2.5 seconds response time each. The task was split into eight runs of 15 images each. Trials lasted 14 seconds with inter-trial intervals jittered between 4 and 10 seconds.\",\n",
      "    \"Conditions\": [\n",
      "        \"Vivid recollection\",\n",
      "        \"Non-vivid recollection\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"Response to memory recall question\",\n",
      "        \"Vividness rating\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"112 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate the brain networks that subserve personal semantics and identify the specific parts of these networks that support the phenomenological experience of vivid autobiographical memory.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Week Discrimination Task\",\n",
      "    \"TaskDescription\": \"Participants identified when events depicted in images occurred.\",\n",
      "    \"DesignDetails\": \"Participants were shown images from the preceding 2 weeks and asked to determine whether the image was from the first week or the second week.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"accuracy in week discrimination\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Autobiographical Reminiscence Task\",\n",
      "    \"TaskDescription\": \"Participants relived their experiences in an fMRI scanner cued by images from their own lives.\",\n",
      "    \"DesignDetails\": \"In the scanner, participants viewed images for 8 seconds, followed by questions about their recollection. Each trial lasted 14 seconds, with a jittered intertrial interval of 4 to 10 seconds. The task was split into eight runs with 15 images each.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"vividness ratings\",\n",
      "        \"memory recall\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"~14 seconds per trial\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6200838\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"affective Stroop task\",\n",
      "        \"The Affective Stroop Task \"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"variation of a response inhibition task and comprises a number Stroop task with trials which vary in cognitive load\",\n",
      "        \"Each trial started with an emotional stimulus, i.e., a negative (Neg) or neutral (Neu) stimulus (150 ms), followed by a task trial (congruent/incongruent/neutral Stroop trial or a blank screen) and finally a relaxation period, i.e., blank screen (350 ms). All pictures were selected from a child-appropriate image system [Developmental Affective Photo System (DAPS);  ]. During task trials, participants were presented with an array of 1 to 4 digits or a blank screen and were asked to press a button corresponding to the number of items displayed. The number of items was either congruent (C; e.g., number 3 in an array of 3) or incongruent (IC; e.g., number 1 in an array of 2) with the digits presented. Star shaped stimuli (S; as a neutral baseline counting condition) and blank trials (B; no response expected from participants) were used as control conditions (for further details see  ). Trial order and interstimulus intervals (which were 350\\u20131850 ms) were randomized using Optseq  and kept constant across participants. A total of 300 task and 100 blank trials were administered (100 for C/IC/S trials, 50 with preceding negative images, 50 with neutral images, in 2 runs), with a total scan time of about 16 min (7.59 min each run). \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate the neuronal and behavioral correlates of the interaction of emotion processing and response inhibition in adolescents with Conduct Disorder (CD) during an affective Stroop task.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Affective Stroop Task\",\n",
      "    \"TaskDescription\": \"The task involves variations of the number Stroop task with trials that vary in cognitive load, preceded by negative and neutral images to study emotion processing and response inhibition.\",\n",
      "    \"DesignDetails\": \"Each trial began with an emotional stimulus (negative or neutral, presented for 150 ms), followed by a task trial (congruent/incongruent/neutral Stroop trial or a blank screen) and finally a relaxation period (blank screen, 350 ms). Participants pressed a button matching the number of digits displayed, with the numbers either congruent or incongruent with the digits shown. Star shapes and blank trials served as control conditions. Trial order and interstimulus intervals were randomized using Optseq and kept constant across participants. 300 task and 100 blank trials were administered, with 100 trials each of congruent, incongruent, and star trials, 50 preceded by negative images and 50 by neutral images, in two runs. The total scan time was about 16 minutes.\",\n",
      "    \"Conditions\": [\n",
      "        \"Congruent\",\n",
      "        \"Incongruent\",\n",
      "        \"Neutral\",\n",
      "        \"Blank\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Reaction time\",\n",
      "        \"Accuracy\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"960 seconds\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate the neuronal and behavioral correlates of the interaction of emotion processing and response inhibition in adolescents with conduct disorder (CD) during an affective Stroop task.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Affective Stroop Task\",\n",
      "    \"TaskDescription\": \"A variation of a response inhibition task that combines emotional images with a number Stroop task to investigate the interaction of emotion processing and response inhibition.\",\n",
      "    \"DesignDetails\": \"Each trial started with an emotional stimulus (150 ms), followed by a task trial (congruent/incongruent/neutral Stroop trial or a blank screen) and a relaxation period (350 ms). A total of 300 task and 100 blank trials were administered (100 for C/IC/S trials, 50 with preceding negative images, 50 with neutral images, in 2 runs), with a total scan time of about 16 min (7.59 min each run).\",\n",
      "    \"Conditions\": [\n",
      "        \"Congruent\",\n",
      "        \"Incongruent\",\n",
      "        \"Neutral\",\n",
      "        \"Blank\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Reaction Times (RTs)\",\n",
      "        \"Accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"16 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6219793\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Social feedback paradigm\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"Following previous studies [ \\u2013 ], negative social feedback was used to induce emotions for two reasons: (a) in daily life emotions are often caused by social stimuli [ , ] and (b) social feedback elicits emotional responses that are long enough to study emotion dynamics [ ]. The social feedback consisted of ratings on desirable (e.g., interesting, honest) and undesirable (e.g., stubborn, superficial) personality traits as well as on an item assessing whether the evaluator would like to have the participant as a friend (an English translation of the original feedback forms is shown in  ). Negative feedback consisted of low (high) ratings on desirable (undesirable) items as well as on the evaluator\\u2019s desire to have the participant as a friend. Neutral feedback consisted of ratings close to the neutral scale midpoint of all items. Feedback was shown in one of two pre-specified orders, preventing the presentation of more than two consecutive trials of the same valence (negative or neutral), counterbalanced across participants. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To replicate and extend previous findings on the psychological and neural mechanisms underlying emotion explosiveness and accumulation, examining whether adopting a self-distanced (vs. self-immersed) perspective modulates activity in brain regions associated with these emotional dynamics.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Emotion intensity profile tracking\",\n",
      "    \"TaskDescription\": \"Participants draw a profile reflecting continuous changes in emotion intensity using a trackball on a grid, following exposure to social feedback.\",\n",
      "    \"DesignDetails\": \"After reading and reflecting on the feedback for specified durations, participants use a trackball to draw an intensity profile on a two-dimensional grid, with the X-axis representing time (divided into reading and reflection periods) and the Y-axis representing emotion intensity.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"Profile shape\",\n",
      "        \"Intensity changes\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Self-distancing perspective task\",\n",
      "    \"TaskDescription\": \"Participants adopt either a self-immersed or self-distanced perspective while reading and thinking about negative social feedback to assess changes in negative affect.\",\n",
      "    \"DesignDetails\": \"The experiment involved presenting participants with negative or neutral social feedback and asking them to adopt a self-distanced or self-immersed perspective across two runs consisting of 10 trials each. Each trial involved instructing the participant on the perspective, presenting the feedback, a fixation cross period, drawing an intensity profile, and a relaxation phase. A two-dimensional grid was used to capture emotion intensity over time.\",\n",
      "    \"Conditions\": [\n",
      "        \"Self-immersed perspective\",\n",
      "        \"Self-distanced perspective\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Emotion intensity profile\",\n",
      "        \"BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"50 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To replicate and extend previous findings on the psychological and neural mechanisms underlying emotion explosiveness and accumulation, specifically examining the effects of self-distancing versus self-immersion on these emotional dynamics.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Self-Distancing vs. Self-Immersion Perspective Task\",\n",
      "    \"TaskDescription\": \"Participants were asked to adopt a self-immersed or self-distanced perspective while reading and thinking about negative social feedback.\",\n",
      "    \"DesignDetails\": \"The task consisted of two runs with 10 trials each, where participants read negative or neutral feedback while adopting the instructed perspective. Each trial included a screen announcing the feedback, a fixation cross, and a period for participants to draw an intensity profile reflecting their emotional experience. The total duration of each trial was approximately 90 seconds, followed by a relaxation period before the next trial.\",\n",
      "    \"Conditions\": [\n",
      "        \"Self-Distanced\",\n",
      "        \"Self-Immersed\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"Emotion intensity profile\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"50 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6303343\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Balloon Analogue Risk Task (BART)\"\n",
      "    ],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Exploring the relationship between stability in task-negative regions of the default mode network (DMN) and feedback learning during fMRI scanning, aiming to understand if stability rather than deactivation of the DMN is more predictive of task engagement.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Balloon Analog Risk Task (BART)\",\n",
      "    \"TaskDescription\": \"Measures participants' willingness to engage in risky behavior to earn rewards, using a sequence of balloons that can be pumped to earn points with an associated risk of explosion.\",\n",
      "    \"DesignDetails\": \"During the scan session, participants were presented with 24 balloons that they could pump up to earn points. Each pump decision earned one point and increased the risk of explosion. Participants could cash out at any time to add points to a running total. Each event was separated by a random jitter (500-4000ms). Balloons would explode after 4 to 10 pumps, and the task was self-paced.\",\n",
      "    \"Conditions\": [\n",
      "        \"Pump\",\n",
      "        \"Cash-Out\",\n",
      "        \"Explosion\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Number of pumps\",\n",
      "        \"Points earned\",\n",
      "        \"Balloon explosion rate\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To explore the role of default mode network (DMN) stability and deactivation in predicting adolescent task engagement during a risky decision-making task.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Balloon Analog Risk Task (BART)\",\n",
      "    \"TaskDescription\": \"The BART measures participants' willingness to engage in risky behavior in order to earn rewards, associated with real-life risk-taking in adolescents and adults.\",\n",
      "    \"DesignDetails\": \"Participants completed a version of the Balloon Analogue Risk Task (BART) during fMRI. The task involved a sequence of 24 balloons that participants could pump to earn points, with the risk of the balloon exploding. Each event was separated by a random jitter (500-4000ms). The task was self-paced, and participants were motivated to earn points throughout the task.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"feedback learning\",\n",
      "        \"sensitivity to valence of feedback\",\n",
      "        \"sensitivity to value of feedback\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6331309\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: This study aimed to identify patterns of gray matter volume (GMV) alteration specific to and common among patients with RLS, migraine, and comorbid migraine and RLS.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Resting State fMRI\",\n",
      "    \"TaskDescription\": \"A recent resting-state functional magnetic resonance imaging (rsfMRI) study with a design comparing three clinical groups of individuals: migraine patients with and without RLS, and healthy controls.\",\n",
      "    \"DesignDetails\": \"In the study, patients with migraine, RLS, and healthy controls were scanned at rest. Disrupted functional connectivity (FC) was observed in patients with migraine with and without RLS when compared with healthy controls. Both groups exhibited FC changes, most prominently in attentional, nociceptive, control, and sensory-related networks.\",\n",
      "    \"Conditions\": [\n",
      "        \"Resting\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Functional connectivity (FC) changes\"\n",
      "    ],\n",
      "    \"RestingState\": true,\n",
      "    \"RestingStateMetadata\": {\n",
      "        \"Duration\": \"Not specified\",\n",
      "        \"Instructions\": \"Not specified\"\n",
      "    },\n",
      "    \"TaskDesign\": [\n",
      "        \"Resting\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Not specified\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: This study aimed to identify patterns of gray matter volume (GMV) alteration specific to and common among patients with RLS, migraine, and comorbid migraine and RLS.\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6411911\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"CUPS task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"they had to decide whether to accept or refuse a series of mixed gambles. In each trial participants were presented with a set of cups (3 to 11). They were informed that one of the cups contained a gain (amount ranging between $3 and $8) and the rest of the cups contained a loss of $1, and they were asked to accept or reject the gamble ( ). When the gamble was accepted, the participant was informed of the gain or loss after a short waiting period. When the gamble was rejected, the participant did not win or lose any money. When no selection was made during the response window, the participant lost $1.   \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Examine neural-network differences between casual methamphetamine users and non-users in the decision-making process to identify characteristics of casual meth users responsible for real-life risk.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"CUPS task\",\n",
      "    \"TaskDescription\": \"Participants decide to accept or refuse a series of mixed gambles while undergoing fMRI.\",\n",
      "    \"DesignDetails\": \"Participants perform a series of mixed gambles with varying expected values. Each trial involves choosing to accept or reject a gamble with a gain in one cup and losses in others. The probability and magnitude of gain were manipulated, creating risk equivalent, advantageous, and disadvantageous categories. The task consists of 72 trials, lasting 12 minutes, with jittered trial intervals.\",\n",
      "    \"Conditions\": [\n",
      "        \"risk equivalent (EV=0)\",\n",
      "        \"risk advantageous (EV>0)\",\n",
      "        \"risk disadvantageous (EV<0)\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"response time\",\n",
      "        \"risk taken\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"12 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To identify neurocognitive differences characteristic of casual methamphetamine users compared to non-users during decision-making and risk-taking.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"CUPS task\",\n",
      "    \"TaskDescription\": \"Participants decided to accept or refuse a series of mixed gambles.\",\n",
      "    \"DesignDetails\": \"The task consisted of 72 trials and lasted 12 minutes. Each trial involved a gamble with varying expected values (risk equivalent, risk advantageous, risk disadvantageous). Participants had to respond within 3 seconds after a variable delay, and the inter-trial interval was also variable. The design was optimized for efficiency, with random jittering of intervals and delays.\",\n",
      "    \"Conditions\": [\n",
      "        \"Risk Equivalent (RE)\",\n",
      "        \"Risk Advantageous (RA)\",\n",
      "        \"Risk Disadvantageous (RD)\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD response\",\n",
      "        \"Risk-taking behavior\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"12 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6463125\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"food cue reactivity\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"During the experimental runs images of neutral objects, high-calorie foods, and low-calorie foods were presented in a block design format. Each run consisted of four randomly presented 21\\u202fs long blocks for each image category in which 7 individual images were presented for 2\\u202fs followed by a 1\\u202fs gap. Each block was separated by 9\\u202fs and each run began with 15\\u202fs of blank screen with only a fixation cross present. \\n\\nA total of 112 pictures for each image category were collected from the International Affective Picture System (IAPS) database ( ) and Internet search engines. In the two sessions, the same 112 images were randomly presented in four runs of 28 images for each image category. High-calorie food images contained an equal number of sweet and savory food images. All images were equated for luminance and contrast and presented centrally, subtending 8\\u202f\\u00d7\\u202f6\\u00b0, on a uniform gray background. Stimuli were projected onto a translucent screen located at the back of the scanner bore using a Panasonic PT-D3500E DLP projector (Matsushita Electric Industrial Co., Ltd., Kadoma, Osaka, Japan) at a refresh rate of 60\\u202fHz, and they were viewed through a mirror attached to the head coil at a viewing distance of 57\\u202fcm. Head motion was minimized using foam padding. Stimulus presentation was controlled by MATLAB R2010a (The MathWorks Inc., Natick, MA, USA) using PTB-3 ( ;  ;   http://psychtoolbox.org/  ). Participants were instructed to pay attention to the images presented on the screen. \\n\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To determine if early alterations of fMRI food cue reactivity in the striatum can predict the outcome of a weight loss intervention.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Old/New Memory Task\",\n",
      "    \"TaskDescription\": \"Assessed memory for images viewed in the scanner to ensure attention.\",\n",
      "    \"DesignDetails\": \"Participants completed an old/new memory task outside the scanner following each fMRI session. They were shown 20 images for each image category (food and non-food) and had to decide if they were seen ('old') or not seen ('new') in the scanner. Half of the images were seen in the scanner, and half were novel.\",\n",
      "    \"Conditions\": [\n",
      "        \"Old\",\n",
      "        \"New\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Accuracy\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Food Cue Reactivity Task\",\n",
      "    \"TaskDescription\": \"Examine neural responses to high- vs. low-calorie food images.\",\n",
      "    \"DesignDetails\": \"During experimental runs, images of neutral objects, high-calorie foods, and low-calorie foods were presented in a block design format. Each run consisted of four randomly presented 21s long blocks for each image category in which 7 individual images were presented for 2s followed by a 1s gap. Each block was separated by 9s and each run began with 15s of a blank screen with a fixation cross.\",\n",
      "    \"Conditions\": [\n",
      "        \"Neutral objects\",\n",
      "        \"High-calorie foods\",\n",
      "        \"Low-calorie foods\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"~6 minutes per run, four runs total\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate the relationship between food cue reactivity in the striatum measured one month after the onset of the weight loss program and weight changes obtained at the end of the six-month intervention.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Food Cue Reactivity Task\",\n",
      "    \"TaskDescription\": \"Participants viewed images of neutral objects, high-calorie foods, and low-calorie foods to measure brain activation in response to food cues.\",\n",
      "    \"DesignDetails\": \"The task consisted of four ~6min long experimental runs and two 10min long resting-state runs. Each run included four randomly presented 21s long blocks for each image category, with 7 individual images presented for 2s followed by a 1s gap. Each block was separated by 9s, and each run began with 15s of a blank screen with a fixation cross.\",\n",
      "    \"Conditions\": [\n",
      "        \"Neutral objects\",\n",
      "        \"High-calorie foods\",\n",
      "        \"Low-calorie foods\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"~6 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6699247\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"picture\\u2013word verification task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"In this task, a simple line drawing of an object is presented (250\\u202fms), and following a short delay (75\\u202fms), a word appears. Subjects are instructed to respond by indicating whether the word was a semantic match (50% of trials) or non-match (unmatched, 50% of trials) to the preceding picture. Pictures consisted of 102 line drawings selected from ( ) and classified into 10 natural categories (clothing, animal, bird, appliance, tool, vehicle, vegetable, fruit, toy, and musical instrument). The full set of pictures comprised a block of 102 trials which was repeated across a total of four blocks, with the order of pictures varying across blocks. Half of the unmatched trials comprised picture-word pairs from a related semantic category (related co-hyponym, e.g., APPLE \\u2192lemon) and the other half comprised picture-word pairs from unrelated semantic categories (unrelated, e.g., APPLE \\u2192cow). Subjects were not asked to distinguish between related and unrelated unmatched target words. Occasional null events were inserted between trials, resulting in trial-to-trial intervals that ranged from 3 to 11\\u202fs. Participants responded by button press with right or left index fingers to indicate if the word matched or did not match the picture. Left / right button presses were counterbalanced across participants. All participants performed several practice trials prior to ERP or fMRI recordings. No feedback was given to signal performance accuracy and participants were told to respond as quickly as possible without sacrificing accuracy. All subjects performed the task at >90% accuracy (See Supplementary Table 1. HC\\u202f=\\u202f95.0% and SZ\\u202f=\\u202f91.8%, F(1,45)\\u202f=\\u202f5.7295,   p  \\u202f=\\u202f0.02). We found no effect of practice across the two sessions, performance accuracy was similar for the first and second session across groups (paired   t  -test, t-stat\\u202f=\\u202f\\u22120.70,   p  \\u202f=\\u202f0.49) and remained insignificant after controlling for the time difference between sessions (p\\u202f=\\u202f0.49). Moreover, there was no effect of session order or time between sessions on the loading weight of the joint component reported below (ANCOVA model with covariates of group, session order and times between sessions). Reaction latency data were collected, and the median reaction times per subject, per condition (matched, unmatched) was determined (see Supplementary Methods and Resul\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To link temporal patterns in ERPs to neuroanatomical patterns from fMRI and investigate relationships between N400 amplitude and neuroanatomical activation in schizophrenia patients and healthy controls.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Picture-Word Matching Task\",\n",
      "    \"TaskDescription\": \"A task where participants indicate if a word presented after a picture is a semantic match or non-match to the picture.\",\n",
      "    \"DesignDetails\": \"In this task, a line drawing of an object is presented for 250ms, followed by a short delay of 75ms, after which a word appears. Subjects respond by indicating whether the word is a semantic match (50% of trials) or non-match (50% of trials) to the preceding picture. The task comprised 102 trials per block, repeated across four blocks. Half of the unmatched trials are semantically related (e.g., APPLE lemon) and the other half unrelated (e.g., APPLE cow). Inter-trial intervals ranged from 3 to 11 seconds.\",\n",
      "    \"Conditions\": [\n",
      "        \"Matched\",\n",
      "        \"Unmatched: Related\",\n",
      "        \"Unmatched: Unrelated\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Response time\",\n",
      "        \"Accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To link temporal patterns in ERPs to neuroanatomical patterns from fMRI and investigate relationships between N400 amplitude and neuroanatomical activation in schizophrenia patients and healthy controls.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Picture-Word Matching Task\",\n",
      "    \"TaskDescription\": \"Participants performed a task where they indicated whether a word was a semantic match or non-match to a preceding picture.\",\n",
      "    \"DesignDetails\": \"In this task, a simple line drawing of an object is presented for 250ms, followed by a word after a 75ms delay. Participants respond by indicating if the word matched the picture. The task consists of 102 trials repeated across four blocks, with half of the unmatched trials being semantically related and the other half being unrelated. Occasional null events were inserted between trials, resulting in inter-trial intervals ranging from 3 to 11 seconds.\",\n",
      "    \"Conditions\": [\n",
      "        \"Matched\",\n",
      "        \"In-category (IC)\",\n",
      "        \"Out of category (OC)\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"N400 amplitude\",\n",
      "        \"Reaction time\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"8:34 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6715348\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"fear conditioning and extinction paradigm\"\n",
      "    ],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: None\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Differential Fear Conditioning\",\n",
      "    \"TaskDescription\": \"Participants underwent a differential fear conditioning paradigm using visual stimuli as conditioned stimuli (CS) and an electric shock as an unconditioned stimulus (US).\",\n",
      "    \"DesignDetails\": \"The experiment involved CS+ and CS- trials, with CS+ followed by an aversive US in some trials. An event-related design separated fMRI signals for different trial types. Habituation phase included four CS+ and four CS- trials. Acquisition phase had 10 CS+/US trials, six CS+ only, and 16 CS- only trials. Extinction phase included 16 CS+ only and 16 CS- only trials. Measures like SCR and self-reported ratings were used.\",\n",
      "    \"Conditions\": [\n",
      "        \"CS+\",\n",
      "        \"CS-\",\n",
      "        \"CS+/US\",\n",
      "        \"CS+ only\",\n",
      "        \"CS- only\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Cerebellar activation\",\n",
      "        \"Response to presentation, prediction, and omission of US\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To address the contribution of the cerebellum in processing predictions and prediction errors during a fear conditioning paradigm.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Fear Conditioning Paradigm\",\n",
      "    \"TaskDescription\": \"Participants underwent a differential fear conditioning paradigm during fMRI to study cerebellar activation related to predictions and prediction errors.\",\n",
      "    \"DesignDetails\": \"An event-related design allowed separation of fMRI signals related to the visual conditioned stimulus (CS) from signals related to the subsequent unconditioned stimulus (US; an aversive electric shock). The paradigm included habituation, acquisition, and extinction phases, with specific trial types (CS+ followed by US, CS+ only, and CS- only).\",\n",
      "    \"Conditions\": [\n",
      "        \"CS+\",\n",
      "        \"CS-\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6831914\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Monetary Incentive Delay (MID) Task\"\n",
      "    ],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To probe neural responsiveness to anticipated and received rewards in the context of bipolar disorder using a Monetary Incentive Delay (MID) task.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Monetary Incentive Delay (MID) Task\",\n",
      "    \"TaskDescription\": \"The task is designed to elicit neural and behavioral responses to monetary incentives and their outcomes during FMRI scan acquisition.\",\n",
      "    \"DesignDetails\": \"Each trial (8000ms total) began with presentation of a visual cue (2000ms) indicating incentive valence and magnitude. This was followed by a variable anticipation period (2000-2500ms), a target display (150-470ms), and an outcome period (2000ms). Participants completed two blocks of 90 trials each, totaling 180 trials, across 9 conditions (4 gain, 4 loss, 1 nonresponse). An adaptive timing algorithm kept target hit rates approximately constant.\",\n",
      "    \"Conditions\": [\n",
      "        \"gain with $0.00\",\n",
      "        \"gain with $0.20\",\n",
      "        \"gain with $1.00\",\n",
      "        \"gain with $5.00\",\n",
      "        \"loss with $0.00\",\n",
      "        \"loss with $0.20\",\n",
      "        \"loss with $1.00\",\n",
      "        \"loss with $5.00\",\n",
      "        \"nonresponse\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"hit rate\",\n",
      "        \"hit reaction time\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To probe neural responsiveness to anticipated and received rewards in the context of bipolar disorder.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Monetary Incentive Delay (MID) Task\",\n",
      "    \"TaskDescription\": \"Designed to elicit neural and behavioral responses to monetary incentives and their outcomes during FMRI scan acquisition.\",\n",
      "    \"DesignDetails\": \"Each trial (8000ms total) began with presentation of a visual cue (cue period; 2000ms). Cue shapes indicated the valence (gain: circle, or loss: square) and horizontal lines across the cues indicated magnitude ($0.00: no lines, $0.20: one line, $1.00: two lines, or $5.00: three lines) of incentives that participants could try to gain or avoid losing by responding to an upcoming target. In addition to gain and loss trials, triangle cues indicated nonresponse trials, in which participants were instructed to not respond to upcoming targets. This version of the MID task therefore included 9 total conditions (4 gain, 4 loss, and 1 nonresponse). After viewing the cue, participants were shown a fixation cross for a variable interval (anticipation period; 2000-2500ms), followed by a target that briefly appeared (150-470ms). Participants were instructed to try to press a button before the disappearance of each target to either gain or avoid losing the previously cued amount of money. After a second variable delay (1030-2350ms), participants received feedback informing them of the amount they had gained or lost on each trial (outcome period; 2000ms). Participants completed two blocks including 90 trials each (20 trials per condition; 180 trials total). Trials were presented in a pseudo-random sequence within each block. An adaptive timing algorithm applied to the targets maintained an approximately constant hit rate within each condition (i.e., if an individual's hit rate for a condition did not approximate an average of 66%, the duration of the next target was shortened or lengthened).\",\n",
      "    \"Conditions\": [\n",
      "        \"gain\",\n",
      "        \"loss\",\n",
      "        \"nonresponse\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"hit rate\",\n",
      "        \"hit reaction time\",\n",
      "        \"positive arousal ratings\",\n",
      "        \"negative arousal ratings\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"8000ms\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6847532\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Go/No-Go\",\n",
      "        \"social Go/No-Go\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"While completing an fMRI scan, participants completed both a control go/no-go, which was used solely to establish baseline cognitive performance in the absence of socioaffective cues, and a social go/no-go task ( ) adapted from prior research ( ), which was used to assess emotion regulation in the context of socioaffective cues. The control go/no-go task consisted of four blocks, each containing 25 trials. The control task was completed prior to the social go/no-go, which included four aversive and four appetitive blocks, which were presented in a randomized order. Participants were presented with blocks of socially appetitive or aversive scenes for 300\\u00a0ms, after which a letter was superimposed on the image for 500\\u00a0ms. During this 500\\u00a0ms window, participants were instructed to respond as quickly as possible by pushing a button for every letter shown (\\u2018go\\u2019) except the letter \\u2018X\\u2019 (\\u2018no-go\\u2019). The control go/no-go task was identical in design structure, but did not include superimposed images (rather, a white square was presented on a black screen for 300\\u00a0ms, after which a black letter was superimposed on the white background for 500\\u00a0ms). In both task variants, 28% of the trials were no-go trials, which created a prepotent response to press, requiring inhibition on no-go trials. A jittered Intertrial interval (ITI) was presented between trials, averaging 1200\\u00a0ms. In total, the social go/no-go consisted of 100 trials per condition across eight randomized blocks. Socially appetitive blocks included scenes of people celebrating, cooperating, and being affiliative, while socially aversive blocks included scenes of people excluding one another, bullying peers, and showing negative affect (see   for selection and reliability of the stimuli). The task was programmed and presented using E-Prime 2.0 (2012). \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: None\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Social Go/No-Go Task\",\n",
      "    \"TaskDescription\": \"A modified go/no-go task assessing emotion regulation in the context of socioaffective cues.\",\n",
      "    \"DesignDetails\": \"The control go/no-go task consisted of four blocks, each containing 25 trials. Participants were presented with blocks of socially appetitive or aversive scenes for 300ms, after which a letter was superimposed on the image for 500ms. During this 500ms window, participants were instructed to respond as quickly as possible by pushing a button for every letter shown (go) except the letter X (no-go). In both task variants, 28% of the trials were no-go trials, which created a prepotent response to press, requiring inhibition on no-go trials. A jittered Intertrial interval (ITI) was presented between trials, averaging 1200ms. In total, the social go/no-go consisted of 100 trials per condition across eight randomized blocks.\",\n",
      "    \"Conditions\": [\n",
      "        \"Appetitive\",\n",
      "        \"Aversive\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Behavioral performance (d')\",\n",
      "        \"Neural activation\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To assess how the presence of socioaffective cues differentially impacts inhibitory and neural responses in high-risk delinquent and low-risk community adolescents during a cognitive control task.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Social Go/No-Go Task\",\n",
      "    \"TaskDescription\": \"Participants were instructed to inhibit a prepotent behavioral response while distracted by socioaffective cues, which were either appetitive or aversive social stimuli.\",\n",
      "    \"DesignDetails\": \"The social go/no-go task included four aversive and four appetitive blocks, presented in a randomized order. Each block consisted of socially appetitive or aversive scenes for 300ms, followed by a letter superimposed on the image for 500ms. Participants were instructed to respond as quickly as possible by pushing a button for every letter shown (go) except the letter X (no-go). The task was programmed and presented using E-Prime 2.0 (2012).\",\n",
      "    \"Conditions\": [\n",
      "        \"Appetitive\",\n",
      "        \"Aversive\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"d' (sensitivity index)\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Not specified\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6969196\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"emotional go/no-go paradigm\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"A modified emotional go/no-go task was created with stimuli selected based on data from the pilot phase ( ). Within a go/no-go paradigm participants are instructed to press a button as quickly as possible when shown a \\u2018go\\u2019 (i.e., target) stimulus and to inhibit their response by not pressing the button when shown a \\u2018no-go\\u2019 (i.e., non-target) stimulus.   \\nGo/no-go task design. The figure displays three trials in a run with calm faces as target stimuli and angry faces as non-targets. \\n  Fig. 1   \\n\\nThe rapid event-related task comprised four blocks presented across four different runs. Each block contained two facial emotions (calm and happy, or calm and angry), one instructed to be the target and one as the non-target stimulus, leading to four conditions: (i) happy go/calm no-go, (ii) happy no-go/calm go, (iii) angry go/calm no-go, and (iv) angry no-go/calm go. \\n\\nAt the start of each block participants were shown a screen indicating the target emotion for that block, and reminding them not to respond to other emotions. Each block consisted of 16 trials, with targets (\\u2018go\\u2019) occurring on 75% of these trials, resulting in a total of 96 angry/happy go trials (48 happy, 48 angry), 32 angry/happy no-go trials (16 happy, 16 angry), 96\\u202fcalm go trials and 32\\u202fcalm no-go trials across the four runs (256 trials in total). Trials within a block, and block order within a run, were randomized across participants. Each trial started with a face, which was displayed for 500\\u202fms, followed by a fixation cross which was displayed for a variable interstimulus interval between 1500\\u202fms\\u20132500\\u202fms (in steps of 500\\u202fms). After the last trial of a block, the fixation cross was displayed for 10\\u202fs. During the last trial of a run the fixation cross was displayed for 20\\u202fs in order to acquire the final BOLD response in full. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: The study aims to elucidate how the interplay between emotional cues and cognitive control differs between adolescents who show high or low levels of risk-taking behaviour.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Columbia Card Task (CCT)\",\n",
      "    \"TaskDescription\": \"Measures risk-taking under conditions of low and high emotional arousal where participants draw from a deck of cards to earn or lose points.\",\n",
      "    \"DesignDetails\": \"Participants were shown the outcome of each card after turning it over, and decided if they wish to continue or move on to the next trial. The risk parameters change each time a card is turned over, which makes this a dynamic risk-taking task. The task is divided into hot and cold conditions, with the hot condition involving high emotional arousal.\",\n",
      "    \"Conditions\": [\n",
      "        \"hot condition\",\n",
      "        \"cold condition\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"risk-taking score\",\n",
      "        \"performance deviations\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Emotional Go/No-Go Task\",\n",
      "    \"TaskDescription\": \"Participants were instructed to respond to one emotion and ignore the other, using happy, angry, and calm faces as stimuli.\",\n",
      "    \"DesignDetails\": \"The task was a rapid event-related design comprising four blocks presented across four different runs. Each block contained two facial emotions (calm and happy, or calm and angry), one instructed to be the target and one as the non-target stimulus, leading to four conditions: (i) happy go/calm no-go, (ii) happy no-go/calm go, (iii) angry go/calm no-go, and (iv) angry no-go/calm go. Each block consisted of 16 trials, with targets (go) occurring on 75% of these trials, resulting in a total of 256 trials across the four runs.\",\n",
      "    \"Conditions\": [\n",
      "        \"happy go/calm no-go\",\n",
      "        \"happy no-go/calm go\",\n",
      "        \"angry go/calm no-go\",\n",
      "        \"angry no-go/calm go\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"response time\",\n",
      "        \"inhibitory control\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate the effect of emotional cues on cognitive control in high and low risk-taking adolescents using an emotional go/no-go paradigm.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Emotional Go/No-Go Task\",\n",
      "    \"TaskDescription\": \"Participants were instructed to respond to one emotion (go) and ignore the other (no-go) using facial expressions of calm, happy, and angry faces as stimuli.\",\n",
      "    \"DesignDetails\": \"The task comprised four blocks presented across four different runs, with each block containing two facial emotions (calm and happy, or calm and angry). Each block consisted of 16 trials, with targets (go) occurring on 75% of these trials, resulting in a total of 256 trials across the four runs. Trials within a block and block order were randomized across participants. Each trial started with a face displayed for 500ms, followed by a variable interstimulus interval between 1500ms and 2500ms, and a fixation cross displayed for 10s after the last trial of a block.\",\n",
      "    \"Conditions\": [\n",
      "        \"happy go/calm no-go\",\n",
      "        \"happy no-go/calm go\",\n",
      "        \"angry go/calm no-go\",\n",
      "        \"angry no-go/calm go\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"proportion of hits\",\n",
      "        \"misses\",\n",
      "        \"correct rejections\",\n",
      "        \"false alarms\",\n",
      "        \"mean reaction times\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Approximately 10 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6969350\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"passive viewing task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"Forty-five short video-clips were taken from a larger set created and validated by  . Each clip depicted one actor, dressed in black against a green background, moving in an angry, happy or neutral manner. Six actors were males and nine females, with each actor recorded three times for each of the three emotions. The videos were recorded using a digital video camera and were edited to two-second long clips (50 frames at 25 frames per second). The faces in the videos were masked with Gaussian filters so that only information from the body was perceived (for full details and validation of stimuli (see   and  )). In addition, to use as control stimuli, we selected videos depicting non-human moving objects (e.g. windscreen wipers, windmills, metronomes, etc.) from the internet. We edited these clips using Adobe Premiere so that they matched the body stimuli in terms of size, resolution, and luminance. A green border matching the colour of the human video background was added. In the fMRI experiment, stimuli were presented in blocks of five clips (10\\u202fs). \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate the development of body-selective areas in the visual cortex and their modulation by emotion in children, adolescents, and adults.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Passive emotional body viewing\",\n",
      "    \"TaskDescription\": \"Subjects passively viewed short videos of angry, happy, or neutral body movements.\",\n",
      "    \"DesignDetails\": \"In the fMRI experiment, stimuli were presented in blocks of five clips (10s each). An experimental run consisted of 48 10-seconds-long blocks: eighteen blocks of non-human stimuli, eighteen blocks of human stimuli (three blocks of each emotion), and twelve 10-seconds-long blocks of blank screen as a baseline. Stimuli were back-projected onto a screen, and participants were instructed to maintain gaze at the center.\",\n",
      "    \"Conditions\": [\n",
      "        \"Angry bodies\",\n",
      "        \"Happy bodies\",\n",
      "        \"Neutral bodies\",\n",
      "        \"Non-human objects\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate the functional development of the EBA, pSTS, FBA and amygdala in relation to the perception of emotional human body movements across childhood and adolescence.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Forced-Choice Emotion Recognition Task\",\n",
      "    \"TaskDescription\": \"Participants completed a task to gauge their understanding of the emotional content of the stimuli after the fMRI scan.\",\n",
      "    \"DesignDetails\": \"Participants were presented with the same stimuli used in the fMRI task and asked to identify the emotion conveyed. There was no difference in accuracy between age groups, with mean accuracy at 89% for children, 85% for adolescents, and 89% for adults.\",\n",
      "    \"Conditions\": [\n",
      "        \"Angry\",\n",
      "        \"Happy\",\n",
      "        \"Neutral\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Accuracy\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Passive Viewing fMRI Task\",\n",
      "    \"TaskDescription\": \"Participants passively viewed short videos of angry, happy, or neutral body movements and non-human objects.\",\n",
      "    \"DesignDetails\": \"The experiment consisted of 48 blocks, each lasting 10 seconds, with 18 blocks of non-human stimuli, 18 blocks of human stimuli (three blocks for each emotion), and 12 blocks of blank screen as a baseline. The order of blocks was pseudo-randomized to avoid correlation effects. Each block contained five clips, and the total duration of the task was approximately 8 minutes.\",\n",
      "    \"Conditions\": [\n",
      "        \"Angry\",\n",
      "        \"Happy\",\n",
      "        \"Neutral\",\n",
      "        \"Non-Body\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Approximately 8 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6970153\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"asymmetric matching pennies game\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"The participants played the \\u2018asymmetric matching pennies game\\u2019 in the MRI scanner with three types of opponents: the human opponent (HUM) who played the game outside the scanner and the two artificial agents (FIX and LRN). FIX\\u2019s choices were determined stochastically according to the mixed Nash strategy, while LRN used a machine-learning algorithm to attempt to predict and exploit the participant\\u2019s choices. In the instructions, the human opponent was described as a student of the same sex at the same university, FIX as a computer program that would always follow a fixed, economically rational strategy and LRN as a computer program that would constantly learn to predict the participant\\u2019s choices through interaction. \\n\\nAt the beginning of each trial, a cue indicating the type of opponent (HUM, FIX or LRN) was presented for 0.5\\u00a0s ( ). Then two choice options were presented on the left and right sides of the display. The participant was asked to choose either option within 2\\u00a0s. Immediately after either button was pressed, the frame of the chosen option was colored red. After a jittered fixation duration (2, 4 or 6\\u00a0s), the choice of the opponent was indicated by a green frame, along with the outcome amount for the participant displayed at the center. After a jittered inter-trial interval (ITI) (2, 4 or 6\\u00a0s), the next trial began. \\n  \\nTask sequence (A) and payoff matrix (B). A. In each trial, a conditional cue was presented for 0.5\\u00a0s. Two options were presented on the left and right sides of the display for 2\\u00a0s. When the participant pressed a button, the frame of the chosen option was colored red. After a jittered fixation duration, the choice of the opponent was shown as a green frame along with the monetary outcome the participant obtained. ITI\\u2009=\\u2009inter-trial interval. B. The participant won the game when his/her choice matched with the opponent\\u2019s choice. The participant received 60 JPY by winning with the left choice (\\u2018star\\u2019) and 20 JPY by winning with the right choice (\\u2018diamond\\u2019) but received nothing for losing the game. The combination of left/right, star/diamond and outcome amounts was counterbalanced across participants. \\n  \\nThe participant won the game and received a monetary reward when their choice matched the opponent\\u2019s choice. As shown in  , the participant received 60 JPY if both participant and opponent selected the left choice (\\u2018star\\u2019 in this example) and 20 JPY if both selected the right choice (\\u2018diamond\\u2019), while the opponent received nothing in either case. In contrast, when the two selections did not match, the participant received nothing, while the opponent received 40 JPY for \\u2018star\\u2019 or 20 JPY for \\u2018diamond\\u2019. Thus, the game payoff was asymmetric between the two players. We adopted this asymmetric payoff to make the game-theoretic probabilistic mixed strategy different from a random 50\\u201350 choice; if the payoffs were symmetrical, it would be impossible to distinguish the game-theoretically rational strategy from a purely random choice ( ;  ). The combination of left/right sides, star/diamond shapes and outcome amounts was counterbalanced across participants. The participant\\u2019s understanding of the payoff matrix was confirmed by a series of quizzes prior to the experiment. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To examine the involvement of the left and right temporoparietal junction (TPJ) in second-order inferences during strategic competitive interactions.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Asymmetric Matching Pennies Game\",\n",
      "    \"TaskDescription\": \"An economic game used to study the involvement of TPJ in second-order inferences during competitive interactions.\",\n",
      "    \"DesignDetails\": \"Participants played the asymmetric matching pennies game in the MRI scanner against three types of opponents: a human opponent outside the scanner and two artificial agents (FIX and LRN). The game involved selecting between two options, matching the opponent's choice to win, with differing monetary outcomes. Each opponent type had different strategies - FIX following a fixed probabilistic Nash equilibrium, and LRN using a machine-learning algorithm to predict and exploit participant choices. Each fMRI run included 3 blocks of 12 trials, totaling 144 trials. The game stimuli were presented on an MRI-compatible display, and responses were recorded using an MRI-compatible response pad.\",\n",
      "    \"Conditions\": [\n",
      "        \"Human opponent (HUM)\",\n",
      "        \"Artificial agent using fixed strategy (FIX)\",\n",
      "        \"Artificial agent using machine learning (LRN)\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Choice behavior\",\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"Response time\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To examine the involvement of the temporoparietal junction (TPJ) for second-order inferences during strategic competitive interaction.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Asymmetric Matching Pennies Game\",\n",
      "    \"TaskDescription\": \"Participants played an economic game against three types of opponents: a human opponent, a fixed strategy artificial agent (FIX), and a learning artificial agent (LRN).\",\n",
      "    \"DesignDetails\": \"The task consisted of 144 trials divided into 4 fMRI runs, with each run containing 3 blocks of 12 consecutive trials. Each trial began with a cue indicating the opponent type (HUM, FIX, or LRN) for 0.5 seconds, followed by a choice presentation for 2 seconds. After a jittered fixation duration (2, 4, or 6 seconds), the opponent's choice and the participant's outcome were displayed. The order of opponent blocks was counterbalanced across participants.\",\n",
      "    \"Conditions\": [\n",
      "        \"HUM\",\n",
      "        \"FIX\",\n",
      "        \"LRN\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"choice behavior\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Approximately 10 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 6981017\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"simple motor task (SMT)\"\n",
      "    ],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Examine the relationship between sleep disruption-induced pain sensitivity and functional connectivity (FC) of cognitive networks contributing to pain modulation.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Simple Motor Task\",\n",
      "    \"TaskDescription\": \"Designed to elicit intrinsic network connectivity without the risk of participants falling asleep.\",\n",
      "    \"DesignDetails\": \"Participants responded to the direction of a projected arrow (left or right). Arrows were displayed for 500ms each, with a total of 150 trials. Interstimulus intervals were jittered and ranged from 1s to 4s.\",\n",
      "    \"Conditions\": [\n",
      "        \"Left Arrow\",\n",
      "        \"Right Arrow\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Functional connectivity changes\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"10 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To examine the relationship between sleep disruption-induced pain sensitivity and functional connectivity (FC) of cognitive networks contributing to pain modulation.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Simple Motor Task (SMT)\",\n",
      "    \"TaskDescription\": \"A task designed to elicit intrinsic network connectivity without the risk of participants falling asleep.\",\n",
      "    \"DesignDetails\": \"Participants responded to the direction of a projected arrow (left or right) displayed for 500ms at a time, with a total of 150 trials. Left and right arrows were shown in equal proportion, and interstimulus intervals were jittered, ranging from 1s to 4s each.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"10 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 7018765\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Testing Emotional Attunement and Mutuality (TEAM)\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"his task builds on past studies that have utilized error processing paradigms in the study of ER (e.g.,  ;  ;  ) by specifically examining dyadic error processing. Parent-adolescent dyads completed the TEAM task while simultaneously undergoing fMRI scanning. This task was developed to examine brain activation in both parents and adolescents when the other member of the parent-adolescent dyad makes a costly error. It thus allows us to probe emotion reactivity and regulation in response to being \\u201clet down\\u201d by the other person within the context of a relationship and processing an error made by a family member. The costly errors result in a monetary loss for the dyad, which evokes negative affect in participants, particularly because they themselves responded accurately ( ). In order to continue performing the task on the next trial, participants must regulate their emotional response to their partner\\u2019s error (implicit ER).\",\n",
      "        \"The TEAM task is an event-related design and consists of 17 trials during which participants first see a pattern of colored arrows presented sequentially on the screen for 3 s, twice in a row (totaling 6 s). They are then given 4 s to reconstruct the sequence by pressing colored buttons on a response box (shown in  ). At the end of the 4-s response window, participants see a message with feedback regarding both dyad members\\u2019 performance on that trial. Prior to the scan, participants are told that if one or both members of the parent-adolescent dyad respond incorrectly to a trial, they will lose $5 from a starting amount of $50. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: The study examined brain activation related to parenting and emotion regulation (ER) in parent-adolescent dyads during concurrent fMRI scanning with the TEAM task, aiming to study dyadic error processing and how it relates to parenting practices.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Testing Emotional Attunement and Mutuality (TEAM) task\",\n",
      "    \"TaskDescription\": \"The TEAM task is designed to probe emotional reactivity and regulation in a dyadic context by examining brain activation in both parents and adolescents when the other dyad member makes a costly error.\",\n",
      "    \"DesignDetails\": \"The task consists of 17 trials where participants see a pattern of colored arrows for 6 seconds, then have 4 seconds to reconstruct the sequence using a response box. Feedback is given on both dyad members' performance. The TEAM task is pre-programmed to show partner errors on 3 trials per run.\",\n",
      "    \"Conditions\": [\n",
      "        \"Partner correct\",\n",
      "        \"Partner incorrect\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Brain activation\",\n",
      "        \"fMRI activation in ER-related regions\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: The current study examined brain activation related to parenting and emotion regulation in parent-adolescent dyads during concurrent fMRI scanning with a novel task, the Testing Emotional Attunement and Mutuality (TEAM) task.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"TEAM task\",\n",
      "    \"TaskDescription\": \"The TEAM task is designed to probe emotional reactivity and regulation in a dyadic context, specifically examining dyadic error processing.\",\n",
      "    \"DesignDetails\": \"The TEAM task consists of 17 trials during which participants first see a pattern of colored arrows presented sequentially on the screen for 3 s, twice in a row (totaling 6 s). They are then given 4 s to reconstruct the sequence by pressing colored buttons on a response box. At the end of the 4-s response window, participants see a message with feedback regarding both dyad members' performance on that trial. The task was pre-programmed to ensure there would be trials where the partner was incorrect but the participant was correct, as the task was simple enough that most participants made few or no errors (average error rate across both runs = 1.81 trials).\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": null,\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"7 min and 50 s\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 7235961\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Pain empathy task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"During the pain empathy task, participants received a cue (2\\u00a0s) if the electrical shock was directed at themselves (arrow pointing left, self-directed trial;  ) or at another participant (arrow pointing right, other-directed trial;  ). Additionally, the color of the arrow informed the participant about the upcoming stimulation intensity (red, painful; green, non-painful). The other participant was a member of the experimental team and actually never received any shocks. After a brief delay jittered between 2 and 5\\u00a0s (mean\\u2009=\\u20093.5\\u00a0s), participants saw a photo of the shock recipient on the screen (1\\u00a0s; self-directed trial: scrambled photo of themselves; other-directed trial: photo of the confederate with painful/neutral facial expression), and a brief electrical shock (500\\u00a0ms) was delivered (during self-directed trials only). This was followed by a fixation period ranging from 3 to 7\\u00a0s (mean\\u2009=\\u20095\\u00a0s) and affect ratings (6\\u00a0s) which were collected during one-third of the trials (self-directed pain ratings: \\u2018How painful was this stimulus for you?\\u2019, other-directed affect ratings: \\u2018How painful was this stimulus for the other person?\\u2019, and \\u2018How unpleasant did it feel when the other person was stimulated?\\u2019). Trials were separated with a short fixation period (2\\u00a0s). In total, participants completed 15 trials per condition (i.e. self-directed painful, self-directed non-painful, other-directed painful, other-directed non-painful). The task was programmed and presented with Cogent (version 1.32,   www.vislab.ucl.ac.uk/cogent.php  ) and lasted for approx. 16\\u00a0min. \\n\\nStimulation intensities during self-directed trials were set to individually calibrated stimulation intensities related to pain ratings of 1 (i.e. non-painful trial) and 7 (i.e. painful trial) throughout the task. The average stimulation intensities were 0.15\\u2009\\u00b1\\u20090.14\\u00a0mA (mean\\u2009\\u00b1\\u2009SEM; pain intensity rating of 1) and 0.74\\u2009\\u00b1\\u20090.6\\u00a0mA (pain intensity rating of 7) during non-painful and painful trials, respectively. \\n\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To test how hippocampal-neocortical pattern similarity and connectivity contribute to pain empathy.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Pain Empathy Task\",\n",
      "    \"TaskDescription\": \"Participants underwent fMRI while receiving painful and non-painful electrical stimulation either to themselves or a confederate.\",\n",
      "    \"DesignDetails\": \"Participants received cues about upcoming electrical shock intensities. Pain ratings were collected during one-third of the trials. Total of 15 trials per condition (self-directed painful, self-directed non-painful, other-directed painful, other-directed non-painful).\",\n",
      "    \"Conditions\": [\n",
      "        \"self-directed painful\",\n",
      "        \"self-directed non-painful\",\n",
      "        \"other-directed painful\",\n",
      "        \"other-directed non-painful\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"pattern similarity\",\n",
      "        \"hippocampal coupling\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"16 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate how hippocampal-neocortical pattern similarity and connectivity contribute to pain empathy.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Pain Empathy Task\",\n",
      "    \"TaskDescription\": \"Participants received painful and non-painful electrical stimulation directed at themselves or a confederate while undergoing fMRI.\",\n",
      "    \"DesignDetails\": \"The task involved 15 trials per condition (self-directed painful, self-directed non-painful, other-directed painful, other-directed non-painful). Each trial began with a cue indicating the type of trial, followed by a brief delay, a photo of the shock recipient, and a rating period for pain and unpleasantness. The task lasted approximately 16 minutes.\",\n",
      "    \"Conditions\": [\n",
      "        \"self pain\",\n",
      "        \"self no pain\",\n",
      "        \"other pain\",\n",
      "        \"other no pain\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"pattern similarity\",\n",
      "        \"pain ratings\",\n",
      "        \"unpleasantness ratings\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"16 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 7377905\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"social perceptual decision task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"On each trial, subjects made a group decision about a visual stimulus with one of four partners. Subjects were told that the partners were created by replaying the responses of four people performing the perceptual task on a separate day but, in reality, the partners were simulated. First, subjects judged whether a field of dots was moving left or right. Next, after being informed about the identity of their partner on the current trial, subjects were asked to report their confidence in the perceptual judgement \\u2013 an estimate which would enter into the group decision. Subjects were then shown the current partner\\u2019s response for that trial. Finally, implementing a common group decision rule ( ), the individual decision made with higher confidence was automatically selected as the group decision, after which feedback about its accuracy was delivered. Subjects were incentivised to help each group achieve as many correct group decisions as possible but, by design, could only affect group decisions through their confidence reports. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Investigating how the human brain supports private-public mappings using an interactive task requiring subjects to adapt their communication of confidence about a perceptual decision to the social context.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Social perceptual decision task (behavioral session)\",\n",
      "    \"TaskDescription\": \"Calibrated subjects' coherence levels for choice accuracy and trained on social task involving private-public mappings.\",\n",
      "    \"DesignDetails\": \"In behavioral session, subjects first experienced coherence calibration, then trained on the social task with four partners in blocks and interleaved manner.\",\n",
      "    \"Conditions\": [\n",
      "        \"Coherence\",\n",
      "        \"Partner type\"\n",
      "    ],\n",
      "    \"TaskMetrics\": null\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Social perceptual decision task\",\n",
      "    \"TaskDescription\": \"Subjects adapted communication of decision confidence to social context to study private-public mappings.\",\n",
      "    \"DesignDetails\": \"Subjects (n=28) performed a perceptual decision task with varying coherence levels to manipulate confidence. They decided motion direction of dots and reported confidence using a discrete scale. Task involved interactions with simulated partners, designed to vary context and encourage context-dependent confidence reporting.\",\n",
      "    \"Conditions\": [\n",
      "        \"Motion coherence\",\n",
      "        \"Partner mean confidence\",\n",
      "        \"Hidden context\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"Confidence reports\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\",\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: Investigated how the human brain supports private-public mappings using an interactive task requiring subjects to adapt their communication of confidence about a perceptual decision to the social context.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Social Perceptual Decision Task\",\n",
      "    \"TaskDescription\": \"Subjects communicated their confidence about simple perceptual decisions in different social contexts, requiring different mappings from private to public confidence.\",\n",
      "    \"DesignDetails\": \"Subjects performed a social perceptual decision task in separate prescan and scan sessions. Each trial began with a fixation cross, followed by a field of moving dots. After making a decision about the direction of motion, subjects reported their confidence on a scale from 1 to 6. The task was designed to manipulate the fraction of coherently moving dots and the identity of the partner to create different social contexts. The fMRI session consisted of four scan runs, with conditions matched across runs to facilitate multivariate analysis of the fMRI data.\",\n",
      "    \"Conditions\": [\n",
      "        \"High Confidence Partner\",\n",
      "        \"Low Confidence Partner\",\n",
      "        \"Medium-High Confidence Partner\",\n",
      "        \"Medium-Low Confidence Partner\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Confidence Report\",\n",
      "        \"Group Decision Accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Varied by trial, with each trial lasting approximately 10 seconds.\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 7426775\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: The study aims to use a Deep Learning framework to predict chronological age from structural MRI scans and identify brain regions contributing to this prediction, focusing on brain aging.\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To predict chronological age from structural magnetic resonance imaging scans using a deep learning framework and to identify brain regions contributing to this prediction.\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 7562935\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Attitude conformity fMRI task \"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"Participants completed the Attitude Conformity task during fMRI. On each trial, participants first viewed a behavior they previously rated (but were not reminded of their original ratings) (2 s). Following a jittered inter-stimulus interval (  M   = 2 s), participants were then shown their parents\\u2019 and peers\\u2019 ratings on each behavior and instructed to choose which person they agreed with most (maximum of 5 s). Participants pressed the left index finger when they agreed with their parent or right index finger when they agreed with their peer. Participants\\u2019 choices were self-paced, such that the task advanced to the next behavior upon participant response. Behaviors were presented in random order and were separated by jittered inter-trial fixation periods (  M   = 2 s). Conformity was operationalized as choosing the person whose rating conflicted with the adolescent\\u2019s original rating, whereas resistance was operationalized as choosing the person whose rating was the same as the adolescent\\u2019s original rating (described below). \\n\\nIn order to examine decisions to conform in the face of conflicting attitudes, we tailored the task to each participant based on their ratings assessed during the behavioral session. Although we collected the parents\\u2019 actual ratings during the behavioral session, and ostensibly collected peers\\u2019 ratings, such ratings were not used as we carefully manipulated the ratings to fall within the attitude conflict and social influence conditions described below. Of the 200 behaviors that participants originally rated at the behavioral session, 120 behaviors were selected for the fMRI task based on two criteria. First, the participant\\u2019s rating for a behavior needed to fall between minimum and maximum plausible ratings determined for each behavior based on pilot data, thereby maximizing ecological validity and checking for deviant responding (e.g., rating \\u201ccheating on a test\\u201d as 10=very good was outside the range of plausibility). Second, given that extreme ratings may be less likely to change ( ), the participant\\u2019s rating for a behavior could not fall at the extremes of the scale (i.e., 1 or 10), ensuring that their parents\\u2019 and peers\\u2019 ratings could be manipulated to be below, above, or centered at participants\\u2019 original ratings. Thus, the strength of participants\\u2019 original ratings was relatively moderate across the subset of behaviors included in the fMRI task, with a balanced distribution across constructive and unconstructive behaviors. \\n\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To examine how conformity decisions are evaluated in the developing brain and unfold across social contexts, particularly when adolescents' own opinions conflict with those of their parents, peers, or both.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Behavioral Session - Attitude Rating\",\n",
      "    \"TaskDescription\": \"Participants rated their baseline attitudes toward everyday behaviors using a Likert scale to provide data for subsequent fMRI task manipulation.\",\n",
      "    \"DesignDetails\": \"Participants rated 200 behaviors on a 10-point Likert scale, including both constructive and unconstructive behaviors. Ratings were collected to ensure sufficient trials for manipulation during the fMRI task.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"attitude ratings\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Attitude Conformity Task\",\n",
      "    \"TaskDescription\": \"Participants indicate who they agree with when presented with their own, their parent's, and a peer's attitudes towards everyday behaviors.\",\n",
      "    \"DesignDetails\": \"Participants first viewed a behavior they previously rated, then were shown their parents' and peers' ratings on each behavior. They had to choose which person they agreed with most. The task included 120 trials divided equally by attitude conflict condition (Parent Conflict, Peer Conflict, Mutual Conflict). Each attitude conflict condition was equally divided by type of behavior (constructive and unconstructive behaviors) with social influence manipulated towards positive or negative influence.\",\n",
      "    \"Conditions\": [\n",
      "        \"Parent Conflict\",\n",
      "        \"Peer Conflict\",\n",
      "        \"Mutual Conflict\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"response time\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: This fMRI study assessed the extent to which early adolescents conform to their parents and peers conflicting attitudes toward different types of behavior (unconstructive and constructive) and in response to different types of influence (negative and positive).\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Attitude Conformity task\",\n",
      "    \"TaskDescription\": \"Participants completed the Attitude Conformity task during fMRI, where they were shown parent and peer attitudes that conflicted with their own attitudes and instructed to indicate who they agree with.\",\n",
      "    \"DesignDetails\": \"On each trial, participants first viewed a behavior they previously rated (2 s). Following a jittered inter-stimulus interval (M = 2 s), participants were then shown their parents and peers ratings on each behavior and instructed to choose which person they agreed with most (maximum of 5 s). Participants pressed the left index finger when they agreed with their parent or right index finger when they agreed with their peer. Participants choices were self-paced, and behaviors were presented in random order with jittered inter-trial fixation periods (M = 2 s).\",\n",
      "    \"Conditions\": [\n",
      "        \"Parent Conflict\",\n",
      "        \"Peer Conflict\",\n",
      "        \"Mutual Conflict\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"conformity\",\n",
      "        \"resistance\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"120 trials\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 7582181\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate the unique and overlapping regional grey matter volume (rGMV) correlates of inter-individual differences in social threat and reward expectancies using voxel-based morphometry (VBM).\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate the relationship between social reward and threat expectancies and regional grey matter volumes (rGMV) in healthy adults using voxel-based morphometry (VBM).\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"LODESTARS\",\n",
      "    \"TaskDescription\": \"The LODESTARS is a 10-item inventory examining the extent to which respondents expect to experience social reward and threat during an imminent vividly imagined social encounter with a group of unfamiliar peers.\",\n",
      "    \"DesignDetails\": \"Participants are asked to imagine that they have joined a new group, club, or society and that this evening they will be going to a social event organized by this group. They indicate their anticipated and anticipatory cognitions and emotions about the upcoming imagined event, responding to 5 threat items and 5 reward items on a 5-point Likert scale.\",\n",
      "    \"Conditions\": null,\n",
      "    \"Taskmetrics\": [\n",
      "        \"social reward expectancy (SRE) score\",\n",
      "        \"social threat expectancy (STE) score\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 7649291\n",
      "--- delavega-other ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"fine motor learning task\",\n",
      "        \"gross motor comparison task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"The fine motor learning task involved accurate, rapid tracing of randomize trail mazes, with performance scores incorporating distance and error-rate. All scans were 6 min long with seven on-blocks of 24 s each during which a unique trail was displayed. These on blocks were interleaved with jittered rest blocks averaging 24 s. Each scan began and ended with a rest block. The trails were designed to be too complex to be completed in a single 24 s block. The gross motor training task followed the same block structure and visual display as the fine motor task, but simply had the participants move the mouse at random, with no accuracy metrics. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate WM functional neuroplasticity using motor learning tasks and assess changes in the hemodynamics of white matter using fMRI.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Motor Learning Task\",\n",
      "    \"TaskDescription\": \"Participants completed fine and gross motor tasks with both hands using an MRI compatible computer mouse.\",\n",
      "    \"DesignDetails\": \"Participants guided a cursor through a marked trail on the monitor, completing tasks with each hand. They had a baseline scan, then 2 weeks of training followed by midpoint and endpoint scans. Fine motor tasks involved tracing mazes and were 6 min long with seven 24s on-blocks. Gross motor tasks involved random movements with no accuracy metrics.\",\n",
      "    \"Conditions\": [\n",
      "        \"Fine motor task\",\n",
      "        \"Gross motor task\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Performance metrics based on speed and accuracy\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate white matter neuroplasticity and the effects of motor learning on the internal capsule and hemodynamic response variability.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Motor Training Task\",\n",
      "    \"TaskDescription\": \"Participants trained on motor tasks with both hands to assess motor learning effects.\",\n",
      "    \"DesignDetails\": \"Participants completed daily at-home task training for 2 weeks, with a baseline scan involving both fine and gross motor tasks. The fine motor task was counterbalanced with a gross motor comparison task. Performance metrics were recorded based on speed and accuracy, emphasizing the difference in dominant and non-dominant hand proficiency.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"motor task score\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Fine Motor Learning Task\",\n",
      "    \"TaskDescription\": \"Participants traced randomized trail mazes to assess fine motor skills.\",\n",
      "    \"DesignDetails\": \"The task involved accurate, rapid tracing of randomized trail mazes, with performance scores incorporating distance and error-rate. Each scan lasted 6 minutes, consisting of seven on-blocks of 24 seconds each, interleaved with jittered rest blocks averaging 24 seconds. Each scan began and ended with a rest block, and the trails were designed to be too complex to be completed in a single 24-second block.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"distance traveled\",\n",
      "        \"error-rate\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"6 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 7689031\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Regulation of Craving (ROC) Task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"Participants were trained to decrease their desire to consume personally-desired (for task purposes these are referred to as \\u201ccraved\\u201d foods) foods using cognitive reappraisal ( ;  ). Participants viewed unhealthy craved foods (\\u201cCraved\\u201d condition), unhealthy not-craved foods (\\u201cNot Craved\\u201d condition), or healthy vegetables (\\u201cNeutral\\u201d condition). For unhealthy craved foods, participants either actively viewed the foods (\\u201cLook\\u201d condition) or reappraised their craving for them (\\u201cRegulate\\u201d condition). On \\u201cLook\\u201d trials, participants imagined how they would interact with the food if it were in front of them. On \\u201cRegulate\\u201d trials, participants reappraised the foods by focusing on the short- or long-term negative health consequences associated with consumption (e.g., stomach aches, weight gain, cavities, etc.). Participants generated several negative health consequences with the help of the experimenter, to ensure they had multiple reappraisals they could use during the task. To minimize demand characteristics (e.g., reduced craving ratings on regulate trials), participants were reassured they were not expected to be able to regulate well on every trial and were told that it was important to rate their cravings honestly. Neutral stimuli were only viewed under \\u201cLook\\u201d instructions, and are not used in the present analyses. To keep task time to a minimum, only craved foods were viewed under \\u201cRegulate\\u201d instructions. \\n\\nTo maximize craving, participants selected their most craved and least craved food from the following menu of unhealthy food categories for the \\u201cCrave\\u201d and \\u201cNot Craved\\u201d conditions respectively: chocolate, cookies, donuts, French fries, ice cream, pasta, pizza. Stimuli were independently rated for desirability, such that the mean desirability of stimuli within each unhealthy food category did not differ significantly across categories ( ). As such, each participant viewed a set of individually adapted unhealthy food stimulus categories; across all participants, the only domain on which the \\u201cCraved\\u201d and \\u201cNot Craved\\u201d conditions differed was with regard to individual food preferences. Each condition (Look Neutral, Look Craved, Look Not Craved, and Regulate Craved) had 20 trials, presented across two task runs. On each trial (see  ), participants are presented with an instruction (2 s; Look or Regulate), viewed a food image while following the instruction (5 s), and rated their craving for the food on a 5-point Likert scale (4 s; 1 = not at all, 5 = very much). Each 11s trial of this event-related design was followed by a jittered fixation cross (  M   = 1 s) and trial order is optimized using a genetic algorithm ( ). Stimuli were presented using Psychtoolbox 3 ( ), and participants responded using a five-button box. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: None\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: The present study aimed to investigate the associations between food craving reactivity and regulation-related brain activity and measures of craving for and consumption of healthy and unhealthy foods in a community sample of middle-aged adults with higher BMIs, as well as how this brain activity at baseline predicted changes in food craving and consumption over the course of 6 months.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Regulation of Craving (ROC) Task\",\n",
      "    \"TaskDescription\": \"Participants were trained to decrease their desire to consume personally-desired foods using cognitive reappraisal.\",\n",
      "    \"DesignDetails\": \"Participants viewed unhealthy craved foods (Craved condition), unhealthy not-craved foods (Not Craved condition), or healthy vegetables (Neutral condition). For unhealthy craved foods, participants either actively viewed the foods (Look condition) or reappraised their craving for them (Regulate condition). Each condition had 20 trials, presented across two task runs. On each trial, participants are presented with an instruction (2 s; Look or Regulate), viewed a food image while following the instruction (5 s), and rated their craving for the food on a 5-point Likert scale (4 s; 1 = not at all, 5 = very much). Each 11s trial of this event-related design was followed by a jittered fixation cross (M = 1 s).\",\n",
      "    \"Conditions\": [\n",
      "        \"Craved\",\n",
      "        \"Not Craved\",\n",
      "        \"Neutral\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"craving rating\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Not specified\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 7836234\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Face-word Stroop task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"The task comprised face-word stimuli consisting of male and female faces [Glasgow Face Database,  ], overlaid by the words \\u201cman\\u201d or \\u201cwoman\\u201d (in German language). These words could be either congruent or incongruent with the sex of the face ( A). The paradigm consisted of the 120 faces (60 males and 60 females) previously presented in the familiarization task. Faces were displayed for 1500\\u00a0ms in the center of a white screen. We used an event-related design in order to identify separate trial-related Blood-oxygen-level-dependent (BOLD) responses. Trial onsets randomly varied with a pseudo-exponentially distributed SOA of 5000\\u00a0ms (75%), 7500\\u00a0ms (17%) and 10,000\\u00a0ms (8%). The words were displayed in red ink superimposed on the faces. Both trial types were presented in a randomized order and occurred with a probability of 50%. Subjects were randomly assigned to one of two stimulus sets. Stimuli that were congruent in one group of subjects were incongruent in the other group. Subjects had to indicate whether a face was male or female by pressing the left or right mouse button (button assignments were counterbalanced across participants in each group). Throughout the entire paradigm, a black fixation dot was displayed in the middle of the face (and right below the word) to ensure accurate fixation.   \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate the relevance of the hippocampus for conflict-induced memory improvement in patients with mesial temporal lobe epilepsy (MTLE) and healthy controls using a face-word Stroop task during fMRI.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Face-word Stroop Task\",\n",
      "    \"TaskDescription\": \"Participants performed a face-word Stroop task during fMRI. The task involves congruent or incongruent alignment of words with male or female faces, requiring participants to identify the gender of the face despite the distraction of the word.\",\n",
      "    \"DesignDetails\": \"The task used an event-related design with 120 faces (60 male, 60 female) from the Glasgow Face Database, each overlaid with the words 'man' or 'woman'. Trials varied randomly with a pseudo-exponentially distributed stimulus-onset-asynchrony of 5000ms (75%), 7500ms (17%) and 10,000ms (8%). Participants indicated face gender using a mouse button press. Each trial lasted 1500ms with fixation throughout the paradigm.\",\n",
      "    \"Conditions\": [\n",
      "        \"Congruent\",\n",
      "        \"Incongruent\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"response time\",\n",
      "        \"accuracy\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate how hippocampal lesions in MTLE patients affect the impact of conflict resolution on memory encoding at a behavioral level and at the level of BOLD responses.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Incidental memory task\",\n",
      "    \"TaskDescription\": \"An incidental memory task of the previously seen faces was conducted outside of the MRI scanner.\",\n",
      "    \"DesignDetails\": \"About 30-45 minutes after the face-word Stroop task, all 120 faces that had been shown in the face-word Stroop task and 40 novel faces were presented in a random order. The faces were displayed in the center of a white screen for 2000ms, followed by an inter-trial interval of 2000ms. Subjects were asked to indicate whether they had seen the face before or not by pressing one of four buttons.\",\n",
      "    \"Conditions\": [\n",
      "        \"Seen before\",\n",
      "        \"Not seen before\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"retrieval rates\",\n",
      "        \"false alarm rates\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Face-word Stroop task\",\n",
      "    \"TaskDescription\": \"Participants performed a face-word Stroop task during functional magnetic resonance imaging (fMRI).\",\n",
      "    \"DesignDetails\": \"The task comprised face-word stimuli consisting of male and female faces overlaid by the words man or woman. The paradigm consisted of 120 faces (60 males and 60 females) previously presented in a familiarization task. Faces were displayed for 1500ms in the center of a white screen. An event-related design was used to identify separate trial-related Blood-oxygen-level-dependent (BOLD) responses. Trial onsets randomly varied with a pseudo-exponentially distributed SOA of 5000ms (75%), 7500ms (17%) and 10,000ms (8%). Both trial types were presented in a randomized order and occurred with a probability of 50%.\",\n",
      "    \"Conditions\": [\n",
      "        \"Congruent\",\n",
      "        \"Incongruent\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"response time\",\n",
      "        \"accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Approximately 30-45 minutes after familiarization.\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 7913329\n",
      "--- delavega-other ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"monetary incentive delay task (MIDT)\",\n",
      "        \"Monetary incentive delay task (MIDT)\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"a bet (a dollar, a cent, or no money, randomly intermixed) appeared on the screen at the beginning of each trial. After a randomized fore-period between 1 and 5\\u00a0s (uniform distribution), a target box was shown for a short period (response window, see below). Subjects were told to press a button as quickly as possible to collect the money (win) before the target box disappeared. An accurate trial was defined by a button press before disappearance of the target box. Otherwise, subjects would lose the bet, with the amount deducted from the total win. A premature button press prior to the appearance of the target box terminated the trial, and similarly resulted in loss. Feedback was shown on the screen after each trial to indicate the amount of money won or lost. Approximately 42% of all trials were dollar trials, 42% were cent trials, and \\u201cno money\\u201d constituted the remaining trials. The inter-trial-interval was 1.5\\u00a0s. The response window started at 300\\u00a0ms, and was stair-cased for each trial type (dollar/cent/no money) separately; for instance, if the subject succeeded at two successive dollar trials, the window decreased by 30\\u00a0ms, making it more difficult to win again; conversely, if a subject failed for two successive trials, the response window increased by 30\\u00a0ms, making it easier to win. We anticipated that the subjects would win in approximately 67% each for dollar and cent trials. Each subject completed two 10-min runs of the task. Across subjects, there were 184\\u2009\\u00b1\\u20094 (mean\\u2009\\u00b1\\u2009SD) trials in a study\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Examine sex differences in the neural responses to wins and losses and how individual reward and punishment sensitivity modulates these regional activities using fMRI and a variant of the Monetary Incentive Delay Task (MIDT).\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Sensitivity to Punishment and Sensitivity to Reward Questionnaire (SPSRQ)\",\n",
      "    \"TaskDescription\": \"Questionnaire assesses sensitivity to punishment and sensitivity to reward with behavioral impulsivity and avoidance scales.\",\n",
      "    \"DesignDetails\": \"The SPSRQ consists of 48 yes-no items divided equally into scales for sensitivity to reward and punishment. Scores are obtained by totaling the yes-responses for each scale.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"Sensitivity to reward score\",\n",
      "        \"Sensitivity to punishment score\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Monetary Incentive Delay Task (MIDT)\",\n",
      "    \"TaskDescription\": \"Participants press a button to collect rewards (dollar, cent, or nil) with a titrated reaction time window to achieve a success rate of approximately 67%.\",\n",
      "    \"DesignDetails\": \"In the MIDT, participants pressed a button as quickly as possible to collect rewards. A bet appeared at the beginning of each trial followed by a randomized fore-period (1-5s), and then a target box was shown for a brief time (response window starting at 300ms). If successful, feedback was provided, losing or gaining depending on the outcome. The response window adjusted (by 30ms) based on success or failure in previous trials to maintain an approximate 67% win rate. Each subject completed two 10-min runs.\",\n",
      "    \"Conditions\": [\n",
      "        \"Dollar trials\",\n",
      "        \"Cent trials\",\n",
      "        \"No money trials\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Reaction time\",\n",
      "        \"Accuracy\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"20 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To examine sex differences in the neural responses to wins and losses and how individual reward and punishment sensitivity modulates these regional activities.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Monetary Incentive Delay Task (MIDT)\",\n",
      "    \"TaskDescription\": \"Participants pressed a button to collect reward ($1, 1, or nil), with the reaction time window titrated across trials so participants achieved a success rate of approximately 67%.\",\n",
      "    \"DesignDetails\": \"In the MIDT, a bet (a dollar, a cent, or no money) appeared on the screen at the beginning of each trial. After a randomized fore-period between 1 and 5 seconds, a target box was shown for a short period. Subjects were instructed to press a button as quickly as possible to collect the money before the target box disappeared. An accurate trial was defined by a button press before the disappearance of the target box. The inter-trial interval was 1.5 seconds, and the response window started at 300 milliseconds, stair-cased for each trial type. Each subject completed two 10-minute runs of the task, with a total of 1844 trials across subjects.\",\n",
      "    \"Conditions\": [\n",
      "        \"Dollar\",\n",
      "        \"Cent\",\n",
      "        \"Nil\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Reaction Time\",\n",
      "        \"Accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"20 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 8107785\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Empathy for Pain Paradigm\",\n",
      "        \"Touch Paradigm\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"In this task, short-lasting (500\\u00a0ms) and individually calibrated painful or nonpainful electrical stimulation was delivered to participants, or to another person (a confederate of the experimenters). Trials were structured as follows: First, an arrow (2000\\u00a0ms) indicated the   target   (self vs. other) of the upcoming stimulus. The intensity of the upcoming stimulus was indicated by the color of this arrow (red: painful vs. green: nonpainful). After a jittered blank screen (3500\\u2009\\u00b1\\u20091500\\u00a0ms), the electrical stimulus (500\\u00a0ms) was delivered during simultaneous presentation of another visual delivery stimulus (1000\\u00a0ms). The latter consisted of a picture of the confederate\\u2019s face, shown with either a painful or a neutral expression, or, in case of self-directed stimulation, scrambled versions of these pictures were shown to control for visual stimulation. Depending on the stimulus category, these pictures were accompanied by either a red (painful) or green (nonpainful) flash in the lower right corner of the picture. The delivery cue was followed by a fixation cross (5000\\u2009\\u00b1\\u20092500\\u00a0ms), and an optional rating (self-directed: one rating question; other-directed: two rating questions; 6000-ms answering time per each question). After self-directed stimulation, participants rated their own pain (self-directed pain ratings), using the question \\u201cHow painful was this stimulus for you?\\u201d on a seven-point rating scale ranging from \\u201cnot at all\\u201d to \\u201cextremely painful.\\u201d After other-directed stimulation, participants rated the other person\\u2019s pain (other-directed pain ratings; \\u201cHow painful was this stimulus for the other person?\\u201d answered using the same seven-point rating scale as for the self-directed pain ratings), as well as their own unpleasantness during other-directed stimulation (unpleasantness ratings; \\u201cHow unpleasant did it feel when the other person was stimulated?\\u201d; seven-point scale, from \\u201cnot at all\\u201d to \\u201cextremely unpleasant\\u201d). Ratings were collected in about one third of the trials in a pseudorandomized fashion. Between trials, a fixation cross (2000\\u00a0ms) was presented. In sum, 15 trials per condition (i.e., self-directed pain/no pain; other-directed pain/no pain) were presented. Participants were instructed to empathize with the other person. \",\n",
      "        \"Following the empathy for pain paradigm, we applied a touch paradigm ( ;  ) including 15 pleasant, 15 unpleasant and 15 neutral stimuli in pseudo-randomized order (see also  ). This paradigm consisted of two separate runs: In the first run (self-directed affective touch), the participant was stimulated to measure behavioral responses and brain activation related to the first-hand experience of affective touch. In the second run (empathy for affective touch) a confederate acting as a second participant was supposedly undergoing affective touch, and participants were instructed to empathize with her feelings. In every single self-directed trial, visual presentation of an object was accompanied by simultaneous stroking of the left palm at 1\\u00a0Hz for 2\\u00a0s in proximal-to-distal direction with a material whose touch resembled the touch of the object depicted on the screen. For example, touching the participant\\u2019s hand with down feathers was accompanied by the picture of a chick to elicit a pleasant affective touch experience. The stimuli had been selected in extensive pretesting based on maximum agreement among participants in terms of congruency between visual and somatosensory stimulus and emotional responses (see   for paradigm validation test). In one third of the trials (5 per condition), participants were asked to rate the stimulation in that trial on a 9-point scale ranging from very unpleasant (left extreme of the scale) to very pleasant (right extreme) for either themselves or, supposedly, for the other participant (i.e., the confederate). Each single trial consisted of a jittered fixation cross (5000\\u2009+\\u2009\\u22122000\\u00a0ms), followed by visuo-tactile stimulation (2000\\u00a0ms) and a jittered blank screen (1500\\u2009+\\u2009\\u22121000\\u00a0ms). In trials with ratings, the rating was presented after the jittered blank screen for 5000\\u00a0ms and was followed by another jittered blank screen (1500\\u2009+\\u2009\\u22121000\\u00a0ms). Other-directed trials were identical apart from the absence of tactile stimulation of the participant, and the instruction that participants should empathize with their feelings. \",\n",
      "        \"self-unpleasant: control group\\u2009>\\u2009placebo group]\",\n",
      "        \"other-unpleasant: control group\\u2009>\\u2009placebo group\",\n",
      "        \"self-pleasant: control group\\u2009>\\u2009placebo group\",\n",
      "        \"other-pleasant: control group\\u2009>\\u2009placebo group\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To test whether shared neural representations between first-hand pain and empathy for pain are pain-specific or extend to empathy for unpleasant affective touch, and to investigate the effects of placebo analgesia on these experiences using functional magnetic resonance imaging and psychopharmacological experiments.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Empathy for Pain Paradigm\",\n",
      "    \"TaskDescription\": \"Examining empathy by delivering painful or nonpainful electrical stimulation to participants or a confederate, with the task measuring participants' empathy for pain.\",\n",
      "    \"DesignDetails\": \"In this task, short-lasting (500ms) painful or nonpainful electrical stimulation was delivered to participants or another person (confederate). Trials start with an arrow (2000ms) indicating the target of the upcoming stimulus, with color indicating stimulus intensity. A jittered blank screen follows (3500-1500ms), then a 500ms electrical stimulus with a visual cue (1000ms) of a confederate's face with expressions. Participants rated their own and confederate's pain using a seven-point scale on some trials.\",\n",
      "    \"Conditions\": [\n",
      "        \"Self-directed pain\",\n",
      "        \"Self-directed no pain\",\n",
      "        \"Other-directed pain\",\n",
      "        \"Other-directed no pain\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Self-directed pain ratings\",\n",
      "        \"Other-directed pain ratings\",\n",
      "        \"Unpleasantness ratings\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"15 minutes\"\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Touch Paradigm\",\n",
      "    \"TaskDescription\": \"Measures first-hand experience and empathy for affective touch through visual and tactile stimuli, with stimuli categorized as pleasant, unpleasant, or neutral.\",\n",
      "    \"DesignDetails\": \"Includes self-directed and empathy touch trials. Visual stimuli paired with tactile stroking of the hand (2s, 1Hz) using materials matching on-screen images. Participants rate touch experiences for themselves or a confederate on a 9-point scale in one-third of trials. Each trial includes a jittered fixation, visuo-tactile stimulation, and a rating screen.\",\n",
      "    \"Conditions\": [\n",
      "        \"Pleasant\",\n",
      "        \"Unpleasant\",\n",
      "        \"Neutral\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Touch pleasantness and unpleasantness ratings\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"10 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To test whether shared neural representations between first-hand pain and empathy for pain are pain-specific or extend to empathy for unpleasant affective touch as well.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Empathy for Pain Paradigm\",\n",
      "    \"TaskDescription\": \"This task involved delivering short-lasting painful or nonpainful electrical stimulation to participants or a confederate, with participants rating their own pain and the pain of the other person.\",\n",
      "    \"DesignDetails\": \"Trials were structured with an arrow indicating the target (self vs. other), followed by a stimulus indicating pain intensity. After a jittered blank screen, the electrical stimulus was delivered, accompanied by a visual stimulus of the confederate's face. Participants rated their pain and the other person's pain after the stimulus, with 15 trials per condition (self-directed pain/no pain; other-directed pain/no pain).\",\n",
      "    \"Conditions\": [\n",
      "        \"self-directed pain\",\n",
      "        \"self-directed no pain\",\n",
      "        \"other-directed pain\",\n",
      "        \"other-directed no pain\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"pain ratings\",\n",
      "        \"unpleasantness ratings\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"15 minutes\"\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Touch Paradigm\",\n",
      "    \"TaskDescription\": \"This paradigm included self-directed and empathy for affective touch tasks, where participants experienced and empathized with affective touch stimuli.\",\n",
      "    \"DesignDetails\": \"The task consisted of two runs: the first run involved self-directed affective touch with visual presentation of objects and simultaneous stroking of the left palm. The second run involved empathy for affective touch, where participants empathized with a confederate receiving touch. Each run included 15 trials per condition (pleasant, unpleasant, neutral) in a pseudo-randomized order.\",\n",
      "    \"Conditions\": [\n",
      "        \"self-directed pleasant touch\",\n",
      "        \"self-directed unpleasant touch\",\n",
      "        \"self-directed neutral touch\",\n",
      "        \"other-directed pleasant touch\",\n",
      "        \"other-directed unpleasant touch\",\n",
      "        \"other-directed neutral touch\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"pleasantness ratings\",\n",
      "        \"unpleasantness ratings\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"20 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 8318202\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"attentive listening\",\n",
      "        \"word repetition\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"During attentive listening, participants were asked to stay alert, still, and keep their eyes focused on a fixation cross while listening to a sequence of auditory sounds, including words, silence, and noise (single-channel noise vocoded words)\",\n",
      "        \"During word repetition, participants were asked to do the same as in attentive listening, with the addition of repeating the word they just heard aloud. Participants were instructed to repeat the words following the volume acquisition after each word\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate age-related differences in auditory cortex activity during spoken word recognition using fMRI in young and older adults.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Attentive Listening Task\",\n",
      "    \"TaskDescription\": \"Participants listen attentively to spoken words without giving any motor response.\",\n",
      "    \"DesignDetails\": \"Participants listened to a sequence of 120 auditory stimuli (words, silence, and noise) over four scanning blocks while keeping still and focusing on a fixation cross. The fMRI task alternated between blocks of attentive listening and word repetition, counterbalanced among participants.\",\n",
      "    \"Conditions\": [\n",
      "        \"Words\",\n",
      "        \"Silence\",\n",
      "        \"Noise\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Word Repetition Task\",\n",
      "    \"TaskDescription\": \"Participants are required to repeat out loud the spoken words they hear.\",\n",
      "    \"DesignDetails\": \"Participants listened to a sequence of 120 auditory stimuli (words, silence, and noise) and repeated the words aloud during scanning, alternating the task type with attentive listening in blocks. Spoken responses were recorded for accuracy offline.\",\n",
      "    \"Conditions\": [\n",
      "        \"Words\",\n",
      "        \"Silence\",\n",
      "        \"Noise\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate age-related differences in brain networks supporting spoken word recognition in young and older adults during attentive listening and word repetition tasks.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Attentive Listening\",\n",
      "    \"TaskDescription\": \"Participants listened to a sequence of auditory sounds, including words, silence, and noise, while remaining still and focused on a fixation cross.\",\n",
      "    \"DesignDetails\": \"Participants were instructed to stay alert and keep their eyes focused on a fixation cross while listening to a sequence of auditory sounds. The task involved no motor response, and participants were presented with words, silence, and noise in a counterbalanced manner across scanning blocks.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Word Repetition\",\n",
      "    \"TaskDescription\": \"Participants repeated the words they heard aloud after each word was presented.\",\n",
      "    \"DesignDetails\": \"During the word repetition task, participants were instructed to repeat the word they just heard aloud after each word was presented. The task was performed in alternating blocks with the attentive listening task, and participants practiced the task prior to scanning.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 8342928\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Taylor Aggression Paradigm\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"To investigate the role of MT in altering neural signatures of retaliatory aggression, we administered a version of the TAP adapted for the fMRI scanner (e.g.,  ;  ). Recent research supports flexible use of the TAP, as it has shown to be psychometrically robust to variations in sampling, laboratory settings, and analytical approaches ( ;  ). Participants were informed that they would play an online computerized game with a participant situated in another lab. Participants were told that they would compete in multiple trials of a reaction time competition, in which the loser of each trial received an aversive noise blast through headphones, at one of four noise levels chosen by the other player. In reality, participants played against a preset computer program designed to produce four volume levels of white noise, with volume settings ranging from 1 (60 dB) to 4 (105 dB), in 22.5 dB intervals. The TAP consisted of 16 trials ( ). Each trial began with a fixation phase, followed by a decision phase, in which participants selected the volume of noise blast that their partner would receive if their partner lost the reaction time trial. Participants then viewed a fixation cross with a jittered duration (0.5/1.0/1.5 s) before the competition phase, during which participants were required to quickly press a button when a red square target was shown on-screen (5 s). Participants then viewed their opponent\\u2019s (pre-programmed) volume setting. This time point of notification, when the participant perceived the opponent\\u2019s intended noise blast setting, was modeled as the provocation phase (see  ). Finally, in the outcome phase, participants learned whether they won or lost the trial. The \\u201closing\\u201d outcome phase, modeled as the punishment phase, subjected participants to a 5 s noise blast delivered by their opponent. Trials were characterized as retaliatory if they followed trials with high provocation (noise levels 3 or 4) and non-retaliatory if they followed trials with low provocation (levels 1 or 2). The 16-trial task contained eight retaliatory and eight non-retaliatory trials that were randomized across participants. Wins and losses were also randomly ordered across participants. Participants practiced the task first outside of the scanner to provide an opportunity for subjective evaluation of each noise level prior to neuroimaging assessment. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate how mindfulness training impacts functional brain physiology in the regulation of reactive aggression using fMRI during a retaliatory aggression task.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Taylor Aggression Paradigm (TAP)\",\n",
      "    \"TaskDescription\": \"A laboratory task where participants can respond to provocation by choosing whether or not to retaliate in a reaction time competition.\",\n",
      "    \"DesignDetails\": \"The task consists of 16 trials where participants compete against a preset computer program. Each trial includes several phases: decision phase (choosing noise level to administer), provocation phase (viewing opponent's chosen noise level), and outcome phase (determining win or loss outcome). Behavioral responses are recorded based on the chosen noise levels in response to high or low provocation levels.\",\n",
      "    \"Conditions\": [\n",
      "        \"Decision phase\",\n",
      "        \"Provocation phase\",\n",
      "        \"Outcome phase (punishment)\",\n",
      "        \"Outcome phase (reward)\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"retaliatory aggression response\",\n",
      "        \"noise level selection\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Mixed\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"16 trials\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: The current pilot study examined how mindfulness training (MT) impacts functional brain physiology in the regulation of reactive aggression.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Taylor Aggression Paradigm (TAP)\",\n",
      "    \"TaskDescription\": \"A 16-trial game in which participants could respond to provocation by choosing whether or not to retaliate in the next round.\",\n",
      "    \"DesignDetails\": \"The TAP consisted of 16 trials. Each trial began with a fixation phase, followed by a decision phase, in which participants selected the volume of noise blast that their partner would receive if their partner lost the reaction time trial. Participants then viewed a fixation cross with a jittered duration (0.5/1.0/1.5 s) before the competition phase, during which participants were required to quickly press a button when a red square target was shown on-screen (5 s). Participants then viewed their opponents (pre-programmed) volume setting. The losing outcome phase subjected participants to a 5 s noise blast delivered by their opponent. Trials were characterized as retaliatory if they followed trials with high provocation (noise levels 3 or 4) and non-retaliatory if they followed trials with low provocation (levels 1 or 2).\",\n",
      "    \"Conditions\": [\n",
      "        \"High provocation (noise levels 3 or 4)\",\n",
      "        \"Low provocation (levels 1 or 2)\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"45 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 8564184\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Identify structural brain alterations in chronic visceral pain patients (UC and IBS) and investigate associations with visceral symptoms and chronic stress.\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To elucidate structural brain alterations in patients with chronic visceral pain, specifically comparing patients with ulcerative colitis and irritable bowel syndrome to healthy controls, and to assess associations with symptom severity and chronic stress.\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 8597975\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"antisaccade\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"We implemented an event-related design, mixing pro and antisaccades to faces and cars.   represents the time course of the task. Each trial started with a fixation cross displayed for periods ranging from 2000 to 8000\\u00a0ms (sampled on an exponential distribution,  ), to introduce some jitter between trials onset and thereby improve design efficiency. A visual cue then appeared for 340\\u00a0ms on the screen. The cue was a central disc (54\\u2009\\u00d7\\u200954 pixels, 1\\u00b0 visual angle) whose color (red or green) defined the nature of the saccadic task that should follow. A blank screen lasting 200\\u00a0ms marked the transition between the visual cue (central disc) and the visual target stimulus (peripheral image) that was presented 10\\u00b0 to the left or 10\\u00b0 to the right of the screen center for 1000\\u00a0ms. A green cue prompted the participants to look toward the appearing stimulus (prosaccade) and a red cue signaled that participants should look to the opposite side (to the mirror location) of the appearing stimulus (antisaccade). Participants were instructed to execute pro and antisaccades as quickly as possible. The experiment consisted of six 4-min long runs. Each run comprised 20 face and 20 car stimuli (40 trials in total) with 50% pro and 50% antisaccade instructions, randomized to the left and right hemi-field. We determined 6 different schedules (one for each run) using optseq2 (version 2.0, available at   http://surfer.nmr.mgh.harvard.edu/optseq  ), optimizing the order and timing of the 8 conditions (anti- or prosaccades toward faces or cars to the left or right side of the hemi field). The order of the 6 different schedules was counterbalanced across participants. We explained the task to the participants outside the scanner and had them perform a 2-min long practice session inside the scanner before the experiment started to ensure that the instruction was properly understood. Participants took breaks varying from a few seconds up to 2\\u00a0min between runs, during which we reminded them of the instructions. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Characterize developmental changes in brain activity related to the influence of a social stimulus on cognitive control, specifically inhibitory control.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Antisaccade Task\",\n",
      "    \"TaskDescription\": \"Participants are asked to inhibit their reflexive eye movement to an abruptly appearing peripheral visual target and reprogram a saccade in the opposite direction, using faces or cars as stimuli.\",\n",
      "    \"DesignDetails\": \"We implemented an event-related design, mixing pro and antisaccades to faces and cars. Each trial started with a fixation cross displayed for periods ranging from 2000 to 8000ms. A visual cue then appeared for 340ms, followed by a blank screen for 200ms before the target stimulus. The experiment consisted of six 4-min long runs with 20 face and 20 car stimuli per run, randomized to the left and right hemi-field. Participants performed a 2-min practice session to ensure understanding.\",\n",
      "    \"Conditions\": [\n",
      "        \"Pro saccade\",\n",
      "        \"Anti saccade\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"reaction time\",\n",
      "        \"antisaccade error rate\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"6 runs of 4 minutes each\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To characterize developmental changes in brain activity related to the influence of a social stimulus on cognitive control and more specifically on inhibitory control during an antisaccade task with faces and cars as visual stimuli.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Antisaccade Task\",\n",
      "    \"TaskDescription\": \"Participants are asked to inhibit their reflexive eye movement to an abruptly appearing peripheral visual target and to reprogram a saccade in the opposite direction.\",\n",
      "    \"DesignDetails\": \"The task was designed as an event-related design, mixing pro and antisaccades to faces and cars. Each trial started with a fixation cross displayed for periods ranging from 2000 to 8000ms, followed by a visual cue for 340ms that indicated the type of saccade to perform. A blank screen lasting 200ms marked the transition to the visual target stimulus presented for 1000ms. The experiment consisted of six 4-min long runs, each comprising 20 face and 20 car stimuli, with 50% pro and 50% antisaccade instructions, randomized to the left and right hemi-field.\",\n",
      "    \"Conditions\": [\n",
      "        \"Antisaccade to face\",\n",
      "        \"Antisaccade to car\",\n",
      "        \"Prosaccade to face\",\n",
      "        \"Prosaccade to car\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"reaction time\",\n",
      "        \"accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"24 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 8764488\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Toulouse n\\u2010back Task (TNT)\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"The Toulouse n\\u2010back Task (TNT) was implemented in MATLAB (MathWorks) using the Psychophysics Toolbox extensions (Brainard,\\u00a0 ; Kleiner, Brainard, & Pelli,\\u00a0 ; Pelli & Vision,\\u00a0 ). The task is described in detail in a previous publication (Mandrick et al.,\\u00a0 ). The task was developed to combine a classical n\\u2010back task with mental arithmetic. Instead of memorizing and comparing unique items, as in the classical n\\u2010back task, the participants had to memorize and to compare the results of arithmetic operations, computed beforehand. Arithmetic operations were either additions or subtractions. All numbers were multiples of five (e.g., 15\\u2009+\\u200940, 90\\u201335). The arithmetic operations (trials) were presented for 2.5\\u00a0s, followed by an interstimulus\\u2010interval of 0.5\\u00a0s. Volunteers were required to compute the result of the arithmetic operations and compare it with either a fixed number (0\\u2010back) or the result obtained two trials before (2\\u2010back). In the 0\\u2010back condition, the \\u201ctarget\\u201d fixed number was \\u201c50.\\u201d Participants were therefore asked to press a specific button when the result of the operation was 50. In the 2\\u2010back condition, the participants were asked to press the button whenever the result of the arithmetic operation was identical to the one presented two trials ago (\\u201cmatch\\u201d), see Figure\\u00a0 . \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: The present fMRI study aimed at highlighting patterns of brain activations and autonomic activity when confronted with high mental workload and the threat of auditory stressors.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Toulouse n-back Task (TNT)\",\n",
      "    \"TaskDescription\": \"The task combines a classical n-back task with mental arithmetic to study cognitive workload and response to auditory stressors.\",\n",
      "    \"DesignDetails\": \"The TNT task consists of 0-back and 2-back levels, with mental arithmetic calculations. Each active block has 12 trials with arithmetic operations and target identification, interleaved with rest blocks. A blocked design was used, with task conditions counterbalanced with safe and threat contexts.\",\n",
      "    \"Conditions\": [\n",
      "        \"0-back safe\",\n",
      "        \"0-back threat\",\n",
      "        \"2-back safe\",\n",
      "        \"2-back threat\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"performance success\",\n",
      "        \"reaction times\",\n",
      "        \"d-prime\",\n",
      "        \"heart rate\",\n",
      "        \"pupil diameter\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"1 hour and 15 minutes across multiple runs\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: The present fMRI study aimed at highlighting patterns of brain activations and autonomic activity when confronted with high mental workload and the threat of auditory stressors.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Toulouse nback Task (TNT)\",\n",
      "    \"TaskDescription\": \"The task combines a classical nback task with mental arithmetic, requiring participants to memorize and compare results of arithmetic operations.\",\n",
      "    \"DesignDetails\": \"The active blocks consisted of 12 trials and lasted 36s, interleaved with 24s rest blocks. Participants responded to targets and nontargets by pressing one of two different buttons. The task included two levels of difficulty (0back and 2back) and was presented in a blocked design across five functional runs (two safe runs and three threat runs).\",\n",
      "    \"Conditions\": [\n",
      "        \"0back\",\n",
      "        \"2back\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"percentage of correct responses\",\n",
      "        \"reaction times\",\n",
      "        \"dprime\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Approximately 1 hour and 15 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 8857499\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"giving task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"adolescents divided either a small or large number of coins (giving magnitude manipulation) between themselves and either a friend or unfamiliar peer (target manipulation) in an audience or anonymous context (peer presence manipulation)\",\n",
      "        \"#### Giving magnitude: giving small and large amounts \\n  \\nTo assess the neural correlates of giving behavior, we used a modified fMRI version of the Dictator Game previously validated in adults ( ,  ). Participants divided 7 coins between themselves and another person - who could not reject the decision - in either a small or large giving condition. Giving was operationalized as the number of given coins. In the small giving condition, participants could give away 1, 2, or 3 out of 7 coins. In the large giving condition, participants could give away 4, 5, or 6 out of 7 coins. Participants could not give 0 coins, 7 coins, or make an equal split to warrant comparability between the small and large giving condition (see  ).   \\n(A) The small and large giving conditions of the task, in which participants could give away 1, 2, or 3 coins, or 4, 5, or 6 coins, respectively (depicted in orange). Participants would keep the remainder of the 7 coins (depicted in yellow) to themselves. The name of the target (which could either be a friend or unfamiliar peer) was displayed at the top of the screen for each trial. (B) In two out of four blocks of the giving task, participants made anonymous giving choices. In the other two blocks, participants were aware that the peer audience depicted on the screen would observe their choices later in time (i.e., the audience condition). Note that the screens indicating whether blocks were audience or anonymous were only displayed at the start of each block, not during trials. Audience and anonymous blocks were presented in counterbalanced order. \\n  Fig. 1   \\n\\n\\n#### Familiarity of the target: friend and unfamiliar peer \\n  \\nThe target of giving was either the participant\\u2019s closest friend (same-sex, similar-age) or an unfamiliar peer (same-sex, similar-age, anonymized participant of the same study). For each trial, the name of the target was displayed at the top of the screen (see  ). Participants were instructed that the coins they divided in each trial were worth actual money (i.e., 20 eurocents each) and that the computer would randomly select a few trials of the task to determine the payout of the participant, friend, and unfamiliar other. Accordingly, participants received a payment for themselves (  M   = \\u20ac.80,   SD   = \\u20ac.12) and their friend (  M   = \\u20ac.68,   SD   = \\u20ac.08), and experimenters transferred the payment to the unfamiliar other (i.e., another participant of the current study;   M   = \\u20ac.61,   SD   = \\u20ac.09). \\n\\n\\n#### Peer presence: anonymous and audience giving \\n  \\nTo assess the effects of peer presence on giving, the task consisted of two blocks in which participants made anonymous choices, and two blocks in which participants\\u2019 choices were evaluated by peers later in time (see  ). The order of anonymous and audience blocks was counterbalanced across participants. In a practice session prior to the MRI session, participants viewed a video clip of six peers (three males and three females, aged 9\\u201319) with neutral expressions. To the awareness of participants, these six peers were invited after study completion to observe and evaluate choices of participants. Trials in the anonymous blocks were not shown to anyone, as \\u2013 beknown to participants - experimenters covered the screen in the control room. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: This pre-registered fMRI study investigated behavioral and neural correlates of adolescents small versus large size giving in different social contexts related to target and peer presence.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Modified Dictator Game\",\n",
      "    \"TaskDescription\": \"Participants divided coins between themselves and another person in either a small or large giving condition, with target and peer presence manipulations.\",\n",
      "    \"DesignDetails\": \"The task involved two giving conditions (small and large) where participants could give away 1, 2, or 3 coins in the small condition, and 4, 5, or 6 coins in the large condition. The target was either a friend or unfamiliar peer. Conditions also varied by peer presence: anonymous or audience. The task consisted of four blocks with 40 trials each, totaling 160 trials. Trials started with a jittered fixation and participants had to respond within 2000ms. The order of blocks was counterbalanced.\",\n",
      "    \"Conditions\": [\n",
      "        \"small giving\",\n",
      "        \"large giving\",\n",
      "        \"giving to friend\",\n",
      "        \"giving to unfamiliar peer\",\n",
      "        \"anonymous\",\n",
      "        \"audience\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"neural activation (fMRI BOLD signal)\",\n",
      "        \"percentage of coins given\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"15 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: This pre-registered fMRI study investigated behavioral and neural correlates of adolescents' small versus large size giving in different social contexts related to target (i.e., giving to a friend or unfamiliar peer) and peer presence (i.e., anonymous versus audience giving).\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Giving Task\",\n",
      "    \"TaskDescription\": \"Participants divided either a small or large number of coins between themselves and either a friend or unfamiliar peer in an audience or anonymous context.\",\n",
      "    \"DesignDetails\": \"The task included two giving conditions, in which participants could give away either a relatively small (i.e., less than half of seven coins; low-costly) or large amount (i.e., more than half of seven coins; high-costly). The task consisted of four blocks with 40 trials per block, resulting in 160 trials in total. All combinations of conditions (giving magnitude, target, and peer presence) were equally distributed across trials. The order of trials was optimized using OptSeq. The total length of the task, excluding instructions and pauses, was approximately 15 minutes.\",\n",
      "    \"Conditions\": [\n",
      "        \"Small giving (1, 2, or 3 coins)\",\n",
      "        \"Large giving (4, 5, or 6 coins)\",\n",
      "        \"Giving to a friend\",\n",
      "        \"Giving to an unfamiliar peer\",\n",
      "        \"Anonymous giving\",\n",
      "        \"Audience giving\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Number of coins given\",\n",
      "        \"Reaction time\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Approximately 15 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 9148994\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"spatial contextual memory task\"\n",
      "    ],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: This study examined age effects on consolidation-related neural mechanisms and their susceptibility to interference using functional magnetic resonance imaging data from younger and older healthy participants.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Resting-state fMRI\",\n",
      "    \"TaskDescription\": \"The task involves resting-state fMRI measurements obtained at baseline, after initial encoding, and after a modulatory task to study memory consolidation.\",\n",
      "    \"DesignDetails\": \"In each experiment, a spatial contextual memory task was implemented, consisting of an encoding, a consolidation, and a retrieval phase. Resting periods occurred before encoding, after encoding, and before retrieval. Participants were instructed to keep their eyes closed and relax without falling asleep during the acquisition of the resting-state fMRI data.\",\n",
      "    \"Conditions\": [\n",
      "        \"Control condition\",\n",
      "        \"Interference condition\",\n",
      "        \"Reminder condition\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": true,\n",
      "    \"RestingStateMetadata\": \"Resting-state fMRI measurements were obtained at three points: baseline (before any task), immediately after the encoding task, and after the modulatory task. Participants were instructed to keep their eyes closed and not fall asleep.\",\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"7 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: This study examined age effects on consolidation-related neural mechanisms and their susceptibility to interference using functional magnetic resonance imaging data from younger and older healthy participants.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Memory Performance Assessment\",\n",
      "    \"TaskDescription\": \"Participants' memory performance was operationalized by the percentage of correctly retrieved object-location associations.\",\n",
      "    \"DesignDetails\": \"A mixed-effects ANOVA was used to evaluate the effects of age and interference on memory performance across three conditions: control, reminder, and interference.\",\n",
      "    \"Conditions\": [\n",
      "        \"Control\",\n",
      "        \"Reminder\",\n",
      "        \"Interference\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Percentage of correct responses\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Spatial Contextual Memory Task\",\n",
      "    \"TaskDescription\": \"Participants performed a spatial contextual memory task consisting of an encoding, consolidation, and retrieval phase.\",\n",
      "    \"DesignDetails\": \"The encoding task consisted of 64 trials where participants memorized objects presented in specific positions. Each stimulus was shown for 3 seconds with a variable inter-trial interval of 2 to 12 seconds. The consolidation phase was interrupted by a modulatory task that varied in its interference with the initial encoding. The three conditions were control (no interference), reminder (repeated presentation without interference), and interference (repeated presentation with interference).\",\n",
      "    \"Conditions\": [\n",
      "        \"Control\",\n",
      "        \"Reminder\",\n",
      "        \"Interference\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fALFF\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 9202476\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: The aim of the study was to evaluate correlation of sleep quality with visual attention and search, functional, and tracts properties of the DAN and VAN.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Match-to-sample (MTS) task\",\n",
      "    \"TaskDescription\": \"A computerized task assessing visual search and attention using a complex figure matching.\",\n",
      "    \"DesignDetails\": \"Subjects were presented with a complex figure in the middle of the screen, followed by a few patterns in the periphery. The task began with two patterns and increased to eight patterns in the final trials. A total of 48 trials were conducted, calculating total correct, mean reaction time (RT), and mean RT change from 2 to 8 pattern trials.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"Total correct responses\",\n",
      "        \"Mean reaction time\",\n",
      "        \"Mean RT change\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Resting-state fMRI\",\n",
      "    \"TaskDescription\": \"Examine the resting state activity of functional networks related to visual attention (DAN and VAN).\",\n",
      "    \"DesignDetails\": \"Functional data were first resampled to 2 mm voxel size, motion-corrected, centered, and slice timing-corrected. Independent component analysis (ICA) was applied to identify resting state networks. Spatial match to the DAN and possibly the VAN was computed using a template overlap.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": true,\n",
      "    \"RestingStateMetadata\": \"resting-state fMRI sequence with subjects with open eyes, duration of 6:08 mins\",\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"6:08 mins\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: The aim of the study was to evaluate correlation of sleep quality with visual attention and search, functional, and tracts properties of the DAN and VAN.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"match-to-sample (MTS) task\",\n",
      "    \"TaskDescription\": \"This computerized task assesses visual search and attention by presenting a complex figure and requiring subjects to match it with patterns shown in the periphery.\",\n",
      "    \"DesignDetails\": \"In this computerized task, subjects were presented with a complex figure in the middle of the screen. Then, a few patterns were shown in the periphery, from which one was matched with the presented pattern. In the first trials, two patterns were presented in the periphery, and it was increased to eight patterns in the final trials. A total of 48 trials were conducted for each subject.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"total correct\",\n",
      "        \"mean reaction time (RT)\",\n",
      "        \"mean RT change from 2 to 8 pattern trials\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 9261172\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"music listening\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"The fMRI task consisted of 24 trials altogether. In each trial, participants were first presented with a musical stimulus (lasting 20\\u00a0s), then they were given the task of rating how familiar they found the music to be (familiarity rating lasted 2\\u00a0s), and how much they liked the music (liking rating also lasted 2\\u00a0s). Musical stimuli for the MRI task consisted of 24 different audio excerpts. Each auditory stimulus was from one of the following three categories: participant self-selected music (6/24 stimuli), other-selected (researcher-selected) music including well-known excerpts spanning multiple musical genres  (10/24 stimuli) and novel music spanning multiple genres (8/24 stimuli). A list of the researcher-selected musical selections is given in Supplementary Materials Table  . Stimuli were presented in a randomized order, and participants made ratings of familiarity and liking on the scales of 1 to 4: for familiarity: 1\\u2009=\\u2009very unfamiliar, 2\\u2009=\\u2009unfamiliar, 3\\u2009=\\u2009familiar, 4\\u2009=\\u2009very familiar; for liking: 1\\u2009=\\u2009hate, 2\\u2009=\\u2009neutral, 3\\u2009=\\u2009like, 4\\u2009=\\u2009love. Participants made these ratings by pressing a corresponding button on a button-box (Cambridge Research Systems) inside the scanner. Participants wore MR-compatible over-the-ear headphones (Cambridge Research Systems) over musician-grade silicone ear plugs during MRI data acquisition. The spatial mapping between buttons and the numerical categories of ratings were counterbalanced between participants to reduce any systematic association between familiarity or liking and the motor activity resulting from making responses. This fMRI task was completed before and after the MBI. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: The study aims to investigate the effects of a music-based intervention (MBI) on the activity and connectivity of the brain's auditory and reward systems in cognitively unimpaired older adults, with a focus on changes following an 8-week personalized MBI.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Music Listening Task\",\n",
      "    \"TaskDescription\": \"Participants listened to musical stimuli and rated each on familiarity and liking.\",\n",
      "    \"DesignDetails\": \"The task consisted of 24 trials where participants were presented with a musical stimulus for 20 seconds, then rated familiarity and liking of the music for 2 seconds each. Ratings were made on a scale from 1 to 4 for both familiarity and liking. Participants listened to six self-selected, ten other-selected well-known, and eight novel music excerpts. Stimuli were randomly presented.\",\n",
      "    \"Conditions\": [\n",
      "        \"Self-selected music\",\n",
      "        \"Researcher-selected music\",\n",
      "        \"Novel music\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"familiarity rating\",\n",
      "        \"liking rating\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"11.4 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate the effects of music liking, familiarity, and self-selection on reward processing before and after an 8-week music-based intervention in cognitively unimpaired older adults.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Music Listening Task\",\n",
      "    \"TaskDescription\": \"Participants listened to musical excerpts and rated their familiarity and liking of each piece.\",\n",
      "    \"DesignDetails\": \"The fMRI task consisted of 24 trials. In each trial, participants were presented with a musical stimulus lasting 20 seconds, followed by a 2-second familiarity rating and a 2-second liking rating. The stimuli included self-selected music, researcher-selected well-known music, and novel music, presented in a randomized order.\",\n",
      "    \"Conditions\": [\n",
      "        \"Self-selected music\",\n",
      "        \"Researcher-selected music\",\n",
      "        \"Novel music\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"Familiarity rating\",\n",
      "        \"Liking rating\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"11.4 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 9308012\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Trust Game task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"participants played with a computer, a stranger, and a close friend who accompanied them to the experiment.\",\n",
      "        \"MRI participants were told that they would be playing a game called the investment game in real time with their friend, the stranger and a computer partner. On a given trial of the task, the MRI participant would play with one of their three partners as indicated by a photo and name presented on the screen. Participants were instructed that they would start each trial with $8 and that they would have a choice between sending (investing) different proportions of that $8 to their partner on a given trial. The amounts that could be sent varied on a trial to trial basis, ranging from $0-$8. Participants would have up to 3 s to indicate via a button press on an MRI compatible response box which of the two investment options they preferred. Participants were instructed that whatever amount they chose to invest would be multiplied by a factor of 3 (i.e., an investment of $6 would become $18 for the partner), and that their partner could decide to split the multiplied amount evenly with them (reciprocate) or keep it all for themselves (defect). Upon entering their response, participants would see a screen that said \\u2018waiting\\u2019, (1.5 s) during which time they believed that their decision was being presented to their partner in another room in the research suite. After the waiting screen, a variable ISI was presented (mean = 1.42 s), and participants were then notified (2 s) whether their partner decided to split that amount evenly with them (reciprocate) or keep (defect) all of the money. Unbeknownst to participants, all outcomes were predetermined, and all partners were preprogrammed to reciprocate 50% of the time as per our previous work ( ). \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate age-related differences in the effects of social closeness on trust behavior and the neural representation of reciprocity using an economic trust game.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Trust Game Task\",\n",
      "    \"TaskDescription\": \"Participants played an economic trust game as the investor with three partners (friend, stranger, and computer) while undergoing fMRI.\",\n",
      "    \"DesignDetails\": \"Participants played a game called the investment game with different partners as indicated by a photo and name presented on the screen. Each trial started with $8 and participants chose between sending different proportions of that $8 to their partner. After investing, amounts were multiplied by 3, and partners could choose to reciprocate or defect. Outcomes were predetermined, with partners reciprocating 50% of the time. The task included up to 5 runs, each with 36 trials (12 trials per partner).\",\n",
      "    \"Conditions\": [\n",
      "        \"Reciprocate\",\n",
      "        \"Defect\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD responses\",\n",
      "        \"Task-dependent connectivity\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"90 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To systematically investigate age-related differences in the effects of social closeness on trust behavior and the neural representation of reciprocity.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Trust Game Task\",\n",
      "    \"TaskDescription\": \"Participants played an economic trust game as the investor with three partners (friend, stranger, and computer) who played the role of investee.\",\n",
      "    \"DesignDetails\": \"Participants started each trial with $8 and had a choice between sending (investing) different proportions of that $8 to their partner. The amounts that could be sent varied on a trial to trial basis, ranging from $0-$8. Participants had up to 3 seconds to indicate their investment choice. After a waiting screen, they were notified whether their partner decided to split the amount (reciprocate) or keep it all (defect). Each run of the task consisted of 36 trials, with 12 trials per partner.\",\n",
      "    \"Conditions\": [\n",
      "        \"Friend\",\n",
      "        \"Stranger\",\n",
      "        \"Computer\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"Investment amount\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"90 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 9454014\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Food Cue Reactivity Task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"It consisted of 44 food images [22 high energy-dense foods (HED), e.g., ice cream, cookies; 22 low energy-dense foods (LED), e.g., salad, fruit] matched on valence, arousal, image complexity, brightness, and hue, and 44 degraded images to serve as a visual baseline. Objective values of image properties were obtained with a photo editing program (GIMP, Berkeley, CA). Matching was confirmed by employee ratings. Degraded images were created from the food images using Image Shuffle (San Diego, CA). To improve signal in the primary task condition and contrast of interest, food images were presented twice each; degraded images were presented once. Each picture was presented for 1,750 ms followed by a fixation cross presented for 250\\u20134,250 ms. Participants were asked to indicate whether they \\u201cliked,\\u201d \\u201cdisliked,\\u201d or felt \\u201cneutral\\u201d about each image by pressing a corresponding button within 2,000 ms of image presentation; ratings and reaction times were logged via a fiber-optic response box. Total duration of the task was 5:54 min, and included an initial 9 s fixation period to allow for magnetization stabilization (excluded from analyses). \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Investigate the neuronal relations of mechanical gastric band restriction on brain activation in response to food cues in individuals who underwent laparoscopic adjustable gastric banding (LAGB).\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Food Cue Reactivity Task\",\n",
      "    \"TaskDescription\": \"Modified task to assess brain activity in response to food images versus degraded images.\",\n",
      "    \"DesignDetails\": \"The task featured 44 food images (22 high energy-dense and 22 low energy-dense) and 44 degraded images as a baseline. Food images were presented twice, degraded images once. Participants rated their preference for each image using a button press within 2000 ms of presentation. Total duration was 5:54 minutes, including an initial 9-second fixation period.\",\n",
      "    \"Conditions\": [\n",
      "        \"High energy-dense foods\",\n",
      "        \"Low energy-dense foods\",\n",
      "        \"Degraded images\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD response\",\n",
      "        \"Reaction time\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"5:54 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate the impact of acute loss of stomach restriction on neuronal activation during food viewing in participants who underwent laparoscopic adjustable gastric banding (LAGB).\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Food Cue Reactivity Task\",\n",
      "    \"TaskDescription\": \"The task measures brain activation in response to food images compared to degraded images.\",\n",
      "    \"DesignDetails\": \"The task consisted of 44 food images (22 high energy-dense foods and 22 low energy-dense foods) matched on various properties, and 44 degraded images as a visual baseline. Each food image was presented twice for 1,750 ms, followed by a fixation cross for 2504-250 ms. Participants rated their liking of each image within 2,000 ms. The total duration of the task was 5:54 min, including a 9 s fixation period at the start.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD response to food images\",\n",
      "        \"reaction times\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"5:54 min\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 9729227\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To map the subcortical architecture of the Ventral and Dorsal Attention Networks using advanced methods of functional alignment applied to resting-state fMRI.\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To explore the subcortical anatomy of the ventral and dorsal attention networks by aligning individual resting-state functional maps in a common functional space.\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 9837608\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Synchronization task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"To measure real-time synchronization among interacting participants, the study used a computer-based movement synchronization multiagent paradigm ( ). This game allows individuals to interact nonverbally by controlling the movement of circle-shaped figures with different colors. The displays are fully synchronized as the computers are connected via a closed network. During the game, each player faces a screen with a rectangle presented on it. The participants are instructed to imagine that the rectangle represents a room. At the beginning of the game, two circles appear on the screens, and each player is assigned one of them (blue, red). Participants are instructed to imagine that the circle represents them, as they are moving in the room. The participant in the scanner uses the response box, while the participant outside the scanner uses a keyboard to control the movement of the circles. \\n\\nThe task includes three conditions. (i) Random condition\\u2014each participant controls the movement of the circle that was assigned to them. The other circle\\u2019s movement is controlled by the computer and is randomized. The participants are aware that the other circle is controlled by a computer. (ii) Free condition\\u2014each participant controls the movement of the circle that was assigned to them, and the other circle is controlled by the other participant. Participants are aware that the other circle is controlled by the other participant and are instructed to move their circle freely. (iii) Sync condition\\u2014this condition is similar to the free condition; however, the participants are instructed to synchronize their movement to the best of their ability. The order of the conditions was maintained for all participants as was established in a previous study ( ) so that instructed synchrony will not affect the emergence of spontaneous synchrony right after it. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: This study examines whether high loneliness individuals show impaired interpersonal synchronization during social interactions and explores the neural activation related to synchronization in high and low loneliness individuals, using a novel computerized fMRI paradigm to measure neural activity during naturalistic, live interaction.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Movement Synchronization Task\",\n",
      "    \"TaskDescription\": \"A computerized movement synchronization task to measure real-time synchronization among individuals using a novel dyadic interaction paradigm.\",\n",
      "    \"DesignDetails\": \"Participants were scanned using fMRI while interacting nonverbally by controlling the movement of differently colored circles in three conditions: random (computer controls one circle), free (participants control circles freely), and sync (participants attempt to synchronize with each other). Each condition was run in blocks with instructions and then executing the task for a set duration. Synchronization was measured using zero-lag correlation and following periods.\",\n",
      "    \"Conditions\": [\n",
      "        \"Random\",\n",
      "        \"Free\",\n",
      "        \"Sync\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"zero-lag correlation score\",\n",
      "        \"following periods\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To examine whether high loneliness individuals show impaired interpersonal synchronization during social interactions and to explore the neural activation related to synchronization in high and low loneliness individuals.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Movement Synchronization Task\",\n",
      "    \"TaskDescription\": \"Participants engaged in a movement synchronization task using colored circles to measure their ability to synchronize movements with a partner.\",\n",
      "    \"DesignDetails\": \"The task included three conditions: a random control condition (random), a free movement condition (free), and a synchronized movement condition (sync). Each condition was scanned in a separate run and contained three blocks. Each block began with a fixation point for 12 seconds, followed by an instruction slide for 5 seconds, and then participants performed the task for 45 seconds. After each block, participants rated their enjoyment for 10 seconds.\",\n",
      "    \"Conditions\": [\n",
      "        \"Random\",\n",
      "        \"Free\",\n",
      "        \"Sync\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"following score\",\n",
      "        \"zero-lag correlation score\",\n",
      "        \"enjoyment score\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Total run duration of each condition.\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 9910278\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"emotion reappraisal\",\n",
      "        \"dietary self-control\"\n",
      "    ],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate behavioral and neural associations in self-regulation across emotion reappraisal and dietary choice domains in human participants.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Emotion Regulation Task\",\n",
      "    \"TaskDescription\": \"Investigates reappraisal of emotional scenes (positive, negative) to assess self-regulation.\",\n",
      "    \"DesignDetails\": \"In the emotion regulation task, participants were presented with positive, negative, and neutral stimuli from the IAPS in blocks of 20 trials. Each block consisted of either viewing or reappraising the emotional content. Participants viewed a phase-scrambled version of the stimulus for 1s, followed by the full image for 7s, and then rated their feelings using a 9-point SAM scale within 4s. Trials were separated by a 1-5s jittered inter-trial interval.\",\n",
      "    \"Conditions\": [\n",
      "        \"Reappraise Positive\",\n",
      "        \"Reappraise Negative\",\n",
      "        \"View Positive\",\n",
      "        \"View Negative\",\n",
      "        \"View Neutral\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Emotion Ratings\",\n",
      "        \"BOLD signal changes\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"25 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate behavioral and neural associations in self-regulation across emotion regulation and dietary choice tasks in human participants.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Emotion Regulation Task\",\n",
      "    \"TaskDescription\": \"Participants were presented with positive, negative, and neutral stimuli and were asked to either view or reappraise the content to regulate their feelings.\",\n",
      "    \"DesignDetails\": \"The task consisted of blocks of 20 trials where participants viewed or reappraised images for 7 seconds, followed by a 4-second rating period. Each block began with a cue indicating the condition (view or reappraise) for that block. Trials were separated by a jittered inter-trial interval of 15 seconds, with a 15-second break after each block.\",\n",
      "    \"Conditions\": [\n",
      "        \"View Positive\",\n",
      "        \"View Negative\",\n",
      "        \"Reappraise Positive\",\n",
      "        \"Reappraise Negative\",\n",
      "        \"View Neutral\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal during reappraisal\",\n",
      "        \"Emotion ratings on a 9-point scale\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"~25 minutes\"\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Dietary Self-Control Task\",\n",
      "    \"TaskDescription\": \"Participants made choices about whether to eat presented food items based on their healthiness and tastiness ratings.\",\n",
      "    \"DesignDetails\": \"Each trial presented a food item for 1 second, followed by a 3-second decision window where participants indicated their choice (yes/no). The task included ~75% challenging choices where health and taste attributes were misaligned. Trials were separated by a jittered inter-trial interval of 26 seconds.\",\n",
      "    \"Conditions\": [\n",
      "        \"Eat Tasty Unhealthy\",\n",
      "        \"Eat Healthy Unpalatable\",\n",
      "        \"Eat Healthy Tasty\",\n",
      "        \"Eat Unhealthy Unpalatable\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal during food choice\",\n",
      "        \"Choice outcomes (eat/not eat)\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"~12 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 9949505\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Social incentive delay task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"Each trial consists of an anticipation cue (circle, diamond or triangle), a jittered crosshair delay, a target (white square) signaling participants to press a button and feedback (e.g. reward/threat). Each cue and corresponding feedback depicted in the lower panel of the figure. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Investigate how neighborhood disadvantage interacts with racial/ethnic background in shaping neural processing of social threats and rewards in US adolescents.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Social Incentive Delay Task\",\n",
      "    \"TaskDescription\": \"Measure neural responses to anticipating social rewards and threats using age-matched, racially diverse adolescent faces.\",\n",
      "    \"DesignDetails\": \"Participants completed the Social Incentive Delay Task with cues signaling a potential reward (happy face), threat (angry face) or neutral (blurred face) image. Two rounds of the task were conducted, totaling 116 trials (48 reward, 48 threat, 20 neutral), with jittered crosshair delay, a target for button press, and feedback.\",\n",
      "    \"Conditions\": [\n",
      "        \"Threat\",\n",
      "        \"Reward\",\n",
      "        \"Neutral\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Neural activation in response to threat and reward cues\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: The current study examined how neighborhood disadvantage intersects with racial/ethnic background in relation to neural sensitivity to social cues.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Social Incentive Delay Task\",\n",
      "    \"TaskDescription\": \"Participants completed the Social Incentive Delay Task while undergoing fMRI to measure neural responses to anticipating social rewards and threats.\",\n",
      "    \"DesignDetails\": \"Each trial began with a cue (i.e. a shape) that signaled whether the upcoming image was a potential reward (i.e. happy face), threat (i.e. angry face) or neutral (i.e. blurred face). Participants completed two rounds of the task, totaling 116 trials (48 reward, 48 threat and 20 neutral). In order to make the task motivationally salient, we used age-matched, racially diverse adolescent faces taken from the National Institute of Mental Health Child Emotional Faces Picture Set.\",\n",
      "    \"Conditions\": [\n",
      "        \"Reward\",\n",
      "        \"Threat\",\n",
      "        \"Neutral\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"neural responses to anticipating social rewards and threats\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 10031743\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [],\n",
      "    \"TaskDescription\": null\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: This study investigates the neural correlates of four formal thought disorder (FTD) dimensions in patients with schizophrenia spectrum disorders using multimodal brain imaging, including resting-state cerebral blood flow (CBF) and brain structure assessments.\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: This study tested whether four FTD dimensions differ in their association with brain perfusion and brain structure.\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 10129386\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"verbal self-referential processing (SRP)\",\n",
      "        \"checking-in\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"participants were administered a set of trait adjectives from the Affective Norms of Emotion Words database ( ). The original 20 positive and 20 negative trait adjectives were translated to Swedish by the first author, slightly modified to make the words more relevant in Swedish, and divided into blocks of five words each, which were either all positive or all negative. To each block, one \\u2018fluid\\u2019 word and one \\u2018constant\\u2019 word judged to be of the same valence as the rest of the block were added (total 16 words relating to fluidity/solidity). Examples of such words were \\u2018best\\u00e4ndig\\u2019 (durable; positive-constant), \\u2018dynamisk\\u2019 (dynamic; positive-fluid), \\u2018statisk\\u2019 (static; negative-constant), and \\u2018flyktig\\u2019 (volatile; negative-fluid). Half of the words, evenly distributed across positive and negative, were presented in uppercase, and the other half in lowercase letters. Each word was presented for 3 s, resulting in a total of 21\\u2009s per seven-word block. A fixation cross was presented for 4 s between each block. The blocks were presented three times, once in conjunction with each of three questions: \\u2018describes me?\\u2019 (Self condition), \\u2018is positive?\\u2019 (Valence condition), and \\u2018is uppercase?\\u2019 (Case condition). While Self was the condition of primary interest, Valence was included as a close control condition, and Case as a disparate control condition. Block order was the same across participants and was pseudo-randomized, so that no block type was presented two times in a row. The duration of this run was 10.48\\u2009minutes (540 TRs of 1.2 s each), and the field of view was 224\\u2009\\u00d7\\u2009232\\u2009\\u00d7\\u200997 mm , comprising 44 dynamic slices. \",\n",
      "        \"This task was also presented through E-prime and consisted of simple math questions interspersed with presentations of shapes (circle, square, triangle, or arrow). Participants were instructed beforehand to \\u2018focus on the centre of your experience, the \\u201cexperiencer\\u201d or \\u201cobserver\\u201d\\u2019 whenever the arrow was presented. Again, while Arrow was the condition of interest, Symbol was a close control condition, and Math a disparate control condition. The duration of math blocks (three questions per block) and arrow blocks were fixed to 12\\u2009s, whereas the duration of the other symbols was jittered (randomized between two and eight TRs), so that the average was 12\\u2009s per block of two symbols. The order of the three conditions (Math, Arrow, and Symbol) was the same for all participants and was pseudo-randomized, so that no block type was presented more than twice in a row. The duration of this run was 12\\u2009minutes (600 TRs of 1.2 s each), and the field of view was 224\\u2009\\u00d7\\u2009232\\u2009\\u00d7\\u200997 mm , comprising 44 dynamic slices. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Investigate brain correlates of trait self-boundarylessness during resting state and performance of two experimental tasks.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Self-Referential Processing (SRP) Task\",\n",
      "    \"TaskDescription\": \"A task using trait adjectives to probe self-referential processing.\",\n",
      "    \"DesignDetails\": \"Participants were presented with blocks of trait adjectives, which were either all positive or all negative. These blocks included words relating to fluidity or constancy. Words were presented in uppercase or lowercase. Each block was presented three times with different questions: 'describes me?', 'is positive?', and 'is uppercase?'. The order of blocks was pseudo-randomized.\",\n",
      "    \"Conditions\": [\n",
      "        \"Self\",\n",
      "        \"Valence\",\n",
      "        \"Case\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"10.48 minutes\"\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Checking-In Task\",\n",
      "    \"TaskDescription\": \"A novel task to strengthen the sense of perspectival ownership by asking participants to focus on their centre of experience.\",\n",
      "    \"DesignDetails\": \"Consisted of simple math questions interspersed with shapes. During arrow presentations, participants were instructed to focus on the centre of their experience. Other symbol presentations were jittered.\",\n",
      "    \"Conditions\": [\n",
      "        \"Math\",\n",
      "        \"Arrow\",\n",
      "        \"Symbol\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"12 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate brain correlates of trait-level sense of self-boundaries in participants with varying meditation experience during rest, a narrative self task, and a minimal self task focused on the sense of perspectival ownership of experience.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Resting State\",\n",
      "    \"TaskDescription\": \"Participants were instructed to keep their eyes closed and rest but not fall asleep and to not deliberately meditate.\",\n",
      "    \"DesignDetails\": \"The resting-state scan lasted 10.43 minutes (350 TRs of 1.8 s each). The field of view was 224x232x149 mm comprising 68 slices.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": true,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Other\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"10.43 minutes\"\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Self-Referential Processing (SRP) Task\",\n",
      "    \"TaskDescription\": \"Participants were administered a set of trait adjectives to assess self-endorsement.\",\n",
      "    \"DesignDetails\": \"The SRP task lasted 10.48 minutes (540 TRs of 1.2 s each). Each word was presented for 3 s, resulting in a total of 21s per seven-word block, with a fixation cross presented for 4 s between each block. The blocks were presented three times, once in conjunction with each of three questions: describes me? (Self condition), is positive? (Valence condition), and is uppercase? (Case condition).\",\n",
      "    \"Conditions\": [\n",
      "        \"Self condition\",\n",
      "        \"Valence condition\",\n",
      "        \"Case condition\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"10.48 minutes\"\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Checking-In Task\",\n",
      "    \"TaskDescription\": \"Participants were instructed to focus on the centre of their experience while answering simple math questions interspersed with presentations of shapes.\",\n",
      "    \"DesignDetails\": \"The checking-in task lasted 12 minutes (600 TRs of 1.2 s each). The duration of math blocks (three questions per block) and arrow blocks were fixed to 12s, whereas the duration of the other symbols was jittered (randomized between two and eight TRs), so that the average was 12s per block of two symbols.\",\n",
      "    \"Conditions\": [\n",
      "        \"Math\",\n",
      "        \"Arrow\",\n",
      "        \"Symbol\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Mixed\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"12 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 10458690\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Classmates task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"During the Classmates task, which was adapted from  , participants viewed yearbook photos of their peers from school. The yearbook photos (i.e., targets) used in the task were selected based on the sociometric data from the previous year (wave 1), because of the time required to process the sociometric data and create the scan task. However, we ran correlations between wave 1 and wave 2 for all four sociometric categories to establish that peer status was highly stable across years (i.e. High Popularity: r\\u00a0=\\u00a00.67, p\\u00a0<\\u00a0.001; Low Popularity: r\\u00a0=\\u00a00.76, p\\u00a0<\\u00a0.001; High Social Preference: r\\u00a0=\\u00a00.80, p\\u00a0<\\u00a0.001; Low Social Preference: r\\u00a0=\\u00a00.81, p\\u00a0<\\u00a0.001) in line with prior research ( ). To be selected as a target for the task, the peer needed to have a sociometric z-score between 1 and 5 (representing 1\\u20135\\u00a0SD above the mean on popularity/social preference in their school and grade) or between \\u2212\\u00a01 and \\u2212\\u00a05 (representing 1\\u20135\\u00a0SD below the mean on popularity/social preference in their school and grade). One version of the task was created for each grade level within each school (three middle schools, each with 2 grades, resulting in six versions total). The task had four conditions: High social preference (i.e., z-score between 1 and 5 on social preference), low social preference (i.e., z-score between \\u22121 and \\u22125 on social preference), high popularity (e.g., z-score between 1 and 5 on popularity), and low popularity (i.e., z-score between \\u22121 and \\u22125 on popularity). Within each condition, there were 10 targets, and we aimed for an equal number of boys and girls within each condition. Due to a data management error, z-scores for the targets from one of the schools (two of the six task versions) were incorrect and did not necessarily fall within the criteria (i.e., z-score between \\u00b1\\u00a01 and 5). Popularity and social preference scores were recalculated for the target images in these task versions. For one group of participants (n\\u00a0=\\u00a020), there were a sufficient number of target images that still fit the criteria for each condition (High Popularity =\\u00a09, Low Popularity =\\u00a08, High Social Preference = 8, Low Social Preference =\\u00a010), so these participants were included. For the other group, there were an insufficient number of target images fitting the criteria (e.g., as few as 5) in the conditions, so these participants (n\\u00a0=\\u00a015) were excluded. Across all versions of the task that were retained, the average z-score within each condition was approximately 2 (see supplement for details). There was no overlap in the targets across conditions, such that each target belonged in only one sociometric category and appeared in only one condition. No participants were included as targets so that no participant would see their own image. Target photos were obtained from school yearbooks from the previous school year, the same year the sociometric ratings were collected. Yearbook photos were digitized into JPEG images. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Examine whether neural regions that track popularity are associated with longitudinal changes in risk-taking and prosocial behavior in adolescents.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Classmates Task\",\n",
      "    \"TaskDescription\": \"Participants viewed pictures of popular and unpopular classmates during an fMRI scan to study neural sensitivity to peer status.\",\n",
      "    \"DesignDetails\": \"During the Classmates task, participants viewed yearbook photos of peers from school. The task had four conditions: high social preference, low social preference, high popularity, and low popularity. Within each condition, there were 10 targets, with the aim for an equal number of boys and girls per condition. The task used an n-back design to maintain participant attention, and faces repeated within blocks. Each run consisted of eight blocks, with two blocks per condition, presented across two runs.\",\n",
      "    \"Conditions\": [\n",
      "        \"High social preference\",\n",
      "        \"Low social preference\",\n",
      "        \"High popularity\",\n",
      "        \"Low popularity\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"Neural activation in brain regions associated with social cognition and valuation\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"672 seconds\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To examine how individual differences in neural sensitivity to peers with high and low status predict longitudinal changes in risk-taking and prosocial behavior in early adolescence.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Prosocial Tendencies Measure\",\n",
      "    \"TaskDescription\": \"Participants reported on how much they feel 19 different behavioral tendencies apply to them on a 5-point scale.\",\n",
      "    \"DesignDetails\": \"The scale includes 5 subscales assessing different motivations for prosocial behavior. The average of all items across these subscales was calculated to create one index of prosocial behavior.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"prosocial behavior\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Adolescent Risk-taking Scale\",\n",
      "    \"TaskDescription\": \"Participants reported on their frequency of engaging in 14 risky behaviors on a 4-point scale.\",\n",
      "    \"DesignDetails\": \"The scale included questions about rule breaking, sexual activity, substance use, and dangerous behavior. A total mean score for all items was calculated.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"risk-taking behavior\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Classmates Task\",\n",
      "    \"TaskDescription\": \"Participants viewed yearbook photos of their classmates from school and grade, sorted by popularity based on sociometric ratings.\",\n",
      "    \"DesignDetails\": \"The task had four conditions: High social preference, low social preference, high popularity, and low popularity. Each condition included 10 targets, with an equal number of boys and girls. The task was programmed in E-Prime and presented across two runs, each consisting of eight blocks, two blocks per condition, with a total of 40 trials per condition. Each target face was presented for 1750ms, separated by a jittered inter-trial interval (M = 2300ms).\",\n",
      "    \"Conditions\": [\n",
      "        \"High social preference\",\n",
      "        \"Low social preference\",\n",
      "        \"High popularity\",\n",
      "        \"Low popularity\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"neural tracking of popularity\",\n",
      "        \"risk-taking behavior\",\n",
      "        \"prosocial behavior\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Approximately 1.5 hours\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 10597625\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"single-cue conditioning paradigm\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"The experimental design was based on the one used by   and  . The main task consisted of three runs (i.e.\\u00a0functional runs in the scanner) of 45 trials per run for a total of 135 trials across all three runs. Half the trials were CS-alone trials in which the US was omitted. In the other half of the trials (CS\\u2013US pairs), the CS co-terminated with the US. In each run, the 45 trials were presented as sequences of one-in-a-row (single), two-in-a-row (double) or three-in-a-row (triple) CS\\u2009+\\u2009US trials and sequences of one-in-a-row (single), two-in-a-row (double) or three-in-a-row (triple) CS-alone trials\\u00a0( ). The sequences were designed to conform to a binomial distribution of two equally probable events (CS-alone trials and CS\\u2009+\\u2009US trials). In a given run, the various sequences were randomly shuffled, following a method adopted from Nicks (1959). Additionally, a CS-alone trial was added at the end of each run to measure expectancy ratings and brain activity related to the last sequence of the run. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To test the hypothesis that there are dissociable brain networks related to the expectancy and associative strength theories using a single-cue fear conditioning paradigm with a pseudo-random intermittent reinforcement schedule during functional magnetic resonance imaging.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Fear Conditioning Task\",\n",
      "    \"TaskDescription\": \"This task uses a single-cue fear conditioning paradigm to assess brain activity related to expectancy and associative strength theories.\",\n",
      "    \"DesignDetails\": \"The main task consisted of three functional runs in the scanner, each with 45 trials (total of 135 trials). Half the trials were CS-alone, and the other half were CS+US pairs. The trial sequences conformed to a binomial distribution of two equally probable events (CS-alone and CS+US trials).\",\n",
      "    \"Conditions\": [\n",
      "        \"CS-alone\",\n",
      "        \"CS+US\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: The present study tested the hypothesis that there are dissociable brain networks related to the expectancy and associative strength theories using a single-cue fear conditioning paradigm with a pseudo-random intermittent reinforcement schedule during functional magnetic resonance imaging.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Single-Cue Fear Conditioning\",\n",
      "    \"TaskDescription\": \"This task involves the presentation of a conditioned stimulus (CS) followed by an unconditioned stimulus (US) to elicit conditioned responses (CRs).\",\n",
      "    \"DesignDetails\": \"The main task consisted of three runs of 45 trials per run for a total of 135 trials across all three runs. Half the trials were CS-alone trials where the US was omitted, and the other half were CS+US pairs. Trials were presented as sequences of single, double, or triple CS+US and CS-alone trials, randomly shuffled within each run. A CS-alone trial was added at the end of each run to measure expectancy ratings and brain activity related to the last sequence of the run.\",\n",
      "    \"Conditions\": [\n",
      "        \"CS+US\",\n",
      "        \"CS-alone\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"expectancy ratings\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Total of 135 trials across three runs.\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 10615837\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"semantic judgment task\",\n",
      "        \"non-verbal tone judgment task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"In both tasks, participants were required to decide whether an auditory stimulus matches with a presented image via yes/no-button press using the index and middle finger of their left hand. The left hand was used to shift motor activity related to the button press to the right hemisphere. The order of buttons was counterbalanced across participants. Tasks were presented in mini-blocks of four trials per task and blocks were separated by short rest intervals. Individual trials were 3.5\\u00a0s long including presentation of auditory and visual stimulus, and button press by the participant. Each run included 88 stimuli with 32 items per condition of semantic judgment and 24 items of tone judgment. Participants completed two runs per session. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To explore the potential of intermittent theta-burst stimulation (iTBS) over a domain-general hub to enhance executive and semantic processing in healthy middle-aged to older adults.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Semantic Judgment Task\",\n",
      "    \"TaskDescription\": \"A task with varying cognitive demand requiring participants to decide if an auditory stimulus matches a presented image.\",\n",
      "    \"DesignDetails\": \"Participants performed two tasks: a verbal semantic judgment task with low demand (WPM) and high demand (FPM) and a non-verbal tone judgment task. Each run included 88 stimuli (32 items per semantic condition, 24 for tone judgment). Trials lasted 3.5s with mini-blocks of four trials per task, separated by short rests.\",\n",
      "    \"Conditions\": [\n",
      "        \"Word-Picture Matching\",\n",
      "        \"Feature-Picture Matching\",\n",
      "        \"Tone Judgment\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"accuracy\",\n",
      "        \"reaction time\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: The study aimed to determine if intermittent theta-burst stimulation (iTBS) could enhance semantic and executive task processing in healthy middle-aged to older adults.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Semantic Judgment Task\",\n",
      "    \"TaskDescription\": \"Participants decided whether an auditory stimulus matches with a presented image via yes/no-button press.\",\n",
      "    \"DesignDetails\": \"The task included two conditions: a low demand word-picture matching (WPM) and a high demand feature-picture matching (FPM). Each run included 88 stimuli with 32 items for WPM and 24 items for tone judgment. Tasks were presented in mini-blocks of four trials, with each trial lasting 3.5 seconds, including the presentation of auditory and visual stimuli and the button press.\",\n",
      "    \"Conditions\": [\n",
      "        \"Word-Picture Matching (WPM)\",\n",
      "        \"Feature-Picture Matching (FPM)\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"accuracy\",\n",
      "        \"reaction time\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Each run included 88 stimuli.\"\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Tone Judgment Task\",\n",
      "    \"TaskDescription\": \"Participants indicated whether auditory and visual stimuli matched via button press.\",\n",
      "    \"DesignDetails\": \"The tone judgment task was included to characterize the modulatory effect of pre-SMA stimulation on non-verbal executive demands. The task was presented in mini-blocks of four trials, with each trial lasting 3.5 seconds, including the presentation of auditory and visual stimuli and the button press.\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"accuracy\",\n",
      "        \"reaction time\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Each run included 88 stimuli.\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 10637045\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"multiple-threat paradigm\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"On the study day, visceral and somatic pain thresholds were initially determined based on ratings of gradually increasing stimulus intensities, as previously accomplished.  Individual pain thresholds served as anchor for the subsequent calibration and matching procedures. During calibration, stimulus intensities for each pain modality were identified based on perceived pain intensity ratings within the target range of 60 to 80 mm on digitised visual analogue scales [VAS] with endpoints labelled \\u2018not painful\\u2019 [0] and \\u2018extremely painful\\u2019 [100], respectively. This procedure allows the determination of individual stimulation intensities [distension pressure for the visceral modality and temperature for the somatic modality] inducing adequately painful sensations for use in within-subject application of repeated pain stimuli during brain imaging. Afterwards, to ensure comparability across modalities, visceral and somatic perceived pain intensities were matched. For this purpose, visceral stimuli were presented together with thermal stimuli and participants were asked to compare the stimuli using Likert-type response options indicating more, less, or equally painful stimuli. If the rating showed a deviation, the intensity of thermal stimuli was successively adjusted until ratings indicated equal perception at least twice consecutively. As in our previous work,  stimulus durations were adjusted for each individual, aiming at matched durations of ascending [increasing pressure and temperature, respectively] and plateau phases [stable pressure and temperature, respectively] of visceral and thermal stimulation [total stimulus duration per stimulus: 20 s]. The stimulation intensities resulting from the matching procedure in patients with UC were comparable to those in healthy volunteers and patients with IBS, for both thermal stimuli [UC: 45.43\\u2005\\u00b1\\u20050.36\\u00b0C; HC: 45.20\\u2005\\u00b1\\u20050.30\\u00b0C; IBS: 43.83\\u2005\\u00b1\\u20050.70\\u00b0C] and rectal distensions [UC: 32.60\\u2005\\u00b1\\u20051.87 mmHg; HC: 38.16\\u2005\\u00b1\\u20052.25 mmHg; IBS: 34.78\\u2005\\u00b1\\u20052.15 mmHg]. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To investigate behavioural and neural correlates of visceral versus somatic pain processing in women with quiescent ulcerative colitis compared to irritable bowel syndrome and healthy women.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Multiple-threat paradigm with visceral and somatic pain stimuli\",\n",
      "    \"TaskDescription\": \"This task models the experience of multiple symptoms arising from viscera and other bodily sites, reflecting patients' clinical reality of aversive intestinal and extraintestinal symptoms.\",\n",
      "    \"DesignDetails\": \"Functional MRI assessed BOLD responses to 10 painful stimuli (five visceral, five somatic), implemented in pseudorandomised order, using previously calibrated and matched pain stimulus intensities. Prior to each stimulus, a fixation cross was shown (jittered 6-12 s), followed by a stimulus presentation. Participants rated perceived pain intensity post-stimulus on a digitised VAS for 9 s. An OFF phase followed each rating with jittered durations of 4.6-6.9 s.\",\n",
      "    \"Conditions\": [\n",
      "        \"Visceral pain\",\n",
      "        \"Somatic pain\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\",\n",
      "        \"perceived pain intensity\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To investigate behavioral and neural correlates of visceral versus somatic pain processing in women with quiescent ulcerative colitis compared to irritable bowel syndrome and healthy women.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Multiple-threat paradigm\",\n",
      "    \"TaskDescription\": \"This task involves the presentation of visceral and somatic pain stimuli to assess neural and behavioral responses.\",\n",
      "    \"DesignDetails\": \"On the study day, visceral and somatic pain thresholds were initially determined based on ratings of gradually increasing stimulus intensities. Individual pain thresholds served as anchors for subsequent calibration and matching procedures. During calibration, stimulus intensities for each pain modality were identified based on perceived pain intensity ratings within the target range of 60 to 80 mm on digitized visual analogue scales. The total stimulus duration per stimulus was 20 seconds, and functional MRI was conducted to assess blood oxygenation level-dependent (BOLD) responses to a total of 10 painful stimuli (five visceral, five somatic, implemented in pseudorandomized order).\",\n",
      "    \"Conditions\": null,\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD responses\",\n",
      "        \"perceived pain intensity\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"~5 min\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 10641579\n",
      "--- delavega-aliceoverlap ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Chatroom task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"In this task, at the first visit, participants were asked to classify half of the 60 adolescents' photographs into the peers that they wanted to chat with (liked peers) and another half of the photographs into the peers that they did not want to chat with (unliked peers). At the second visit, in the fMRI scanner, participants received feedback indicating whether each of 60 adolescents were interested in chatting with them (acceptance) or not (rejection) or did not rate their interest (not rated), and reported how receiving this social feedback made them feel.\",\n",
      "        \"an experimental paradigm that simulates social evaluation ( ,  ). The task consists of a selection phase (out of scanner) and a feedback phase (in scanner) administered in two visits.\",\n",
      "        \"#### Visit 1 (Selection Phase) \\n  \\nParticipants were told they were participating in a nationwide study about how teenagers communicate with each other on the internet, and that they would chat online with a peer selected for them based on similar interests from among participants at the other study sites. To enhance believability, participants created an online profile describing their interests and were told they would have their photograph taken. Participants then completed the selection phase whereby they viewed 60 photographs of mid- to late-adolescents (30 boys, 30 girls) and then placed 30 peers into an \\u201cinterested\\u201d (liked) and 30 into a \\u201cnot interested\\u201d (unliked) onscreen bin. Peer photographs were taken from stimulus sets used in past studies (e.g.,  ). Participants were told the other peers would indicate whether they wanted to chat with the participant using the same procedure. \\n\\n\\n#### Visit 2 (Feedback Phase) \\n  \\nDuring visit two, participants completed the feedback phase of the task while undergoing an MRI scan. Participants were told they would chat online at the end of the visit with the peer selected for them. The fMRI feedback task included 60 trials. On each trial, for 2\\u00a0s, the photograph of each peer for whom participants had previously indicated their interest was displayed, and a reminder appeared about whether participants had judged the peer as one of interest (liked) or not (unliked). An inter-stimulus interval of 2, 4, 6, or 8\\u00a0s was included in equal numbers per duration length across the 60 trials (i.e., 15 trials). Next, for 1\\u00a0s, participants viewed feedback indicating whether the presented peer wanted to interact with the participants (i.e., acceptance feedback; \\u201c  He/she LIKED you  \\u201d), did not want to interact with the participants (i.e., rejection feedback; \\u201c  He/she DID NOT LIKE you  \\u201d), or did not rate their interest of the participants (i.e., \\u201cnot rated\\u201d feedback; \\u201c  NOT RATED  \\u201d). We modified a previous version of the chatroom fMRI task ( ,  ) to include a \\u201cnot rated\\u201d condition to have a neutral comparison event for use in fMRI analyses rather than a general baseline. Feedback types were pseudo-randomized with an equal number of trials (i.e., 15 trials) yielding 6 event types that combine participants\\u2019 selections and peer feedback condition (i.e., acceptance/rejection/not rated from liked peers, acceptance/rejection/not rated from unliked peers). After feedback was displayed for 1\\u00a0s, a rating bar was presented and participants indicated with an MRI response box, \\u201cHow does this make you feel?\\u201d on a scale of 1\\u00a0=\\u00a0very bad to 5\\u00a0=\\u00a0great within a 3-second response duration. An inter-trial interval of 2, 4, 6, or 8\\u00a0s was included in equal numbers per duration length across the 60 trials (i.e., 15 trials). After the fMRI scan, participants were debriefed and told that no social evaluations were actually performed and they would not chat with a peer at the end of the visit. No adverse reactions to the debriefing occurred. \\n\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: Examine associations between rumination and neural responses to social rejection in adolescent girls.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Chatroom fMRI Task\",\n",
      "    \"TaskDescription\": \"Social evaluation fMRI task where participants received fictitious feedback from peers (accepted, rejected, or not rated).\",\n",
      "    \"DesignDetails\": \"The task has two phases: selection (outside scanner) and feedback (in scanner). In the feedback phase, participants viewed a photograph of a peer for 2s, saw feedback (acceptance, rejection, not rated) for 1s, and rated their feelings within a 3s duration. Feedback types were pseudo-randomized across 60 trials with inter-trial intervals of 2-8s.\",\n",
      "    \"Conditions\": [\n",
      "        \"acceptance feedback from liked peers\",\n",
      "        \"rejection feedback from liked peers\",\n",
      "        \"not rated feedback from liked peers\",\n",
      "        \"acceptance feedback from unliked peers\",\n",
      "        \"rejection feedback from unliked peers\",\n",
      "        \"not rated feedback from unliked peers\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"neural activity\",\n",
      "        \"sgACC connectivity\",\n",
      "        \"reaction time\",\n",
      "        \"emotional ratings\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: The current study aimed to examine how rumination relates to neural responses to social rejection in adolescent girls.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Chatroom fMRI Task\",\n",
      "    \"TaskDescription\": \"A social feedback task where participants received fictitious feedback (acceptance, rejection) from peers they liked or disliked.\",\n",
      "    \"DesignDetails\": \"The task consists of a selection phase (out of scanner) and a feedback phase (in scanner) administered in two visits. In the selection phase, participants classified photographs of peers into liked and unliked categories. In the feedback phase, participants received feedback about whether each peer wanted to chat with them, with an inter-stimulus interval of 2, 4, 6, or 8 seconds included across 60 trials. Feedback types were pseudo-randomized, and participants rated their emotional response after each feedback.\",\n",
      "    \"Conditions\": [\n",
      "        \"Liked peers acceptance\",\n",
      "        \"Liked peers rejection\",\n",
      "        \"Liked peers not rated\",\n",
      "        \"Unliked peers acceptance\",\n",
      "        \"Unliked peers rejection\",\n",
      "        \"Unliked peers not rated\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"neural activity\",\n",
      "        \"connectivity\",\n",
      "        \"reaction time\",\n",
      "        \"emotional ratings\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Approximately 60 trials with varying inter-stimulus intervals.\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 10656574\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Classmates task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"The task had four conditions: high social preference (i.e.   z  -score between 1 and 5 on social preference), low social preference (i.e.   z  -score between \\u22121 and \\u22125 on social preference), high popularity (e.g. z-score between 1 and 5 on popularity) and low popularity (i.e.   z  -score between \\u22121 and \\u22125 on popularity). Within each condition, there were 10 targets (i.e.\\u00a040 targets for each participant), and we aimed for an equal number of boys and girls within each condition. Due to a data management error,   z  -scores for the targets in two of the six task versions were incorrect and did not fall within the criteria (i.e.   z  -score between\\u2009\\u00b1\\u20091 and 5). Popularity and social preference (i.e. likability) scores were recalculated for the target images in these task versions. For one group of participants (n\\u2009=\\u200920), there were a sufficient number of target images that fit the criteria for each condition (High Popularity\\u2009=\\u20099, Low Popularity\\u2009=\\u20098, High Social Preference\\u2009=\\u20098, Low Social Preference\\u2009=\\u200910), so these participants were included, resulting in five groups of participants and study stimuli. For the other groups, there were an insufficient number of target images (e.g. as little as\\u00a05) in the conditions, so these participants (n\\u2009=\\u200915) were excluded. The faces in each condition were equally divided by female and male adolescents. We also attempted to build a racially/ethnically diverse paradigm such that each condition contains some faces representing individuals from minoritized groups. Research suggests that social preference (i.e. likability) and popularity are two distinct but correlated sociometric constructs in adolescents ( ). For our stimuli, the correlation between social preference and popularity scores ranged from 0.36 to 0.68 (Group 1:   r  \\u2009=\\u20090.36,   P  \\u2009<\\u20090.05; Group 2:   r  \\u2009=\\u20090.68,   P  \\u2009<\\u20090.001; Group 3:   r  \\u2009=\\u20090.56,   P  \\u2009<\\u20090.001; Group 4:   r  \\u2009=\\u20090.42,   P  \\u2009<\\u20090.01, Group 5:   r  \\u2009=\\u20090.46,   P  \\u2009<\\u20090.005). The correlation between social preference and popularity scores in the larger dataset from which study stimuli were drawn was 0.46,   P  \\u2009<\\u20090.001. Each target belonged in only one sociometric category and there was no overlap between targets in the social preference and popularity conditions. We did so to maximize the difference between neural responses to sociometric likability and popularity and increase the statistical power in our analyses. No adolescent participants in this neuroimaging study were included as face stimuli. The average   z  -score within each condition was approximately 2 (absolute value; see  ). We ensured equal distributions of sociometric scores across schools and conditions for the stimuli. \\n\\nThe Classmates task consisted of 16 blocks, four blocks per condition, presented across two runs. The blocks were presented in a randomized order. Within each block, there were 10 targets chosen to comprise that condition. Peer faces within each block were shown in a fixed order using a randomization algorithm. Participants saw each face 4 times total (2 in each run), with each condition having 40 total trials each. To achieve the power for neuroimaging analysis on visual stimuli, we repeated each peer face across the conditions, in line with previous studies (e.g.  ;  ). We used a 1-back task to ensure that adolescent participants were paying attention to these visual stimuli during scanning, such that each block contained one target that appeared twice in a row. In addition, implicit inferences of social attributes from faces often require face identity recognition processes ( ,  ). A large body of work has shown that 1-back paradigms successfully elicit face identity recognition processes in which participants invoke a mental representation of face identity in the absence of percept ( ;  ;  ). Participants were instructed to press a button with their right pointer finger when a face repeated. Adolescents were not explicitly told to track peer status, which allows us to investigate how the brain spontaneously supports adolescents\\u2019 awareness of peer-based social hierarchies in real-world networks. Each face was shown for 1750\\u2009ms, and a fixation cross was jittered around an average of 2301\\u2009ms (range: 565.8\\u20134936.8\\u2009ms; see\\u00a0 ). We did not include a control condition (e.g. teenagers with equal social attributes from other social networks) to account for other social attributes as it may regress out the phenomenon of our research interest, given that other social attributes (e.g. facial attractiveness, trustworthiness) are inherently correlated with social status in humans ( ). \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To examine adolescents' neural tracking of peers from their real-world social network that vary in social preferences and popularity.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Classmates fMRI task\",\n",
      "    \"TaskDescription\": \"Adolescents viewed images of their peers from their real-world social networks to examine neural tracking of social preference and popularity.\",\n",
      "    \"DesignDetails\": \"The task included 16 blocks presented over two runs, with each block associated with one of four conditions: high social preference, low social preference, high popularity, and low popularity. Peer faces were shown in blocks, repeated across runs with 40 total trials per each of the four conditions. A 1-back task was used to ensure attentiveness.\",\n",
      "    \"Conditions\": [\n",
      "        \"High social preference\",\n",
      "        \"Low social preference\",\n",
      "        \"High popularity\",\n",
      "        \"Low popularity\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To examine adolescents' neural tracking of peers from their real-world social network that varied in social preferences and popularity.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Classmates fMRI task\",\n",
      "    \"TaskDescription\": \"Adolescents viewed yearbook photos of their peers from their school and grade.\",\n",
      "    \"DesignDetails\": \"The task consisted of 16 blocks, four blocks per condition, presented across two runs. Each block contained 10 targets chosen to comprise that condition. Peer faces within each block were shown in a fixed order using a randomization algorithm. Participants saw each face 4 times total (2 in each run), with each condition having 40 total trials each. A 1-back task was used to ensure participants were paying attention to the visual stimuli during scanning, with each block containing one target that appeared twice in a row.\",\n",
      "    \"Conditions\": [\n",
      "        \"high social preference\",\n",
      "        \"low social preference\",\n",
      "        \"high popularity\",\n",
      "        \"low popularity\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD activation\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 10791126\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Empathy-for-pain task\",\n",
      "        \"Emotional reactivity task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \" In trials of the Self condition, participants passively received electrical stimuli. In the Other condition, participants experienced how another person (a confederate) received electrical stimuli. The stimuli were either painful or not painful. In the cue phase, an arrow indicated the recipient (downwards: Self; right: Other) and the intensity (blue: not painful; red: painful) of the next stimulus. In the stimulation phase, the stimulus was delivered. After half of the trials, participants were asked to rate the last stimulus. The confederate depicted has given informed consent that his photograph can be published\",\n",
      "        \"Participants were presented pictures with different content (violent or neutral) and different context (real or game context). After observing a block of pictures, participants rated their current unpleasantness on a visual analog scale from 0 to 100\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To assess the causal effect of violent video games (VVGs) on the behavioral and neural correlates of empathy and emotional reactivity to violence using fMRI.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Empathy-for-pain task\",\n",
      "    \"TaskDescription\": \"Measures empathy for pain by comparing brain responses to painful vs. non-painful stimuli in oneself and another person.\",\n",
      "    \"DesignDetails\": \"In the Self condition, participants received electrical stimuli. In the Other condition, they observed another person receiving electrical stimuli. The stimuli were either painful or not. Trials included cue and stimulation phases, with rating after some trials. 64 trials total, with 16 per condition, pseudorandomized order. Duration approx. 20 min.\",\n",
      "    \"Conditions\": [\n",
      "        \"Self Pain\",\n",
      "        \"Self No Pain\",\n",
      "        \"Other Pain\",\n",
      "        \"Other No Pain\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"painfulness rating\",\n",
      "        \"unpleasantness rating\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"20 minutes\"\n",
      "}\n",
      "{\n",
      "    \"TaskName\": \"Emotional reactivity task\",\n",
      "    \"TaskDescription\": \"Assesses emotional reactivity to violent vs. neutral images in different contexts (real or game).\",\n",
      "    \"DesignDetails\": \"Participants viewed blocks of pictures (violent or neutral), with pictures from real life or game context. Each block had 5 pictures with 3s duration each, followed by unpleasantness rating. Total of 16 blocks, 4 per condition. Duration approx. 5 min.\",\n",
      "    \"Conditions\": [\n",
      "        \"Neutral Real\",\n",
      "        \"Neutral Game\",\n",
      "        \"Violent Real\",\n",
      "        \"Violent Game\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"unpleasantness rating\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"5 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To assess the causal effect of violent video games (VVGs) on the behavioral and neural correlates of empathy and emotional reactivity to violence.\n",
      "BehavioralTasks:\n",
      "{\n",
      "    \"TaskName\": \"Emotional reactivity task\",\n",
      "    \"TaskDescription\": \"Participants were shown pictures of either neutral or violent content and rated their unpleasantness.\",\n",
      "    \"DesignDetails\": \"Each block consisted of five pictures of the same condition, presented for 3 seconds each, with a short interval of 0.2 seconds between pictures. After a jittered interval, participants rated how unpleasant they felt on a 100-step VAS. In total, participants saw 16 blocks of pictures, with 4 blocks per condition (Neutral Real, Neutral Game, Violent Real, Violent Game).\",\n",
      "    \"Conditions\": [\n",
      "        \"Neutral Real\",\n",
      "        \"Neutral Game\",\n",
      "        \"Violent Real\",\n",
      "        \"Violent Game\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"unpleasantness ratings\"\n",
      "    ]\n",
      "}\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Empathy-for-pain task\",\n",
      "    \"TaskDescription\": \"Participants either received electric stimuli themselves or saw images of a confederate indicating that he was currently receiving electric stimulation.\",\n",
      "    \"DesignDetails\": \"The task included 64 trials, with 16 trials per condition (Self Pain, Self No Pain, Other Pain, Other No Pain). Conditions were presented in a pseudorandomized order. The timeline of the task included a cue phase indicating the recipient and intensity of the next stimulus, followed by a stimulation phase where the stimulus was delivered. After half of the trials, participants rated the last stimulus on a visual analog scale (VAS).\",\n",
      "    \"Conditions\": [\n",
      "        \"Self Pain\",\n",
      "        \"Self No Pain\",\n",
      "        \"Other Pain\",\n",
      "        \"Other No Pain\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"painfulness ratings\",\n",
      "        \"unpleasantness ratings\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"approx. 20 min\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 10990450\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"Montreal Imaging Stress Task (MIST)\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"During the 5\\u2005min   stress condition   ( A), participants were asked to answer math problems of varying difficulty under time constraints whilst receiving trial-by-trial on screen performance feedback (\\u2018correct\\u2019 in green, \\u2018error\\u2019 in red, or \\u2018timeout\\u2019 in yellow). To answer, participants were provided a button box and instructed to navigate left or right on a rotary-dial to the correct digit (between 0 and 9). In addition, a performance bar at the top of the screen continuously displayed the \\u2018average\\u2019 performance of previous participants (artificially set to 80%) as well as the participant\\u2019s current performance. Participants were instructed to attain or surpass the average performance of their peers. To induce a high failure rate, the participant\\u2019s response time limit got adjusted throughout the task to enforce a range of approximately 20% to 45% correct responses (Dedovic et al.,  ). Specifically, participants were given 10% less time after three consecutive correct responses and 10% more time after three consecutive incorrect or timeout responses. To further induce psychosocial stress, participants were presented with a 5 sec on screen summary of their current performance and were reminded that their \\u2018performance should be close to or better than the average performance\\u2019. This summary was presented at five timepoints during the stress condition. In addition, participants received scripted negative verbal feedback in between runs from a member of the study team saying: \\u2018Your performance is below average. In order for your data to be used, your performance should be close to or better than the average performance. Please try as hard as you can next round\\u2019. \\n\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: This study examined whether perceived friendship quality was related to better mental health and lower neural stress response in young people with childhood adversity (CA).\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Montreal Imaging Stress Task (MIST)\",\n",
      "    \"TaskDescription\": \"The MIST is an acute psychosocial stress paradigm combining high cognitive demands with negative social feedback, designed to elicit stress responses.\",\n",
      "    \"DesignDetails\": \"The MIST is presented as a block design across two imaging runs, each lasting 11 minutes. It includes stress, control, and rest conditions, with counterbalanced order across participants. The stress condition involves solving math problems under time pressure with performance feedback, while the control condition lacks time constraints and feedback on performance is neutral. The rest condition involves an empty task interface with no responses required.\",\n",
      "    \"Conditions\": [\n",
      "        \"stress\",\n",
      "        \"control\",\n",
      "        \"rest\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"response time\",\n",
      "        \"accuracy\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"22 minutes\"\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: This study examined whether perceived friendship quality was related to better mental health and lower neural stress response in young people with childhood adversity (CA).\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Montreal Imaging Stress Task (MIST)\",\n",
      "    \"TaskDescription\": \"The MIST is a well-validated acute psychosocial stress paradigm combining the stress-eliciting effects of high cognitive demands (solving math problems under time pressure) with negative social feedback.\",\n",
      "    \"DesignDetails\": \"The MIST was presented as a block design across two imaging runs, each lasting 11 minutes. Each run consisted of a stress condition, control condition, and rest condition, with the order of conditions counterbalanced across participants. The stress condition involved answering math problems under time constraints with performance feedback, while the control condition had no time constraints and no performance feedback. The rest condition required participants to keep their eyes open without pressing buttons until the next math problem appeared.\",\n",
      "    \"Conditions\": [\n",
      "        \"Stress\",\n",
      "        \"Control\",\n",
      "        \"Rest\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"state anxiety\",\n",
      "        \"neural activity in frontolimbic brain regions\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Approximately 35 minutes including practice.\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 11063816\n",
      "--- delavega-other ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"six-alternative forced-choice cued-recognition task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"On each trial, a cue and six potential targets were presented simultaneously on the screen. The cue was presented in the middle of the screen with the six possible targets; one target and five foils form the same category (e.g., if the target was hammer, the five foils would be other randomly selected objects from the other events, regardless of closed- vs open-loop or delay vs no-delay status) presented in two rows of three below the cue ( ). Participants had a maximum of 6\\u2005s to respond with a button response that corresponded to the target position on the screen and were instructed to be as accurate as possible in the time given. The position of the correct target was randomly selected on each retrieval trial. The cue and six targets were presented until a response was made or when the maximum 6\\u2005s limit was reached (M response times  \\u2009\\u00b1\\u2009  SD\\u2009=\\u20092.86\\u2005s\\u2009\\u00b1\\u20090.42\\u2005s). Missing responses (i.e., responses that fell outside the 6\\u2005s response window) were treated as incorrect trials for both accuracy and dependency (M percentage of missing responses  \\u2009\\u00b1\\u2009  SD\\u2009=\\u20093.54%\\u2009\\u00b1\\u20093.22%). A 2\\u2009\\u00d7\\u20092 (loop\\u2009\\u00d7\\u2009delay) ANOVA, where the dependent variable was the proportion of missing responses, showed no significant effects (  F  s\\u2009<\\u20090.37). Thus, any differences in dependency across conditions are unlikely to be caused by assuming missing responses would have been incorrect. Note also that due to the six-alternative forced-choice recognition test, the chance of guessing correctly was relatively low (\\u223c16.7%). \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: To assess whether episodic events continue to be reinstated holistically and whether hippocampal pattern completion continues to facilitate holistic reinstatement following a period of consolidation.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Holistic Episodic Memory Retrieval Task\",\n",
      "    \"TaskDescription\": \"A task to assess hippocampal pattern completion and its relationship to neocortical reinstatement across delays.\",\n",
      "    \"DesignDetails\": \"The experiment consisted of two encoding sessions and a retrieval session. Encoding involved presenting pairwise associations of 72 events (closed and open loops). Retrieval involved a six-alternative forced-choice task to determine the target from cues, testing the learned pairs under both no delay and 24h delay after encoding.\",\n",
      "    \"Conditions\": [\n",
      "        \"open-loop no-delay\",\n",
      "        \"open-loop delay\",\n",
      "        \"closed-loop no-delay\",\n",
      "        \"closed-loop delay\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"retrieval accuracy\",\n",
      "        \"retrieval dependency\",\n",
      "        \"fMRI BOLD signal\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: To assess whether episodic events continue to be reinstated holistically and whether hippocampal pattern completion continues to facilitate holistic reinstatement following a period of consolidation.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Cued-Recognition Task\",\n",
      "    \"TaskDescription\": \"Participants performed a six-alternative forced-choice cued-recognition task to retrieve overlapping elements from learned events.\",\n",
      "    \"DesignDetails\": \"The task involved presenting a cue and six potential targets simultaneously on the screen. Participants had a maximum of 6 seconds to respond with a button corresponding to the target position. Each trial was preceded by a 1-second fixation and followed by a 1-second blank screen. The task included 360 retrieval trials across two scanning runs, with trials optimized to measure univariate BOLD activity in four within-subject experimental conditions (open-loop, delay; open-loop, no-delay; closed-loop, delay; closed-loop, no-delay).\",\n",
      "    \"Conditions\": [\n",
      "        \"Open-loop\",\n",
      "        \"Closed-loop\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"Response time\",\n",
      "        \"Accuracy\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"EventRelated\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"6 seconds\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n",
      "PMCID: 11078806\n",
      "--- delavega_nv ---\n",
      "{\n",
      "    \"TaskName\": [\n",
      "        \"stopwatch task\"\n",
      "    ],\n",
      "    \"TaskDescription\": [\n",
      "        \"For each trial, participants were presented with a stopwatch that automatically started; they were asked to presss a button to stop the stopwatch within 50 ms of the 5-s time point. If participants stopped the stopwatch within this time window, the trial was deemed a success. In contrast, if participants failed to stop the stopwatch within this time window, the trial was deemed as a failure. To experimentally manipulate the success/failure outcome (without being affected by participants\\u2019 performance), participants were shown random numbers on the display of the stopwatch after 3 s; thus, they could not accurately calibrate the correct timing. In some blocks, participants completed the task with a focus on approach goals, where they earned points when they succeeded but did not lose any points when they failed. In other blocks, participants completed the same task with a focus on avoidance goals, where they lost points when they failed but did not earn points when they succeeded. \\n\\nEach block started with a short instruction indicating the type of the upcoming block (approach = \\u201cWill win points if you succeed\\u201d; avoidance = \\u201cWill lose points if you fail\\u201d). Participants needed to press a button within 5 s to indicate that they understood the nature of the upcoming block; when participants did not press a button within 5 s, they were told that they needed to respond quickly and shown the same instruction again. This block instruction was followed by a jittered ISI (between 3 to 7 s) and experimental trials. Each trial started with a cue for 2 s. The cue indicated one of the three different points (3, 1, and 0). In the approach blocks, participants were told that they would earn the presented amount of points when they were successful in the trial, but they would not lose any points if they failed in the trial. In the avoidance blocks, participants were told that they would lose the presented amount of points when they failed in the trial, but they would not earn any points when they were successful. The cue was followed by a jittered ISI (between 3 to 7 s), which was replaced by a stopwatch that automatically started. Participants needed to press a button to stop the stopwatch within the time frame described earlier. Once participants pressed the button, the stopwatch stopped and participants found out whether they were sucessful or not. The outcome was shown for 3.2 s, followed by a jittered ITI (3\\u20137 s) and the next trial. When participants failed to press a button within 6 s after the stoptwatch started, they were told to press the button sooner and a new trial started. The three different point conditions were included to increase the unpredictability of the task and thereby encourage participants to pay attention to the cue of each trial. \"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- gpt-4o ---\n",
      "\tStudyObjectve: The present study sought to address how the reward network in the brain, including the striatum and ventromedial prefrontal cortex, is involved when individuals engage in the same task with a focus on approach or avoidance goals.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Stopwatch Task\",\n",
      "    \"TaskDescription\": \"A game-like, intrinsically motivating task where participants can win or lose points, validated as intrinsically engaging for adults.\",\n",
      "    \"DesignDetails\": \"Participants completed a stopwatch task while being scanned in an MRI scanner. They were instructed that they could win or lose points depending on their performance, aiming for a total point of zero or more. The goal frame was manipulated such that they completed the task under different approach and avoidance goal frames across blocks, with points earned in approach frames and lost in avoidance frames. Each trial included cues and success/failure feedback phases.\",\n",
      "    \"Conditions\": [\n",
      "        \"Approach\",\n",
      "        \"Avoidance\",\n",
      "        \"Control\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD responses\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Blocked\"\n",
      "    ],\n",
      "    \"TaskDuration\": null\n",
      "}\n",
      "\n",
      "\n",
      "--- gpt-4o-mini ---\n",
      "\tStudyObjectve: The present study sought to address how the reward network in the brain is involved when individuals engage in the same task with a focus on approach or avoidance goals.\n",
      "fMRITasks:\n",
      "{\n",
      "    \"TaskName\": \"Stopwatch Task\",\n",
      "    \"TaskDescription\": \"Participants completed a stopwatch task while being scanned in an MRI scanner, focusing on either approach or avoidance goals.\",\n",
      "    \"DesignDetails\": \"Participants were instructed that they could win or lose points during the stopwatch task depending on their performance. The goal frame was manipulated such that participants completed the task under an approach goal frame in some blocks but under an avoidance goal frame in other blocks. Each block started with a short instruction indicating the type of the upcoming block (approach = Will win points if you succeed; avoidance = Will lose points if you fail). Each trial started with a cue for 2 seconds, followed by a jittered inter-stimulus interval (ISI) and the stopwatch task. The experiment included 117 trials in total, divided into three runs, with a total of 19 blocks of varying lengths.\",\n",
      "    \"Conditions\": [\n",
      "        \"Approach\",\n",
      "        \"Avoidance\",\n",
      "        \"Control\"\n",
      "    ],\n",
      "    \"TaskMetrics\": [\n",
      "        \"BOLD signal\",\n",
      "        \"Subjective motivational states\"\n",
      "    ],\n",
      "    \"RestingState\": false,\n",
      "    \"RestingStateMetadata\": null,\n",
      "    \"TaskDesign\": [\n",
      "        \"Mixed\"\n",
      "    ],\n",
      "    \"TaskDuration\": \"Total of 117 trials across three runs.\"\n",
      "}\n",
      "\n",
      "\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "clean_print_by_pmcid(has_taskname_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubget",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
