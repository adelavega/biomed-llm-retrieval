{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec33aee1-7c7b-4e3a-8b81-b1bd79e1ca5f",
   "metadata": {},
   "source": [
    "# Full Text Search evaluation with participant demographics\n",
    "\n",
    "Here, I evaluate the strategy of finding relevant text sections using chunk embeddings.\n",
    "\n",
    "First, I split PMC articles using Markdown (and lines), into chunks less than `n_tokens` (~4000).\n",
    "\n",
    "Next, we embed each chunks.\n",
    "\n",
    "Finally, using a text query, we find the most relevant section of each article for finding participant demographics.\n",
    "The query is also embedded and a distance metric is taken between each chunk and the query.\n",
    "\n",
    "To evaluate this method, I will see if this method correctly identifies the section where human annotators found demographic info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250c22c-a887-48b7-9e1d-73071984059f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31043f58-23b0-4ad8-a3db-5b6875442ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from labelrepo.projects.participant_demographics import get_participant_demographics\n",
    "from labelrepo.database import get_database_connection\n",
    "\n",
    "subgroups = get_participant_demographics(include_locations=True)\n",
    "docs_info = pd.read_sql(\n",
    "    \"select pmcid, publication_year, title, text from document\",\n",
    "    get_database_connection(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074e1132-e726-41c9-bbd2-dbbf9c4ba2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only look at single group documents\n",
    "jerome_pd = subgroups[(subgroups.project_name == 'participant_demographics') & \\\n",
    "                      (subgroups.annotator_name == 'Jerome_Dockes')]\n",
    "\n",
    "counts = jerome_pd.groupby('pmcid').count().reset_index()\n",
    "single_group_pmcids = counts[counts['count'] == 1].pmcid\n",
    "single_group = jerome_pd[jerome_pd.pmcid.isin(single_group_pmcids)]\n",
    "all_pd_docs = docs_info[docs_info.pmcid.isin(jerome_pd.pmcid)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d12784-5fb0-4f1c-8845-115130668f39",
   "metadata": {},
   "source": [
    "### Embed all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03363f3-9d81-4f95-b92d-8fff42ef8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from embed import embed_pmc_articles\n",
    "openai.api_key = open('/home/zorro/.keys/open_ai.key').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "57d49df6-edd4-4be4-a103-03c8fb04ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▎                                     | 16/156 [01:02<12:16,  5.26s/it]/home/zorro/repos/biomed-llm-retrieval/split.py:121: UserWarning: Skipping document, not in markdown\n",
      "  warnings.warn(\"Skipping document, not in markdown\")\n",
      "100%|█████████████████████████████████████████| 156/156 [08:30<00:00,  3.27s/it]\n"
     ]
    }
   ],
   "source": [
    "all_embeddings = embed_pmc_articles(all_pd_docs.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "82653681-200e-4df4-a185-69d4376dbea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(all_embeddings, open('data/all_embeddings.pkl', 'wb'))\n",
    "# all_embeddings = pickle.load(open('data/all_embeddings.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "32244c24-e545-410a-92e1-48df1e966d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = pd.DataFrame(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966239b6-86f1-4118-9ec9-393426890a86",
   "metadata": {},
   "source": [
    "### Test query across all documents\n",
    "\n",
    "Given a query, see what the average rank for the chunk matching the human annotation is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "f8d4b024-47c7-4b63-b9b0-056d86bb61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from embed import query_embeddings, get_chunks_heuristic, get_chunk_query_distance\n",
    "import re\n",
    "\n",
    "def get_matching_chunks(ranks_df, annotation_df):\n",
    "    \"\"\" Select the chunks that contains the original annotation \"\"\"\n",
    "    matches = []\n",
    "    for _, row in annotation_df.iterrows():\n",
    "        for ix, start_char in enumerate(row['start_char']):\n",
    "            end_char = row['end_char'][ix]\n",
    "            m = ranks_df[\n",
    "                (ranks_df['pmcid'] == row['pmcid']) & \\\n",
    "                (ranks_df['start_char'] <= start_char) & (ranks_df['end_char'] >= end_char)]\n",
    "            if not m.empty:\n",
    "                matches.append(m)\n",
    "                break\n",
    "    \n",
    "    return pd.concat(matches)\n",
    "\n",
    "def evaluate_query_across_docs(embeddings_df, annotations_df, query):\n",
    "    ranks_df = get_chunk_query_distance(embeddings_df, query)\n",
    "    \n",
    "    mc = get_matching_chunks(ranks_df, annotations_df)\n",
    "\n",
    "    print(\n",
    "        f\"Query: '{query}' \\nMean rank: {mc['rank'].mean():.2f},\\\n",
    "        Percentage match: {mc.pmcid.unique().shape[0] / embeddings_df.pmcid.unique().shape[0]:.2f}\\\n",
    "        top 1 %: {(mc['rank'] == 0).mean():.2f}, \\\n",
    "        top 3 %: {(mc['rank'] < 3).mean():.2f}\"\n",
    "    )\n",
    "\n",
    "def evaluate_query_plus_heuristic(embeddings_df, annotations_df, query, use_heuristic=True, section_2=True):\n",
    "    # Use heuristic to pre-select section\n",
    "    # Fall back to searching entire document if this fails\n",
    "    if use_heuristic:\n",
    "        embeddings_df = get_chunks_heuristic(embeddings_df, section_2=section_2)\n",
    "\n",
    "    # Rank chunks\n",
    "    ranks_df = get_chunk_query_distance(embeddings_df, query)\n",
    "\n",
    "    # Take only the top ranking chunk\n",
    "    top_1 = ranks_df[ranks_df['rank'] == 0]\n",
    "    mc_1 = get_matching_chunks(top_1, annotations_df)\n",
    "\n",
    "    # Only keep chunks that match annotation\n",
    "\n",
    "    top_3 = ranks_df[ranks_df['rank'] < 3]\n",
    "    mc_3 = get_matching_chunks(top_3, annotations_df)\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"Query: '{query}' \\n\\\n",
    "        % match top 1: {mc_1.pmcid.unique().shape[0] / embeddings_df.pmcid.unique().shape[0]:.2f} \\n\\\n",
    "        % match top 3: {mc_3.pmcid.unique().shape[0] / embeddings_df.pmcid.unique().shape[0]:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8afdc-8ad9-48bc-b6e7-4d5a1a51d3e1",
   "metadata": {},
   "source": [
    "### Semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "358bc87c-6506-44d7-a11a-ca1cea20431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['ljskfdklsjdfk', 'Methods section', 'Number of participants',\n",
    "           'The number of subjects or participants that were involved in the study or underwent MRI',\n",
    "           'How many participants or subjects were recruited for this study?',\n",
    "           'How many participants were recruited for this study?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "783a61ba-eb13-4fa5-a3f7-ea6bed0f40cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'ljskfdklsjdfk' \n",
      "Mean rank: 7.40,        Percentage match: 1.00        top 1 %: 0.01,         top 3 %: 0.20\n",
      "Query: 'Methods section' \n",
      "Mean rank: 6.93,        Percentage match: 1.00        top 1 %: 0.06,         top 3 %: 0.29\n",
      "Query: 'Number of participants' \n",
      "Mean rank: 3.40,        Percentage match: 1.00        top 1 %: 0.60,         top 3 %: 0.72\n",
      "Query: 'The number of subjects or participants that were involved in the study or underwent MRI' \n",
      "Mean rank: 3.48,        Percentage match: 1.00        top 1 %: 0.36,         top 3 %: 0.71\n",
      "Query: 'How many participants or subjects were recruited for this study?' \n",
      "Mean rank: 3.19,        Percentage match: 1.00        top 1 %: 0.64,         top 3 %: 0.77\n",
      "Query: 'How many participants were recruited for this study?' \n",
      "Mean rank: 3.16,        Percentage match: 1.00        top 1 %: 0.62,         top 3 %: 0.78\n"
     ]
    }
   ],
   "source": [
    "for q in queries:\n",
    "    evaluate_query_across_docs(all_embeddings, jerome_pd, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3cf52-cb72-4e8e-a6a6-3c08435b0f5b",
   "metadata": {},
   "source": [
    "### Try only on Body\n",
    "\n",
    "Looks like *for some studies* Jerome's annotations were only in the Body of the study, so it would be fair to exclude any embeddings not on the Body of the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "c0ba88aa-96e6-4b3e-b8d1-f306aeb2e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings_body = all_embeddings[all_embeddings.section_0 == 'Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "ea1ac069-f189-4c13-b5fe-7a5d20dc4f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'ljskfdklsjdfk' \n",
      "Mean rank: 4.35,        Percentage match: 0.90        top 1 %: 0.09,         top 3 %: 0.42\n",
      "Query: 'Methods section' \n",
      "Mean rank: 3.49,        Percentage match: 0.90        top 1 %: 0.14,         top 3 %: 0.48\n",
      "Query: 'Number of participants' \n",
      "Mean rank: 0.58,        Percentage match: 0.90        top 1 %: 0.73,         top 3 %: 0.91\n",
      "Query: 'The number of subjects or participants that were involved in the study or underwent MRI' \n",
      "Mean rank: 1.21,        Percentage match: 0.90        top 1 %: 0.43,         top 3 %: 0.88\n",
      "Query: 'How many participants or subjects were recruited for this study?' \n",
      "Mean rank: 0.50,        Percentage match: 0.90        top 1 %: 0.75,         top 3 %: 0.95\n",
      "Query: 'How many participants were recruited for this study?' \n",
      "Mean rank: 0.49,        Percentage match: 0.90        top 1 %: 0.74,         top 3 %: 0.95\n"
     ]
    }
   ],
   "source": [
    "for q in queries:\n",
    "    evaluate_query_across_docs(all_embeddings_body, jerome_pd, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f848e9c-6751-4930-bbd9-6d35df6abbe4",
   "metadata": {},
   "source": [
    "For some, it looks like the correct passage was outside the body (likely Abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be24466-2461-4b48-add4-ddd0ed8d3543",
   "metadata": {},
   "source": [
    "### Single group only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "fd802760-4fa6-4692-846c-99b6e23a2e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_group_embeddings_body = all_embeddings_body[all_embeddings_body.pmcid.isin(single_group.pmcid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "db4399dc-6d25-4e90-86c6-6dca4e0cf3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'ljskfdklsjdfk' \n",
      "Mean rank: 3.33,        Percentage match: 0.92        top 1 %: 0.09,         top 3 %: 0.65\n",
      "Query: 'Methods section' \n",
      "Mean rank: 2.52,        Percentage match: 0.92        top 1 %: 0.20,         top 3 %: 0.67\n",
      "Query: 'Number of participants' \n",
      "Mean rank: 0.25,        Percentage match: 0.92        top 1 %: 0.88,         top 3 %: 0.99\n",
      "Query: 'The number of subjects or participants that were involved in the study or underwent MRI' \n",
      "Mean rank: 0.70,        Percentage match: 0.92        top 1 %: 0.64,         top 3 %: 0.91\n",
      "Query: 'How many participants or subjects were recruited for this study?' \n",
      "Mean rank: 0.19,        Percentage match: 0.92        top 1 %: 0.88,         top 3 %: 0.99\n",
      "Query: 'How many participants were recruited for this study?' \n",
      "Mean rank: 0.16,        Percentage match: 0.92        top 1 %: 0.91,         top 3 %: 0.99\n"
     ]
    }
   ],
   "source": [
    "for q in queries:\n",
    "    evaluate_query_across_docs(single_group_embeddings_body, jerome_pd, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c324202d-eb3a-46dc-ad06-80c02a8b766d",
   "metadata": {},
   "source": [
    "## Try combined approach (heuristic + embedding fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "7984ca9a-5b8c-4f25-addf-1cc0b38b87ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'ljskfdklsjdfk' \n",
      "        % match top 1: 0.01 \n",
      "        % match top 3: 0.27\n",
      "Query: 'Methods section' \n",
      "        % match top 1: 0.10 \n",
      "        % match top 3: 0.37\n",
      "Query: 'Number of participants' \n",
      "        % match top 1: 0.71 \n",
      "        % match top 3: 0.82\n",
      "Query: 'The number of subjects or participants that were involved in the study or underwent MRI' \n",
      "        % match top 1: 0.46 \n",
      "        % match top 3: 0.80\n",
      "Query: 'How many participants or subjects were recruited for this study?' \n",
      "        % match top 1: 0.74 \n",
      "        % match top 3: 0.86\n",
      "Query: 'How many participants were recruited for this study?' \n",
      "        % match top 1: 0.73 \n",
      "        % match top 3: 0.86\n"
     ]
    }
   ],
   "source": [
    "for q in queries:\n",
    "    evaluate_query_plus_heuristic(all_embeddings, jerome_pd, q, use_heuristic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "8f6c1b7f-d1e0-405a-9dc6-9c11b05b5d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'ljskfdklsjdfk' \n",
      "        % match top 1: 0.30 \n",
      "        % match top 3: 0.71\n",
      "Query: 'Methods section' \n",
      "        % match top 1: 0.38 \n",
      "        % match top 3: 0.71\n",
      "Query: 'Number of participants' \n",
      "        % match top 1: 0.76 \n",
      "        % match top 3: 0.86\n",
      "Query: 'The number of subjects or participants that were involved in the study or underwent MRI' \n",
      "        % match top 1: 0.47 \n",
      "        % match top 3: 0.82\n",
      "Query: 'How many participants or subjects were recruited for this study?' \n",
      "        % match top 1: 0.80 \n",
      "        % match top 3: 0.86\n",
      "Query: 'How many participants were recruited for this study?' \n",
      "        % match top 1: 0.78 \n",
      "        % match top 3: 0.86\n"
     ]
    }
   ],
   "source": [
    "for q in queries:\n",
    "    evaluate_query_plus_heuristic(all_embeddings, jerome_pd, q, section_2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "ac7ddea9-7af0-4457-bd41-9d9b12512d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'ljskfdklsjdfk' \n",
      "        % match top 1: 0.67 \n",
      "        % match top 3: 0.83\n",
      "Query: 'Methods section' \n",
      "        % match top 1: 0.68 \n",
      "        % match top 3: 0.82\n",
      "Query: 'Number of participants' \n",
      "        % match top 1: 0.81 \n",
      "        % match top 3: 0.84\n",
      "Query: 'The number of subjects or participants that were involved in the study or underwent MRI' \n",
      "        % match top 1: 0.73 \n",
      "        % match top 3: 0.84\n",
      "Query: 'How many participants or subjects were recruited for this study?' \n",
      "        % match top 1: 0.82 \n",
      "        % match top 3: 0.84\n",
      "Query: 'How many participants were recruited for this study?' \n",
      "        % match top 1: 0.82 \n",
      "        % match top 3: 0.84\n"
     ]
    }
   ],
   "source": [
    "for q in queries:\n",
    "    evaluate_query_plus_heuristic(all_embeddings, jerome_pd, q, section_2=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff02397f-5025-460d-81b5-8b11ec56da1f",
   "metadata": {},
   "source": [
    "Using heuristic seems to increase the chances that the correct chunk is the top chunk, but also slighlty increases overall misses.\n",
    "\n",
    "The best approach is likely to use only the Methods only heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a26b002-83d4-40c7-86f6-59b84edac45b",
   "metadata": {},
   "source": [
    "# Extract Sample Size from relevant section (single group only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "be06f7f4-59ba-481b-82d7-b7bacdd659a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract import extract_from_multiple\n",
    "from templates import ZERO_SHOT_SAMPLE_SIZE_FUNCTION\n",
    "\n",
    "def extract_sample_size_full_text(embeddings_df, annotations_df, query, template, num_workers=3, model_name='gpt-3.5-turbo'):\n",
    "    ranks_df = get_chunk_query_distance(embeddings_df, query)\n",
    "    mc = get_matching_chunks(ranks_df, annotations_df).rename(columns={'rank': 'matching_rank'})[['pmcid', 'matching_rank']]\n",
    "    \n",
    "    ranks_df = pd.merge(ranks_df, mc, on='pmcid')\n",
    "    ranks_df['is_matching_chunk'] = ranks_df['rank'] == ranks_df['matching_rank']\n",
    "\n",
    "    # Subset to only include top ranked chunks\n",
    "    ranks_df = ranks_df[ranks_df['rank'] == 0]\n",
    "\n",
    "    # For every chunk, apply template\n",
    "    predictions = extract_from_multiple(\n",
    "        ranks_df.content.to_list(), \n",
    "        **template, \n",
    "        num_workers=num_workers,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    predictions = pd.DataFrame(predictions)\n",
    "\n",
    "    predictions['is_matching_chunk'] = ranks_df['is_matching_chunk'].tolist()\n",
    "    predictions['pmcid'] = ranks_df['pmcid'].tolist()\n",
    "    predictions['content'] =  ranks_df['content'].tolist()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "9c9ccc6b-7125-4e0e-aae1-f6546e5aa3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 69/69 [00:11<00:00,  5.77it/s]\n"
     ]
    }
   ],
   "source": [
    "full_text_demo = extract_sample_size_full_text(\n",
    "    single_group_embeddings_body, single_group,\n",
    "    'How many participants were recruited for this study?', \n",
    "    ZERO_SHOT_SAMPLE_SIZE_FUNCTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "92ac1236-a4a3-483a-a611-cb74fbb5e0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_group_embeddings_body.pmcid.unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32feaaa3-10da-42de-bd67-cf760c291a47",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "f0cc9ee5-e8ed-4ab9-9796-306c21780933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_percentage(num1, num2, percentage=10):\n",
    "    if int(num1) == int(num2):\n",
    "        return True\n",
    "    if abs(int(num1) - int(num2)) / int(num1) <= percentage / 100:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def _print_evaluation(predictions_df, annotations_df):\n",
    "\n",
    "    # Combine annotations with predicted values\n",
    "    eval_df = annotations_df.reset_index()[['count', 'pmcid']].rename(columns={'count': 'annot_count'})\n",
    "    predictions_df = pd.merge(predictions_df, eval_df)\n",
    "    predictions_df['correct'] = predictions_df['count'] == predictions_df['annot_count']\n",
    "    \n",
    "    wrong_chunk = predictions_df[predictions_df.is_matching_chunk == False]\n",
    "\n",
    "    matching = predictions_df[predictions_df.is_matching_chunk]\n",
    "\n",
    "    non_na_ix = ((pd.isna(matching['count']) == False) & (matching['count'] != 0))\n",
    "\n",
    "    nonna = matching[non_na_ix]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Accuracy: {predictions_df['correct'].mean():.2f}\n",
    "    % FTS chose a chunk w/ annotated information: {predictions_df.is_matching_chunk.mean():.2f}\n",
    "    %  null when wrong chunk: {pd.isna(wrong_chunk['count']).mean():.2f}\n",
    "    Accuracy for cases when correct chunk was given to LLM: {matching['correct'].mean():.2f}\n",
    "    % LLM reported a non-na value when correct chunk was given: {non_na_ix.mean():.2f}\n",
    "    Accuracy for non-NA values w/ correct chunk given: {nonna['correct'].mean():.2f}\n",
    "    Accuracy within 10%: {(nonna.apply(lambda x: is_within_percentage(x['count'], x['annot_count'], 10), axis=1)).mean():.2f}\n",
    "    Accuracy within 20%: {(nonna.apply(lambda x: is_within_percentage(x['count'], x['annot_count'], 20), axis=1)).mean():.2f}\n",
    "    Accuracy within 30%: {(nonna.apply(lambda x: is_within_percentage(x['count'], x['annot_count'], 30), axis=1)).mean():.2f}\"\"\")\n",
    "\n",
    "    return predictions_df, nonna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "be0d2d05-b306-4f15-88c1-e677af49110c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Accuracy: 0.72\n",
      "    % FTS chose a chunk w/ annotated information: 0.91\n",
      "    %  null when wrong chunk: 0.33\n",
      "    Accuracy for cases when correct chunk was given to LLM: 0.79\n",
      "    % LLM reported a non-na value when correct chunk was given: 1.00\n",
      "    Accuracy for non-NA values w/ correct chunk given: 0.79\n",
      "    Accuracy within 10%: 0.84\n",
      "    Accuracy within 20%: 0.97\n",
      "    Accuracy within 30%: 0.98\n"
     ]
    }
   ],
   "source": [
    "full_text_demo, ft_nonna  = _print_evaluation(full_text_demo, single_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ecee8-4b84-488f-b3a2-63c208f85a55",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "When the correct chunk is given the GPT-3.5, it can extract a sample size value (although typically not final N), most of the time, with relatively few gross errors.\n",
    "\n",
    "However, when given the incorrect chunk, it will often not give `null` values when it should"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d9095-5b36-4384-81ab-9c8480d8bcc7",
   "metadata": {},
   "source": [
    "#### Incorrect responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "12bedafb-0c0e-4e41-9eb9-73d5ab85cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = ft_nonna[ft_nonna['correct'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c1854-2cd7-4648-bc81-1f3ed8f38e9f",
   "metadata": {},
   "source": [
    "- Majority are only off by a few, due to complex exclusion crtiera\n",
    "- Attempting to change the prompt to identify # of excluded subjects actually makes accuracy go down, and n deviates further from either final N or total N (and substracting the two numbers doesn't help)\n",
    "- In one case, the annotation is actually incorrect. (1/10)\n",
    "- Sometimes models  confused other info for demographic information (i.e. ROIs).\n",
    "  It seems as if the models are good at putting `n/a` for these section, but sometimes (in a non stable manner), fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90b785-0895-4cd4-bc64-df3bf75ef77c",
   "metadata": {},
   "source": [
    "## GPT-4 Full Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "247a0399-b0e5-4e31-8577-7f78d75e6018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 69/69 [03:17<00:00,  2.86s/it]\n"
     ]
    }
   ],
   "source": [
    "full_text_gpt4 = extract_sample_size_full_text(\n",
    "    single_group_embeddings_body, single_group,\n",
    "    'How many participants were recruited for this study?',\n",
    "    ZERO_SHOT_SAMPLE_SIZE_FUNCTION,\n",
    "    model_name='gpt-4',\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92f3e957-a619-454d-ba54-f8880b3a4a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Accuracy: 0.86\n",
      "    % FTS chose a chunk w/ annotated information: 0.91\n",
      "    %  null when wrong chunk: 0.83\n",
      "    Accuracy for cases when correct chunk was given to LLM: 0.92\n",
      "    % LLM reported a non-na value when correct chunk was given: 1.00\n",
      "    Accuracy for non-NA values w/ correct chunk given: 0.92\n",
      "    Accuracy within 10%: 0.94\n",
      "    Accuracy within 20%: 0.98\n",
      "    Accuracy within 30%: 0.98\n"
     ]
    }
   ],
   "source": [
    "full_text_gpt4, ft_gpt4_nonna  = _print_evaluation(full_text_gpt4, single_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bcd76-1370-408c-bdf8-6578baa30df8",
   "metadata": {},
   "source": [
    "GPT-4 is slightly more accurate, but more importantly, is less likely to hallucinate or get the wrong answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc4cfecf-46f6-43be-8dc3-adb3672d6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(full_text_gpt4, open('data/full_text_gpt4_single_group.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
