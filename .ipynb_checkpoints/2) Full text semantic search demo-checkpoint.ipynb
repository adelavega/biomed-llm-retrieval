{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef92e3c",
   "metadata": {},
   "source": [
    "## Full text semantic search - evaluation\n",
    "\n",
    "In order to perform IR over full texts, we need to find relevant section to use as context for GPT.\n",
    "\n",
    "This is typically done by chunking texts into sections, and obtaining embeddings for each section of the text.\n",
    "Then, using a distance metric, one can find the most relevant section for use as context to the GPT query. \n",
    "\n",
    "Here, I will evaluate this method for finding sample size in papers that do not list it in the Abstract.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef0aa8-7c24-419c-aaf6-ff268519368b",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Load participant annotations, and full text for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a966bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from labelrepo.projects.participant_demographics import (\n",
    "    get_participant_demographics, select_participants_annotations\n",
    ")\n",
    "\n",
    "subgroups = get_participant_demographics(include_locations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "763436ba-3b67-48d9-adf4-656acabd77b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from labelrepo.database import get_database_connection\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "docs_info = pd.read_sql(\n",
    "    \"select pmcid, publication_year, title, text from document\",\n",
    "    get_database_connection(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270ef573-54c8-42d6-bd8a-9e7804ebcf12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a1395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = pd.merge(subgroups, docs_info, how='left')\n",
    "jerome_pd = subgroups[(subgroups.project_name == 'participant_demographics') & \\\n",
    "                      (subgroups.annotator_name == 'Jerome_Dockes')]\n",
    "\n",
    "counts = jerome_pd.groupby('pmcid').count().reset_index()\n",
    "single_group_pmcids = counts[counts['count'] == 1].pmcid\n",
    "single_group = jerome_pd[jerome_pd.pmcid.isin(single_group_pmcids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b437aebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "473a6b65-ec66-412a-9929-537fd9b9be72",
   "metadata": {},
   "source": [
    "# Case study\n",
    "\n",
    "Testing out workflow with single study with participant info not in Abstact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6527ef87-28ee-4f2f-8a2c-e304886d0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = single_group[single_group.pmcid == 5548834]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e44634-7158-4bd1-8e53-9e18e9732cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = example.text.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5c3a6",
   "metadata": {},
   "source": [
    "### Chunk Full Text\n",
    "\n",
    "The maximum context length is 4k-16k tokens for GPT 3.5, therefore we need to break up the text into chunks/\n",
    "The following chunks PMC articles by Markdown into sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "366b7187-d1ef-4e76-83d4-e38e8461171c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from embed import embed_pmc_article, embed_text\n",
    "openai.api_key = open('/home/zorro/.keys/open_ai.key').read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79098dac-f95b-4c74-bd23-df1473ff127a",
   "metadata": {},
   "source": [
    "### Embed chunks\n",
    "\n",
    "`embed_pmc_article` chunks the document, and retrieves embeddings for each chunk, returned alongside chunk meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb0d1cb-a9b1-491d-b27e-d91f4aee91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed_pmc_article(full_text, model_name='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ac4b746-b20d-4ee6-a058-ca5e3cb38960",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.DataFrame(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c070665",
   "metadata": {},
   "source": [
    "### Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e891409f-3082-47af-bb71-b1b563a34af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many subjects are in the study?\"\n",
    "\n",
    "query_embedding = embed_text(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "113b59e3-0260-468a-812c-3fd24bf2a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af99cb77-bc37-45ad-9cc2-debed7708f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = np.array(embeddings['embedding'].tolist())\n",
    "distances = euclidean_distances(doc_embeddings, np.array(query_embedding).reshape(1, -1), squared=True)\n",
    "embeddings['distances'] = distances\n",
    "embeddings.sort_values('distances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53354b86-5484-4862-8c9f-91aa2199dbca",
   "metadata": {},
   "source": [
    "The closest passage is where the participant information is found!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93992ef1-fc01-4cf6-8fb1-dfd0d12ba0bb",
   "metadata": {},
   "source": [
    "### Comparison to Jerome's annotations\n",
    "\n",
    "The goal here is to see if across documents we can design a query that will find the target passage (i.e. where the participant informaton actually exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "172dd0d4-fe9e-4adc-8b24-48ce5a0f03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1 = embeddings.sort_values('distances').iloc[0]\n",
    "example.start_char > top_1.start_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f3749a8-9004-4fc8-9e3b-55c5e438f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example.end_char < top_1.end_char"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
