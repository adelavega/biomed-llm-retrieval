{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec33aee1-7c7b-4e3a-8b81-b1bd79e1ca5f",
   "metadata": {},
   "source": [
    "# Full Text Search evaluation with participant demographics\n",
    "\n",
    "Here, I evaluate the strategy of finding relevant text sections using chunk embeddings.\n",
    "\n",
    "First, I split PMC articles using Markdown (and lines), into chunks less than `n_tokens` (~4000).\n",
    "\n",
    "Next, we embed each chunks.\n",
    "\n",
    "Finally, using a text query, we find the most relevant section of each article for finding participant demographics.\n",
    "The query is also embedded and a distance metric is taken between each chunk and the query.\n",
    "\n",
    "To evaluate this method, I will see if this method correctly identifies the section where human annotators found demographic info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250c22c-a887-48b7-9e1d-73071984059f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "31043f58-23b0-4ad8-a3db-5b6875442ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from labelrepo.projects.participant_demographics import get_participant_demographics\n",
    "from labelrepo.database import get_database_connection\n",
    "\n",
    "subgroups = get_participant_demographics(include_locations=True)\n",
    "docs_info = pd.read_sql(\n",
    "    \"select pmcid, publication_year, title, text from document\",\n",
    "    get_database_connection(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "074e1132-e726-41c9-bbd2-dbbf9c4ba2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only look at single group documents\n",
    "jerome_pd = subgroups[(subgroups.project_name == 'participant_demographics') & \\\n",
    "                      (subgroups.annotator_name == 'Jerome_Dockes')]\n",
    "\n",
    "counts = jerome_pd.groupby('pmcid').count().reset_index()\n",
    "single_group_pmcids = counts[counts['count'] == 1].pmcid\n",
    "single_group = jerome_pd[jerome_pd.pmcid.isin(single_group_pmcids)]\n",
    "single_group_docs = docs_info[docs_info.pmcid.isin(single_group.pmcid)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d12784-5fb0-4f1c-8845-115130668f39",
   "metadata": {},
   "source": [
    "### Embed all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03363f3-9d81-4f95-b92d-8fff42ef8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = open('/home/zorro/.keys/open_ai.key').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05afc81-5548-43b9-8de6-91b815156861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embed import embed_pmc_articles, query_embeddings\n",
    "texts = single_group_docs[['pmcid', 'text']].to_dict(orient='records')\n",
    "# single_group_embeddings = embed_pmc_articles(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82653681-200e-4df4-a185-69d4376dbea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(single_group_embeddings, open('data/single_group_embeddings.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed160b44-d6ec-46db-bbd2-166d31dc0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_group_embeddings = pickle.load(open('data/single_group_embeddings.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966239b6-86f1-4118-9ec9-393426890a86",
   "metadata": {},
   "source": [
    "### Test query across all documents\n",
    "\n",
    "Given a query, see what the average rank for the chunk matching the human annotation is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b423b9e-85ab-429e-957a-d8dc59fe007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_group_embeddings = pd.DataFrame(single_group_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8d4b024-47c7-4b63-b9b0-056d86bb61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def get_matching_chunks(ranks_df, annotation_df):\n",
    "    \"\"\" Select the chunks that contains the original annotation \"\"\"\n",
    "    matches = []\n",
    "    for _, row in annotation_df.iterrows():\n",
    "        for ix, start_char in enumerate(row['start_char']):\n",
    "            end_char = row['end_char'][ix]\n",
    "            m = ranks_df[\n",
    "                (ranks_df['pmcid'] == row['pmcid']) & \\\n",
    "                (ranks_df['start_char'] <= start_char) & (ranks_df['end_char'] >= end_char)]\n",
    "            if not m.empty:\n",
    "                matches.append(m)\n",
    "                break\n",
    "    \n",
    "    return pd.concat(matches)\n",
    "    \n",
    "def evaluate_query_across_docs(embeddings_df, annotations_df, query):\n",
    "    # For every document, get distance and rank between query and embeddings\n",
    "    distances, ranks = zip(*[\n",
    "        query_embeddings(sub_df['embedding'].tolist(), query) \n",
    "        for pmcid, sub_df in embeddings_df.groupby('pmcid', sort=False)\n",
    "    ])\n",
    "\n",
    "    # Combine with meta-data into a df\n",
    "    ranks_df = embeddings_df[['pmcid', 'start_char', 'end_char']].copy()\n",
    "    ranks_df['distance'] = np.concatenate(distances)\n",
    "    ranks_df['rank'] = np.concatenate(ranks)\n",
    "    \n",
    "    mc = get_matching_chunks(ranks_df, annotations_df)\n",
    "\n",
    "    print(\n",
    "        f\"Query: '{query}' \\nMean rank: {mc['rank'].mean():.2f},\\\n",
    "        top 1 %: {(mc['rank'] == 0).mean():.2f}, \\\n",
    "        top 3 %: {(mc['rank'] <= 3).mean():.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "358bc87c-6506-44d7-a11a-ca1cea20431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['ljskfdklsjdfk', 'Methods section', 'Number of participants',\n",
    "           'The number of subjects or participants that were involved in the study or underwent MRI',\n",
    "           'How many participants or subjects were recruited for this study?',\n",
    "           'How many participants were recruited for this study?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "783a61ba-eb13-4fa5-a3f7-ea6bed0f40cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'ljskfdklsjdfk' \n",
      "Mean rank: 5.68,        top 1 %: 0.03,         top 3 %: 0.48\n",
      "Query: 'Methods section' \n",
      "Mean rank: 4.95,        top 1 %: 0.12,         top 3 %: 0.52\n",
      "Query: 'Number of participants' \n",
      "Mean rank: 1.52,        top 1 %: 0.80,         top 3 %: 0.89\n",
      "Query: 'The number of subjects or participants that were involved in the study or underwent MRI' \n",
      "Mean rank: 1.60,        top 1 %: 0.59,         top 3 %: 0.88\n",
      "Query: 'How many participants or subjects were recruited for this study?' \n",
      "Mean rank: 1.39,        top 1 %: 0.81,         top 3 %: 0.91\n",
      "Query: 'How many participants were recruited for this study?' \n",
      "Mean rank: 1.32,        top 1 %: 0.84,         top 3 %: 0.91\n"
     ]
    }
   ],
   "source": [
    "# for q in queries:\n",
    "#     evaluate_query_across_docs(single_group_embeddings, single_group, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3cf52-cb72-4e8e-a6a6-3c08435b0f5b",
   "metadata": {},
   "source": [
    "### Try only on Body\n",
    "\n",
    "Looks like *for some studies* Jerome's annotations were only in the Body of the study, so it would be fair to exclude any embeddings not on the Body of the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddc75c7-9cc9-4143-aaa3-b43964ca45bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_group_embeddings_body = single_group_embeddings[single_group_embeddings.section_name == 'Body'].reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea1ac069-f189-4c13-b5fe-7a5d20dc4f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'ljskfdklsjdfk' \n",
      "Mean rank: 3.33,        top 1 %: 0.09,         top 3 %: 0.72\n",
      "Query: 'Methods section' \n",
      "Mean rank: 2.55,        top 1 %: 0.20,         top 3 %: 0.75\n",
      "Query: 'Number of participants' \n",
      "Mean rank: 0.25,        top 1 %: 0.88,         top 3 %: 0.99\n",
      "Query: 'The number of subjects or participants that were involved in the study or underwent MRI' \n",
      "Mean rank: 0.70,        top 1 %: 0.64,         top 3 %: 0.96\n",
      "Query: 'How many participants or subjects were recruited for this study?' \n",
      "Mean rank: 0.19,        top 1 %: 0.88,         top 3 %: 0.99\n",
      "Query: 'How many participants were recruited for this study?' \n",
      "Mean rank: 0.16,        top 1 %: 0.91,         top 3 %: 0.99\n"
     ]
    }
   ],
   "source": [
    "# for q in queries:\n",
    "#     evaluate_query_across_docs(single_group_embeddings_body, single_group, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a26b002-83d4-40c7-86f6-59b84edac45b",
   "metadata": {},
   "source": [
    "# Extract Sample Size from relevant secton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "be06f7f4-59ba-481b-82d7-b7bacdd659a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract import extract_from_multiple\n",
    "from templates import ZERO_SHOT_SAMPLE_SIZE_FUNCTION\n",
    "\n",
    "def extract_sample_size_full_text(embeddings_df, annotations_df, query, template, num_workers=3, model_name=None):\n",
    "    # For every document, get distance and rank between query and embeddings\n",
    "    distances, ranks = zip(*[\n",
    "        query_embeddings(sub_df['embedding'].tolist(), query) \n",
    "        for pmcid, sub_df in embeddings_df.groupby('pmcid', sort=False)\n",
    "    ])\n",
    "\n",
    "    # Combine with meta-data into a df\n",
    "    ranks_df = embeddings_df[['pmcid', 'content', 'start_char', 'end_char']].copy()\n",
    "    ranks_df['rank'] = np.concatenate(ranks)\n",
    "\n",
    "    # See if chunk being fed to LLM is \"correct\" chunk\n",
    "    mc = get_matching_chunks(ranks_df, annotations_df).rename(columns={'rank': 'matching_rank'})\n",
    "    \n",
    "    mc = mc[['pmcid', 'matching_rank']]\n",
    "    ranks_df = pd.merge(ranks_df, mc, on='pmcid')\n",
    "    ranks_df['is_matching_chunk'] = ranks_df['rank'] == ranks_df['matching_rank']\n",
    "\n",
    "    # Subset to only include top ranked chunks\n",
    "    ranks_df = ranks_df[ranks_df['rank'] == 0]\n",
    "\n",
    "    # For every chunk, apply template\n",
    "    predictions = extract_from_multiple(\n",
    "        ranks_df.content.to_list(), \n",
    "        **template, \n",
    "        num_workers=num_workers,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    predictions['is_matching_chunk'] = ranks_df['is_matching_chunk'].tolist()\n",
    "    predictions['pmcid'] = ranks_df['pmcid'].tolist()\n",
    "    predictions['content'] =  ranks_df['content'].tolist()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c9ccc6b-7125-4e0e-aae1-f6546e5aa3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 69/69 [00:13<00:00,  5.28it/s]\n"
     ]
    }
   ],
   "source": [
    "full_text_demo = extract_sample_size_full_text(\n",
    "    single_group_embeddings_body, single_group,\n",
    "    'How many participants were recruited for this study?', \n",
    "    ZERO_SHOT_SAMPLE_SIZE_FUNCTION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32feaaa3-10da-42de-bd67-cf760c291a47",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f0cc9ee5-e8ed-4ab9-9796-306c21780933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_percentage(num1, num2, percentage=10):\n",
    "    if int(num1) == int(num2):\n",
    "        return True\n",
    "    if abs(int(num1) - int(num2)) / int(num1) <= percentage / 100:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def _print_evaluation(predictions_df, annotations_df):\n",
    "\n",
    "    # Combine annotations with predicted values\n",
    "    eval_df = annotations_df.reset_index()[['count', 'pmcid']].rename(columns={'count': 'annot_count'})\n",
    "    predictions_df = pd.merge(predictions_df, eval_df)\n",
    "    predictions_df['correct'] = predictions_df['count'] == predictions_df['annot_count']\n",
    "\n",
    "\n",
    "    \n",
    "    wrong_chunk = predictions_df[predictions_df.is_matching_chunk == False]\n",
    "\n",
    "    matching = predictions_df[predictions_df.is_matching_chunk]\n",
    "\n",
    "    non_na_ix = ((pd.isna(matching['count']) == False) & (matching['count'] != 0))\n",
    "\n",
    "    nonna = matching[non_na_ix]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Accuracy: {predictions_df['correct'].mean():.2f}\n",
    "    % FTS chose a chunk w/ annotated information: {predictions_df.is_matching_chunk.mean():.2f}\n",
    "    %  null when wrong chunk: {pd.isna(wrong_chunk['count']).mean():.2f}\n",
    "    Accuracy for cases when correct chunk was given to LLM: {matching['correct'].mean():.2f}\n",
    "    % LLM reported a non-na value when correct chunk was given: {non_na_ix.mean():.2f}\n",
    "    Accuracy for non-NA values w/ correct chunk given: {nonna['correct'].mean():.2f}\n",
    "    Accuracy within 10%: {(nonna.apply(lambda x: is_within_percentage(x['count'], x['annot_count'], 10), axis=1)).mean():.2f}\n",
    "    Accuracy within 20%: {(nonna.apply(lambda x: is_within_percentage(x['count'], x['annot_count'], 20), axis=1)).mean():.2f}\n",
    "    Accuracy within 30%: {(nonna.apply(lambda x: is_within_percentage(x['count'], x['annot_count'], 30), axis=1)).mean():.2f}\"\"\")\n",
    "\n",
    "    return predictions_df, nonna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3a0174b2-13b3-4195-aaf5-4014dc6cc249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Accuracy: 0.68\n",
      "    % FTS chose a chunk w/ annotated information: 0.91\n",
      "    %  null when wrong chunk: 0.33\n",
      "    Accuracy for cases when correct chunk was given to LLM: 0.75\n",
      "    % LLM reported a non-na value when correct chunk was given: 1.00\n",
      "    Accuracy for non-NA values w/ correct chunk given: 0.75\n",
      "    Accuracy within 10%: 0.84\n",
      "    Accuracy within 20%: 0.97\n",
      "    Accuracy within 30%: 0.98\n"
     ]
    }
   ],
   "source": [
    "full_text_demo, ft_nonna  = _print_evaluation(full_text_demo, single_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ecee8-4b84-488f-b3a2-63c208f85a55",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "When the correct chunk is given the GPT-3.5, it can extract a sample size value (although typically not final N), most of the time, with relatively few gross errors.\n",
    "\n",
    "However, when given the incorrect chunk, it will often not give `null` values when it should"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d9095-5b36-4384-81ab-9c8480d8bcc7",
   "metadata": {},
   "source": [
    "#### Incorrect responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12bedafb-0c0e-4e41-9eb9-73d5ab85cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = ft_nonna[ft_nonna['correct'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c1854-2cd7-4648-bc81-1f3ed8f38e9f",
   "metadata": {},
   "source": [
    "- Majority are only off by a few, due to complex exclusion crtiera\n",
    "- Attempting to change the prompt to identify # of excluded subjects actually makes accuracy go down, and n deviates further from either final N or total N (and substracting the two numbers doesn't help)\n",
    "- In one case, the annotation is actually incorrect. (1/10)\n",
    "- Sometimes models  confused other info for demographic information (i.e. ROIs).\n",
    "  It seems as if the models are good at putting `n/a` for these section, but sometimes (in a non stable manner), fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90b785-0895-4cd4-bc64-df3bf75ef77c",
   "metadata": {},
   "source": [
    "## GPT-4 Full Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "247a0399-b0e5-4e31-8577-7f78d75e6018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 69/69 [03:06<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "full_text_gpt4 = extract_sample_size_full_text(\n",
    "    single_group_embeddings_body, single_group,\n",
    "    'How many participants were recruited for this study?',\n",
    "    ZERO_SHOT_SAMPLE_SIZE_FUNCTION,\n",
    "    model_name='gpt-4',\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "92f3e957-a619-454d-ba54-f8880b3a4a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Accuracy: 0.75\n",
      "    % FTS chose a chunk w/ annotated information: 0.91\n",
      "    %  null when wrong chunk: 0.83\n",
      "    Accuracy for cases when correct chunk was given to LLM: 0.83\n",
      "    % LLM reported a non-na value when correct chunk was given: 1.00\n",
      "    Accuracy for non-NA values w/ correct chunk given: 0.83\n",
      "    Accuracy within 10%: 0.86\n",
      "    Accuracy within 20%: 0.95\n",
      "    Accuracy within 30%: 0.97\n"
     ]
    }
   ],
   "source": [
    "full_text_gpt4, ft_gpt4_nonna  = _print_evaluation(full_text_gpt4, single_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bcd76-1370-408c-bdf8-6578baa30df8",
   "metadata": {},
   "source": [
    "GPT-4 is slightly more accurate, but more importantly, is less likely to hallucinate or get the wrong answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4cfecf-46f6-43be-8dc3-adb3672d6363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
